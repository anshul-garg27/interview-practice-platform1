{
  "problem_title": "Design a Hotel Booking/Reservation System - Part 3: Scaling for High Traffic",
  "part_number": 3,
  "builds_on": "Part 2",
  "difficulty": "hard",
  "problem_understanding": {
    "what_changes": "Part 2 focused on handling concurrent bookings with distributed locking. Part 3 requires scaling the system 10x while reducing latency. This means we need CQRS (separate read/write paths), aggressive caching, Elasticsearch for search, and geographic distribution. The fundamental shift is from a single database handling everything to a distributed system with eventual consistency for reads and strong consistency for writes.",
    "new_requirements": [
      "Handle 100K search QPS (10x increase)",
      "Handle 10K booking QPS (10x increase)",
      "Reduce search P99 latency from 500ms to 200ms",
      "Reduce booking P99 latency from 1s to 500ms",
      "Support worldwide users with data locality",
      "Implement multi-layer caching with proper invalidation"
    ],
    "new_constraints": [
      "Search results can be eventually consistent (stale up to a few seconds)",
      "Bookings must remain strongly consistent (no double bookings)",
      "Hot spots (popular hotels) must not overwhelm the system",
      "Cache invalidation must propagate within acceptable SLA"
    ],
    "key_insight": "The AHA! moment: Accept eventual consistency for search (reads) but enforce strong consistency for booking (writes). Searches don't need real-time accuracy - showing a room as available when it was just booked is acceptable because the booking service will catch it. This separation allows us to cache aggressively for reads while maintaining correctness for writes."
  },
  "requirements_coverage": {
    "checklist": [
      {
        "requirement": "100K search QPS with <200ms P99",
        "how_met": "Elasticsearch cluster + Redis caching layer + pre-computed popular searches",
        "gotchas": [
          "Cache stampede on popular hotels",
          "Elasticsearch query optimization needed"
        ]
      },
      {
        "requirement": "10K booking QPS with <500ms P99",
        "how_met": "Sharded PostgreSQL by hotel_id + Redis distributed locks + async event publishing",
        "gotchas": [
          "Cross-shard transactions are expensive",
          "Lock contention on hot rooms"
        ]
      },
      {
        "requirement": "Global user support",
        "how_met": "Geographic routing via CDN + regional deployments + cross-region replication",
        "gotchas": [
          "Replication lag affects consistency",
          "Network partitions between regions"
        ]
      },
      {
        "requirement": "Cache invalidation",
        "how_met": "Event-driven invalidation via Kafka + TTL-based fallback + version tags",
        "gotchas": [
          "Cache and DB race conditions",
          "Partial invalidation failures"
        ]
      }
    ],
    "complexity_targets": [
      {
        "operation": "searchRoomsOptimized",
        "target": "O(1) cache hit, O(log n) cache miss",
        "achieved": "O(1) / O(log n)",
        "why": "Redis cache gives O(1), Elasticsearch with proper indexing gives O(log n)"
      },
      {
        "operation": "createBooking",
        "target": "O(1) with lock acquisition",
        "achieved": "O(1)",
        "why": "Direct shard access with optimistic locking"
      }
    ],
    "non_goals": [
      "Real-time analytics",
      "Machine learning recommendations",
      "Payment processing details",
      "Hotel management dashboard"
    ]
  },
  "assumptions": [
    "Hotel metadata changes infrequently (can cache for hours)",
    "Availability can be stale by up to 5 seconds for search",
    "Users search the same locations repeatedly (high cache hit rate)",
    "Regional data has locality (users mostly book local hotels)",
    "We have Kafka, Redis, Elasticsearch infrastructure available"
  ],
  "tradeoffs": [
    {
      "decision": "Cache-aside vs Write-through",
      "chosen": "Cache-aside with event-driven invalidation",
      "why": "Lower write latency, acceptable staleness for searches",
      "alternative": "Write-through",
      "when_to_switch": "If staleness causes significant business impact"
    },
    {
      "decision": "Sharding by hotel_id vs location",
      "chosen": "hotel_id",
      "why": "All operations for a hotel hit same shard, simplifies transactions",
      "alternative": "Location-based sharding",
      "when_to_switch": "If search patterns are highly regional and cross-shard queries are rare"
    },
    {
      "decision": "Eventual vs Strong consistency for search",
      "chosen": "Eventual consistency",
      "why": "10x latency improvement, booking still catches conflicts",
      "alternative": "Strong consistency",
      "when_to_switch": "If showing stale availability causes significant user frustration"
    }
  ],
  "extensibility_notes": {
    "what_to_keep_stable": [
      "Booking confirmation flow",
      "Distributed lock interface",
      "Event schema for Kafka"
    ],
    "what_to_change": [
      "Added CacheManager layer",
      "Added SearchService with Elasticsearch",
      "Added EventPublisher for Kafka",
      "Added GeoRouter for regional routing"
    ],
    "interfaces_and_boundaries": "Clean separation between SearchService (read path) and BookingService (write path). Events flow through Kafka for eventual sync. Each service can scale independently.",
    "invariants": [
      "Bookings always go through distributed lock",
      "Cache invalidation events always published after successful booking",
      "Search never writes to primary database"
    ]
  },
  "visual_explanation": {
    "before_after": "```\nBEFORE (Part 2):                         AFTER (Part 3):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Single Service    \u2502                  \u2502         Global CDN                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502                  \u2502              \u2502                      \u2502\n\u2502  \u2502   PostgreSQL  \u2502  \u2502                  \u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n\u2502  \u2502   + Redis     \u2502  \u2502                  \u2502     \u2502                 \u2502             \u2502\n\u2502  \u2502   Lock Only   \u2502  \u2502                  \u2502  US-East          EU-West           \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502                  \u2502     \u2502                 \u2502             \u2502\n\u2502                     \u2502                  \u2502 \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502  All reads/writes   \u2502                  \u2502 \u2502Search \u2502       \u2502  Search  \u2502        \u2502\n\u2502  go to same DB      \u2502                  \u2502 \u2502 +ES   \u2502       \u2502   +ES    \u2502        \u2502\n\u2502                     \u2502                  \u2502 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2502     \u2502                \u2502              \u2502\n                                         \u2502 \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2534\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n                                         \u2502 \u2502 Redis Cache Cluster      \u2502        \u2502\n                                         \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n                                         \u2502     \u2502                \u2502              \u2502\n                                         \u2502 \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510         \u2502\n                                         \u2502 \u2502Booking\u2502       \u2502 Booking \u2502         \u2502\n                                         \u2502 \u2502Primary\u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u25ba\u2502 Replica \u2502         \u2502\n                                         \u2502 \u2514\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n                                         \u2502     \u2502                               \u2502\n                                         \u2502 \u250c\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n                                         \u2502 \u2502      Kafka Events         \u2502       \u2502\n                                         \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n                                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
    "algorithm_flow": "```\nSEARCH FLOW (Read Path):                  BOOKING FLOW (Write Path):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 1. Check Redis     \u2502                    \u2502 1. Acquire Redis   \u2502\n\u2502    Cache           \u2502                    \u2502    Distributed Lock\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                                         \u2502\n    \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510                              \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Cache   \u2502YES                           \u2502 Write to  \u2502\n    \u2502 Hit?    \u251c\u2500\u2500\u25ba Return cached result      \u2502 Primary   \u2502\n    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518                              \u2502 PostgreSQL\u2502\n         \u2502NO                                 \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n    \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                               \u2502\n    \u2502 Query        \u2502                         \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Elasticsearch\u2502                         \u2502 Publish   \u2502\n    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502 Event to  \u2502\n         \u2502                                   \u2502 Kafka     \u2502\n    \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502 Cache result \u2502                               \u2502\n    \u2502 in Redis     \u2502                         \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502 Invalidate\u2502\n         \u2502                                   \u2502 Caches    \u2502\n    \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2502 Return       \u2502                               \u2502\n    \u2502 to user      \u2502                         \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502 Release   \u2502\n                                             \u2502 Lock      \u2502\n                                             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```"
  },
  "approaches": [
    {
      "name": "Naive Extension - Add More Servers",
      "description": "Simply horizontally scale the Part 2 solution by adding more application servers and database replicas without architectural changes.",
      "time_complexity": "O(n) for search as load increases",
      "space_complexity": "O(n) memory per replica",
      "why_not_optimal": "Database becomes bottleneck at 100K QPS. Single PostgreSQL can't handle this load even with replicas. Lock contention increases linearly. No caching means every request hits DB. Can't meet 200ms P99 target."
    },
    {
      "name": "Optimal Approach - CQRS with Multi-layer Caching",
      "description": "Separate read path (Elasticsearch + aggressive caching) from write path (sharded PostgreSQL + distributed locks). Use event-driven architecture for cache invalidation and consistency.",
      "time_complexity": "O(1) cache hit, O(log n) cache miss for search; O(1) for booking",
      "space_complexity": "O(hotels * rooms) in Elasticsearch, O(active_searches) in cache",
      "key_insight": "By accepting eventual consistency for searches and caching aggressively, we eliminate 95%+ of database load. The booking path remains strongly consistent with distributed locks, catching any stale availability shown in search."
    }
  ],
  "optimal_solution": {
    "explanation_md": "## CQRS Architecture for 100K QPS\n\n**Core Principle**: Separate concerns completely.\n\n### Read Path (Search)\n1. **First Layer - CDN**: Static assets, common search results\n2. **Second Layer - Redis**: Hot data, recent searches, hotel metadata\n3. **Third Layer - Elasticsearch**: Full-text search, filtered queries\n\n### Write Path (Booking)\n1. **Distributed Lock**: Redis-based lock on room+dates\n2. **Primary Database**: Sharded PostgreSQL by hotel_id\n3. **Event Publishing**: Kafka for async cache invalidation\n\n### Key Optimizations:\n- **Cache-aside pattern** with 5-second TTL for availability\n- **Pre-computed aggregations** for popular searches\n- **Bloom filters** for quick 'definitely not available' checks\n- **Connection pooling** with circuit breakers\n- **Graceful degradation** when services fail",
    "data_structures": [
      {
        "structure": "Redis Cluster",
        "purpose": "Distributed caching with sharding, distributed locks, and pub/sub for invalidation"
      },
      {
        "structure": "Elasticsearch Cluster",
        "purpose": "Scalable full-text search with geo-filtering and real-time indexing"
      },
      {
        "structure": "Kafka Topics",
        "purpose": "Event streaming for booking events, cache invalidation, and cross-service communication"
      },
      {
        "structure": "Consistent Hash Ring",
        "purpose": "Determine which shard handles a hotel_id for both cache and database"
      }
    ],
    "algorithm_steps": [
      "Step 1: Route request to nearest regional deployment via DNS/CDN",
      "Step 2: For search - check cache first, then Elasticsearch, cache result",
      "Step 3: For booking - acquire distributed lock, validate availability, write to DB",
      "Step 4: Publish booking event to Kafka for async processing",
      "Step 5: Event consumers invalidate relevant caches across regions",
      "Step 6: Return response to user with booking confirmation"
    ]
  },
  "solution_python_lines": [
    "\"\"\"",
    "Hotel Booking System - Part 3: Scaling for High Traffic",
    "CQRS Architecture with Multi-layer Caching",
    "\"\"\"",
    "",
    "import asyncio",
    "import hashlib",
    "import json",
    "import time",
    "import threading",
    "import random",
    "from abc import ABC, abstractmethod",
    "from dataclasses import dataclass, field",
    "from datetime import datetime, timedelta",
    "from enum import Enum",
    "from typing import Any, Dict, List, Optional, Set, Tuple, Callable",
    "from collections import defaultdict",
    "from concurrent.futures import ThreadPoolExecutor",
    "import heapq",
    "",
    "",
    "# =============================================================================",
    "# ENUMS AND DATA CLASSES",
    "# =============================================================================",
    "",
    "class BookingStatus(Enum):",
    "    PENDING = \"PENDING\"",
    "    CONFIRMED = \"CONFIRMED\"",
    "    CANCELLED = \"CANCELLED\"",
    "    FAILED = \"FAILED\"",
    "",
    "",
    "class Region(Enum):",
    "    US_EAST = \"us-east\"",
    "    US_WEST = \"us-west\"",
    "    EU_WEST = \"eu-west\"",
    "    ASIA_PACIFIC = \"asia-pacific\"",
    "",
    "",
    "class CacheStrategy(Enum):",
    "    CACHE_ASIDE = \"cache_aside\"",
    "    WRITE_THROUGH = \"write_through\"",
    "    WRITE_BEHIND = \"write_behind\"",
    "",
    "",
    "@dataclass",
    "class Hotel:",
    "    hotel_id: str",
    "    name: str",
    "    location: str",
    "    region: Region",
    "    amenities: List[str] = field(default_factory=list)",
    "    rating: float = 0.0",
    "    price_range: Tuple[float, float] = (0.0, 0.0)",
    "",
    "",
    "@dataclass",
    "class Room:",
    "    room_id: str",
    "    hotel_id: str",
    "    room_type: str",
    "    price_per_night: float",
    "    capacity: int",
    "    amenities: List[str] = field(default_factory=list)",
    "",
    "",
    "@dataclass",
    "class SearchRequest:",
    "    location: str",
    "    check_in: datetime",
    "    check_out: datetime",
    "    guests: int = 1",
    "    min_price: Optional[float] = None",
    "    max_price: Optional[float] = None",
    "    amenities: List[str] = field(default_factory=list)",
    "    cache_hint: str = \"allow_stale\"  # allow_stale, require_fresh",
    "    region_hint: Optional[Region] = None",
    "",
    "",
    "@dataclass",
    "class SearchResult:",
    "    room: Room",
    "    hotel: Hotel",
    "    available: bool",
    "    total_price: float",
    "",
    "",
    "@dataclass",
    "class SearchResponse:",
    "    results: List[SearchResult]",
    "    from_cache: bool",
    "    latency_ms: float",
    "    cache_age_seconds: float = 0.0",
    "    total_count: int = 0",
    "",
    "",
    "@dataclass",
    "class Booking:",
    "    booking_id: str",
    "    user_id: str",
    "    room_id: str",
    "    hotel_id: str",
    "    check_in: datetime",
    "    check_out: datetime",
    "    status: BookingStatus",
    "    total_price: float",
    "    created_at: datetime = field(default_factory=datetime.now)",
    "    version: int = 1",
    "",
    "",
    "@dataclass",
    "class BookingResponse:",
    "    status: BookingStatus",
    "    booking_id: Optional[str]",
    "    latency_ms: float",
    "    message: str = \"\"",
    "",
    "",
    "@dataclass",
    "class CacheEntry:",
    "    data: Any",
    "    created_at: float",
    "    ttl_seconds: float",
    "    version: str",
    "",
    "    def is_expired(self) -> bool:",
    "        return time.time() - self.created_at > self.ttl_seconds",
    "",
    "    def age_seconds(self) -> float:",
    "        return time.time() - self.created_at",
    "",
    "",
    "@dataclass",
    "class Event:",
    "    event_type: str",
    "    payload: Dict[str, Any]",
    "    timestamp: float = field(default_factory=time.time)",
    "    event_id: str = field(default_factory=lambda: f\"evt_{random.randint(100000, 999999)}\")",
    "",
    "",
    "# =============================================================================",
    "# SIMULATED INFRASTRUCTURE COMPONENTS",
    "# =============================================================================",
    "",
    "class RedisClusterSimulator:",
    "    \"\"\"",
    "    Simulates a Redis cluster with sharding and distributed locks.",
    "    In production, use redis-py-cluster or similar.",
    "    \"\"\"",
    "    ",
    "    def __init__(self, num_shards: int = 3):",
    "        self.num_shards = num_shards",
    "        self.shards: List[Dict[str, CacheEntry]] = [",
    "            {} for _ in range(num_shards)",
    "        ]",
    "        self.locks: Dict[str, Tuple[str, float]] = {}  # key -> (owner, expiry)",
    "        self._lock = threading.RLock()",
    "        self.stats = {",
    "            \"hits\": 0,",
    "            \"misses\": 0,",
    "            \"sets\": 0,",
    "            \"deletes\": 0,",
    "            \"lock_acquired\": 0,",
    "            \"lock_failed\": 0,",
    "        }",
    "    ",
    "    def _get_shard(self, key: str) -> int:",
    "        \"\"\"Consistent hashing to determine shard.\"\"\"",
    "        hash_val = int(hashlib.md5(key.encode()).hexdigest(), 16)",
    "        return hash_val % self.num_shards",
    "    ",
    "    def get(self, key: str) -> Optional[Any]:",
    "        \"\"\"Get value from cache. O(1) operation.\"\"\"",
    "        shard_idx = self._get_shard(key)",
    "        with self._lock:",
    "            entry = self.shards[shard_idx].get(key)",
    "            if entry is None:",
    "                self.stats[\"misses\"] += 1",
    "                return None",
    "            if entry.is_expired():",
    "                del self.shards[shard_idx][key]",
    "                self.stats[\"misses\"] += 1",
    "                return None",
    "            self.stats[\"hits\"] += 1",
    "            return entry",
    "    ",
    "    def set(self, key: str, data: Any, ttl_seconds: float = 300, version: str = \"\") -> bool:",
    "        \"\"\"Set value in cache with TTL. O(1) operation.\"\"\"",
    "        shard_idx = self._get_shard(key)",
    "        entry = CacheEntry(",
    "            data=data,",
    "            created_at=time.time(),",
    "            ttl_seconds=ttl_seconds,",
    "            version=version or str(time.time()),",
    "        )",
    "        with self._lock:",
    "            self.shards[shard_idx][key] = entry",
    "            self.stats[\"sets\"] += 1",
    "        return True",
    "    ",
    "    def delete(self, key: str) -> bool:",
    "        \"\"\"Delete key from cache. O(1) operation.\"\"\"",
    "        shard_idx = self._get_shard(key)",
    "        with self._lock:",
    "            if key in self.shards[shard_idx]:",
    "                del self.shards[shard_idx][key]",
    "                self.stats[\"deletes\"] += 1",
    "                return True",
    "        return False",
    "    ",
    "    def delete_pattern(self, pattern_prefix: str) -> int:",
    "        \"\"\"Delete all keys matching prefix. O(n) but necessary for invalidation.\"\"\"",
    "        deleted = 0",
    "        with self._lock:",
    "            for shard in self.shards:",
    "                keys_to_delete = [",
    "                    k for k in shard.keys() if k.startswith(pattern_prefix)",
    "                ]",
    "                for key in keys_to_delete:",
    "                    del shard[key]",
    "                    deleted += 1",
    "        self.stats[\"deletes\"] += deleted",
    "        return deleted",
    "    ",
    "    def acquire_lock(",
    "        self, key: str, owner: str, ttl_seconds: float = 30",
    "    ) -> bool:",
    "        \"\"\"",
    "        Acquire distributed lock. Simulates Redis SETNX + EXPIRE.",
    "        O(1) operation.",
    "        \"\"\"",
    "        lock_key = f\"lock:{key}\"",
    "        current_time = time.time()",
    "        ",
    "        with self._lock:",
    "            # Check if lock exists and is not expired",
    "            if lock_key in self.locks:",
    "                existing_owner, expiry = self.locks[lock_key]",
    "                if current_time < expiry:",
    "                    # Lock is held by someone else",
    "                    if existing_owner != owner:",
    "                        self.stats[\"lock_failed\"] += 1",
    "                        return False",
    "                    # Same owner - extend lock",
    "                    self.locks[lock_key] = (owner, current_time + ttl_seconds)",
    "                    return True",
    "            ",
    "            # Acquire new lock",
    "            self.locks[lock_key] = (owner, current_time + ttl_seconds)",
    "            self.stats[\"lock_acquired\"] += 1",
    "            return True",
    "    ",
    "    def release_lock(self, key: str, owner: str) -> bool:",
    "        \"\"\"Release distributed lock. Only owner can release.\"\"\"",
    "        lock_key = f\"lock:{key}\"",
    "        with self._lock:",
    "            if lock_key in self.locks:",
    "                existing_owner, _ = self.locks[lock_key]",
    "                if existing_owner == owner:",
    "                    del self.locks[lock_key]",
    "                    return True",
    "        return False",
    "    ",
    "    def get_stats(self) -> Dict[str, Any]:",
    "        \"\"\"Get cache statistics.\"\"\"",
    "        total = self.stats[\"hits\"] + self.stats[\"misses\"]",
    "        hit_rate = self.stats[\"hits\"] / total if total > 0 else 0",
    "        return {**self.stats, \"hit_rate\": hit_rate}",
    "",
    "",
    "class ElasticsearchSimulator:",
    "    \"\"\"",
    "    Simulates Elasticsearch for hotel/room search.",
    "    In production, use elasticsearch-py with proper cluster setup.",
    "    \"\"\"",
    "    ",
    "    def __init__(self):",
    "        self.hotels: Dict[str, Hotel] = {}",
    "        self.rooms: Dict[str, Room] = {}",
    "        self.availability: Dict[str, Set[str]] = {}  # date_str -> set of booked room_ids",
    "        self._lock = threading.RLock()",
    "        self.query_count = 0",
    "    ",
    "    def index_hotel(self, hotel: Hotel) -> None:",
    "        \"\"\"Index a hotel document.\"\"\"",
    "        with self._lock:",
    "            self.hotels[hotel.hotel_id] = hotel",
    "    ",
    "    def index_room(self, room: Room) -> None:",
    "        \"\"\"Index a room document.\"\"\"",
    "        with self._lock:",
    "            self.rooms[room.room_id] = room",
    "    ",
    "    def update_availability(self, room_id: str, check_in: datetime, check_out: datetime, is_booked: bool) -> None:",
    "        \"\"\"Update room availability index.\"\"\"",
    "        with self._lock:",
    "            current = check_in",
    "            while current < check_out:",
    "                date_str = current.strftime(\"%Y-%m-%d\")",
    "                if date_str not in self.availability:",
    "                    self.availability[date_str] = set()",
    "                if is_booked:",
    "                    self.availability[date_str].add(room_id)",
    "                else:",
    "                    self.availability[date_str].discard(room_id)",
    "                current += timedelta(days=1)",
    "    ",
    "    def search(",
    "        self,",
    "        location: str,",
    "        check_in: datetime,",
    "        check_out: datetime,",
    "        min_price: Optional[float] = None,",
    "        max_price: Optional[float] = None,",
    "        amenities: Optional[List[str]] = None,",
    "        limit: int = 50,",
    "    ) -> List[Tuple[Room, Hotel, bool]]:",
    "        \"\"\"",
    "        Search for available rooms. Simulates ES query.",
    "        O(log n) with proper indexing in real ES.",
    "        \"\"\"",
    "        self.query_count += 1",
    "        results = []",
    "        ",
    "        # Simulate query latency (50-100ms for ES)",
    "        time.sleep(random.uniform(0.05, 0.1))",
    "        ",
    "        with self._lock:",
    "            # Find hotels in location",
    "            matching_hotels = {",
    "                h.hotel_id: h for h in self.hotels.values()",
    "                if location.lower() in h.location.lower()",
    "            }",
    "            ",
    "            # Find rooms in matching hotels",
    "            for room in self.rooms.values():",
    "                if room.hotel_id not in matching_hotels:",
    "                    continue",
    "                ",
    "                # Price filter",
    "                if min_price and room.price_per_night < min_price:",
    "                    continue",
    "                if max_price and room.price_per_night > max_price:",
    "                    continue",
    "                ",
    "                # Amenity filter",
    "                if amenities:",
    "                    if not all(a in room.amenities for a in amenities):",
    "                        continue",
    "                ",
    "                # Check availability",
    "                is_available = self._check_availability(",
    "                    room.room_id, check_in, check_out",
    "                )",
    "                ",
    "                hotel = matching_hotels[room.hotel_id]",
    "                results.append((room, hotel, is_available))",
    "                ",
    "                if len(results) >= limit:",
    "                    break",
    "        ",
    "        return results",
    "    ",
    "    def _check_availability(",
    "        self, room_id: str, check_in: datetime, check_out: datetime",
    "    ) -> bool:",
    "        \"\"\"Check if room is available for date range.\"\"\"",
    "        current = check_in",
    "        while current < check_out:",
    "            date_str = current.strftime(\"%Y-%m-%d\")",
    "            if date_str in self.availability:",
    "                if room_id in self.availability[date_str]:",
    "                    return False",
    "            current += timedelta(days=1)",
    "        return True",
    "",
    "",
    "class KafkaSimulator:",
    "    \"\"\"",
    "    Simulates Kafka for event streaming.",
    "    In production, use confluent-kafka-python.",
    "    \"\"\"",
    "    ",
    "    def __init__(self):",
    "        self.topics: Dict[str, List[Event]] = defaultdict(list)",
    "        self.consumers: Dict[str, List[Callable[[Event], None]]] = defaultdict(list)",
    "        self._lock = threading.RLock()",
    "        self.executor = ThreadPoolExecutor(max_workers=4)",
    "    ",
    "    def publish(self, topic: str, event: Event) -> None:",
    "        \"\"\"Publish event to topic.\"\"\"",
    "        with self._lock:",
    "            self.topics[topic].append(event)",
    "            # Notify consumers asynchronously",
    "            for consumer in self.consumers[topic]:",
    "                self.executor.submit(consumer, event)",
    "    ",
    "    def subscribe(self, topic: str, callback: Callable[[Event], None]) -> None:",
    "        \"\"\"Subscribe to topic with callback.\"\"\"",
    "        with self._lock:",
    "            self.consumers[topic].append(callback)",
    "    ",
    "    def get_events(self, topic: str) -> List[Event]:",
    "        \"\"\"Get all events from topic.\"\"\"",
    "        with self._lock:",
    "            return list(self.topics[topic])",
    "",
    "",
    "class PostgreSQLSimulator:",
    "    \"\"\"",
    "    Simulates sharded PostgreSQL for booking storage.",
    "    In production, use psycopg2 with connection pooling.",
    "    \"\"\"",
    "    ",
    "    def __init__(self, num_shards: int = 3):",
    "        self.num_shards = num_shards",
    "        self.shards: List[Dict[str, Booking]] = [",
    "            {} for _ in range(num_shards)",
    "        ]",
    "        self.room_bookings: List[Dict[str, List[Tuple[datetime, datetime]]]] = [",
    "            defaultdict(list) for _ in range(num_shards)",
    "        ]",
    "        self._locks = [threading.RLock() for _ in range(num_shards)]",
    "        self.query_count = 0",
    "    ",
    "    def _get_shard(self, hotel_id: str) -> int:",
    "        \"\"\"Shard by hotel_id for data locality.\"\"\"",
    "        hash_val = int(hashlib.md5(hotel_id.encode()).hexdigest(), 16)",
    "        return hash_val % self.num_shards",
    "    ",
    "    def create_booking(",
    "        self, booking: Booking",
    "    ) -> Tuple[bool, str]:",
    "        \"\"\"",
    "        Create a booking with availability check.",
    "        Uses optimistic locking within the shard.",
    "        \"\"\"",
    "        shard_idx = self._get_shard(booking.hotel_id)",
    "        ",
    "        # Simulate DB latency (20-50ms)",
    "        time.sleep(random.uniform(0.02, 0.05))",
    "        self.query_count += 1",
    "        ",
    "        with self._locks[shard_idx]:",
    "            # Check for conflicts",
    "            existing = self.room_bookings[shard_idx].get(booking.room_id, [])",
    "            for start, end in existing:",
    "                if booking.check_in < end and booking.check_out > start:",
    "                    return False, \"Room already booked for these dates\"",
    "            ",
    "            # Create booking",
    "            self.shards[shard_idx][booking.booking_id] = booking",
    "            self.room_bookings[shard_idx][booking.room_id].append(",
    "                (booking.check_in, booking.check_out)",
    "            )",
    "            ",
    "            return True, \"Booking created successfully\"",
    "    ",
    "    def get_booking(self, booking_id: str, hotel_id: str) -> Optional[Booking]:",
    "        \"\"\"Get booking by ID.\"\"\"",
    "        shard_idx = self._get_shard(hotel_id)",
    "        self.query_count += 1",
    "        with self._locks[shard_idx]:",
    "            return self.shards[shard_idx].get(booking_id)",
    "    ",
    "    def cancel_booking(",
    "        self, booking_id: str, hotel_id: str",
    "    ) -> Tuple[bool, str]:",
    "        \"\"\"Cancel a booking.\"\"\"",
    "        shard_idx = self._get_shard(hotel_id)",
    "        self.query_count += 1",
    "        ",
    "        with self._locks[shard_idx]:",
    "            booking = self.shards[shard_idx].get(booking_id)",
    "            if not booking:",
    "                return False, \"Booking not found\"",
    "            ",
    "            booking.status = BookingStatus.CANCELLED",
    "            ",
    "            # Remove from room bookings",
    "            room_list = self.room_bookings[shard_idx].get(booking.room_id, [])",
    "            self.room_bookings[shard_idx][booking.room_id] = [",
    "                (s, e) for s, e in room_list",
    "                if not (s == booking.check_in and e == booking.check_out)",
    "            ]",
    "            ",
    "            return True, \"Booking cancelled\"",
    "",
    "",
    "# =============================================================================",
    "# CQRS SERVICES",
    "# =============================================================================",
    "",
    "class CacheManager:",
    "    \"\"\"",
    "    Manages multi-layer caching with intelligent invalidation.",
    "    Implements cache-aside pattern with TTL and event-driven invalidation.",
    "    \"\"\"",
    "    ",
    "    # Cache TTL configuration",
    "    TTL_HOTEL_METADATA = 3600  # 1 hour - rarely changes",
    "    TTL_ROOM_METADATA = 1800   # 30 minutes",
    "    TTL_SEARCH_RESULTS = 5     # 5 seconds - availability changes frequently",
    "    TTL_AVAILABILITY = 10     # 10 seconds",
    "    ",
    "    def __init__(self, redis: RedisClusterSimulator):",
    "        self.redis = redis",
    "    ",
    "    def _make_search_key(self, request: SearchRequest) -> str:",
    "        \"\"\"Generate cache key for search request.\"\"\"",
    "        key_parts = [",
    "            f\"search:{request.location}\",",
    "            request.check_in.strftime(\"%Y%m%d\"),",
    "            request.check_out.strftime(\"%Y%m%d\"),",
    "            str(request.min_price or 0),",
    "            str(request.max_price or 0),",
    "            \",\".join(sorted(request.amenities)) if request.amenities else \"\",",
    "        ]",
    "        return \":\".join(key_parts)",
    "    ",
    "    def get_search_results(",
    "        self, request: SearchRequest",
    "    ) -> Optional[Tuple[List[SearchResult], float]]:",
    "        \"\"\"Get cached search results. Returns (results, cache_age) or None.\"\"\"",
    "        key = self._make_search_key(request)",
    "        entry = self.redis.get(key)",
    "        if entry:",
    "            return entry.data, entry.age_seconds()",
    "        return None",
    "    ",
    "    def set_search_results(",
    "        self, request: SearchRequest, results: List[SearchResult]",
    "    ) -> None:",
    "        \"\"\"Cache search results with short TTL.\"\"\"",
    "        key = self._make_search_key(request)",
    "        self.redis.set(key, results, ttl_seconds=self.TTL_SEARCH_RESULTS)",
    "    ",
    "    def get_hotel(self, hotel_id: str) -> Optional[Hotel]:",
    "        \"\"\"Get cached hotel metadata.\"\"\"",
    "        entry = self.redis.get(f\"hotel:{hotel_id}\")",
    "        return entry.data if entry else None",
    "    ",
    "    def set_hotel(self, hotel: Hotel) -> None:",
    "        \"\"\"Cache hotel metadata with long TTL.\"\"\"",
    "        self.redis.set(",
    "            f\"hotel:{hotel.hotel_id}\",",
    "            hotel,",
    "            ttl_seconds=self.TTL_HOTEL_METADATA,",
    "        )",
    "    ",
    "    def invalidate_search_cache(self, location: str) -> int:",
    "        \"\"\"",
    "        Invalidate all search caches for a location.",
    "        Called when booking changes availability.",
    "        \"\"\"",
    "        return self.redis.delete_pattern(f\"search:{location}\")",
    "    ",
    "    def invalidate_room_availability(self, room_id: str) -> None:",
    "        \"\"\"Invalidate availability cache for a specific room.\"\"\"",
    "        self.redis.delete(f\"availability:{room_id}\")",
    "",
    "",
    "class SearchService:",
    "    \"\"\"",
    "    Read path service optimized for high QPS search.",
    "    Uses Elasticsearch with caching layer.",
    "    \"\"\"",
    "    ",
    "    def __init__(",
    "        self,",
    "        elasticsearch: ElasticsearchSimulator,",
    "        cache_manager: CacheManager,",
    "    ):",
    "        self.es = elasticsearch",
    "        self.cache = cache_manager",
    "        self.request_count = 0",
    "        self.cache_hits = 0",
    "    ",
    "    async def search_rooms_optimized(",
    "        self, request: SearchRequest",
    "    ) -> SearchResponse:",
    "        \"\"\"",
    "        Optimized search with caching.",
    "        ",
    "        Flow:",
    "        1. Check cache (O(1))",
    "        2. If miss, query Elasticsearch (O(log n))",
    "        3. Cache results with short TTL",
    "        4. Return results",
    "        ",
    "        Args:",
    "            request: Search criteria with cache hints",
    "            ",
    "        Returns:",
    "            SearchResponse with results and metadata",
    "        \"\"\"",
    "        start_time = time.time()",
    "        self.request_count += 1",
    "        ",
    "        # Check if fresh data required",
    "        allow_cache = request.cache_hint != \"require_fresh\"",
    "        ",
    "        # Step 1: Try cache",
    "        if allow_cache:",
    "            cached = self.cache.get_search_results(request)",
    "            if cached:",
    "                results, cache_age = cached",
    "                self.cache_hits += 1",
    "                latency = (time.time() - start_time) * 1000",
    "                return SearchResponse(",
    "                    results=results,",
    "                    from_cache=True,",
    "                    latency_ms=latency,",
    "                    cache_age_seconds=cache_age,",
    "                    total_count=len(results),",
    "                )",
    "        ",
    "        # Step 2: Query Elasticsearch",
    "        es_results = self.es.search(",
    "            location=request.location,",
    "            check_in=request.check_in,",
    "            check_out=request.check_out,",
    "            min_price=request.min_price,",
    "            max_price=request.max_price,",
    "            amenities=request.amenities,",
    "        )",
    "        ",
    "        # Transform results",
    "        nights = (request.check_out - request.check_in).days",
    "        search_results = [",
    "            SearchResult(",
    "                room=room,",
    "                hotel=hotel,",
    "                available=available,",
    "                total_price=room.price_per_night * nights,",
    "            )",
    "            for room, hotel, available in es_results",
    "        ]",
    "        ",
    "        # Step 3: Cache results",
    "        self.cache.set_search_results(request, search_results)",
    "        ",
    "        latency = (time.time() - start_time) * 1000",
    "        return SearchResponse(",
    "            results=search_results,",
    "            from_cache=False,",
    "            latency_ms=latency,",
    "            total_count=len(search_results),",
    "        )",
    "    ",
    "    def get_cache_hit_rate(self) -> float:",
    "        \"\"\"Get cache hit rate for monitoring.\"\"\"",
    "        if self.request_count == 0:",
    "            return 0.0",
    "        return self.cache_hits / self.request_count",
    "",
    "",
    "class BookingService:",
    "    \"\"\"",
    "    Write path service with strong consistency guarantees.",
    "    Uses distributed locking and event publishing.",
    "    \"\"\"",
    "    ",
    "    LOCK_TTL_SECONDS = 30",
    "    ",
    "    def __init__(",
    "        self,",
    "        database: PostgreSQLSimulator,",
    "        redis: RedisClusterSimulator,",
    "        kafka: KafkaSimulator,",
    "        elasticsearch: ElasticsearchSimulator,",
    "        cache_manager: CacheManager,",
    "    ):",
    "        self.db = database",
    "        self.redis = redis",
    "        self.kafka = kafka",
    "        self.es = elasticsearch",
    "        self.cache = cache_manager",
    "        self.booking_count = 0",
    "    ",
    "    def _generate_booking_id(self) -> str:",
    "        \"\"\"Generate unique booking ID.\"\"\"",
    "        return f\"BK_{int(time.time() * 1000)}_{random.randint(1000, 9999)}\"",
    "    ",
    "    def _make_lock_key(",
    "        self, room_id: str, check_in: datetime, check_out: datetime",
    "    ) -> str:",
    "        \"\"\"Generate lock key for room+date range.\"\"\"",
    "        return f\"{room_id}:{check_in.strftime('%Y%m%d')}:{check_out.strftime('%Y%m%d')}\"",
    "    ",
    "    async def create_booking(",
    "        self,",
    "        user_id: str,",
    "        room_id: str,",
    "        hotel_id: str,",
    "        check_in: datetime,",
    "        check_out: datetime,",
    "        price_per_night: float,",
    "    ) -> BookingResponse:",
    "        \"\"\"",
    "        Create booking with distributed locking.",
    "        ",
    "        Flow:",
    "        1. Acquire distributed lock (prevents race conditions)",
    "        2. Validate availability in database",
    "        3. Create booking record",
    "        4. Publish event to Kafka",
    "        5. Invalidate caches",
    "        6. Release lock",
    "        ",
    "        Time complexity: O(1) with proper sharding",
    "        ",
    "        Args:",
    "            user_id: User making the booking",
    "            room_id: Room to book",
    "            hotel_id: Hotel containing the room",
    "            check_in: Check-in date",
    "            check_out: Check-out date",
    "            price_per_night: Price per night",
    "            ",
    "        Returns:",
    "            BookingResponse with status and booking ID",
    "        \"\"\"",
    "        start_time = time.time()",
    "        self.booking_count += 1",
    "        ",
    "        lock_key = self._make_lock_key(room_id, check_in, check_out)",
    "        lock_owner = f\"{user_id}_{time.time()}\"",
    "        ",
    "        # Step 1: Acquire distributed lock",
    "        if not self.redis.acquire_lock(",
    "            lock_key, lock_owner, ttl_seconds=self.LOCK_TTL_SECONDS",
    "        ):",
    "            latency = (time.time() - start_time) * 1000",
    "            return BookingResponse(",
    "                status=BookingStatus.FAILED,",
    "                booking_id=None,",
    "                latency_ms=latency,",
    "                message=\"Could not acquire lock - room is being booked\",",
    "            )",
    "        ",
    "        try:",
    "            # Step 2 & 3: Create booking in database",
    "            nights = (check_out - check_in).days",
    "            booking = Booking(",
    "                booking_id=self._generate_booking_id(),",
    "                user_id=user_id,",
    "                room_id=room_id,",
    "                hotel_id=hotel_id,",
    "                check_in=check_in,",
    "                check_out=check_out,",
    "                status=BookingStatus.CONFIRMED,",
    "                total_price=price_per_night * nights,",
    "            )",
    "            ",
    "            success, message = self.db.create_booking(booking)",
    "            ",
    "            if not success:",
    "                latency = (time.time() - start_time) * 1000",
    "                return BookingResponse(",
    "                    status=BookingStatus.FAILED,",
    "                    booking_id=None,",
    "                    latency_ms=latency,",
    "                    message=message,",
    "                )",
    "            ",
    "            # Step 4: Publish event for async processing",
    "            event = Event(",
    "                event_type=\"BOOKING_CREATED\",",
    "                payload={",
    "                    \"booking_id\": booking.booking_id,",
    "                    \"room_id\": room_id,",
    "                    \"hotel_id\": hotel_id,",
    "                    \"check_in\": check_in.isoformat(),",
    "                    \"check_out\": check_out.isoformat(),",
    "                },",
    "            )",
    "            self.kafka.publish(\"bookings\", event)",
    "            ",
    "            # Step 5: Update Elasticsearch (async in production)",
    "            self.es.update_availability(",
    "                room_id, check_in, check_out, is_booked=True",
    "            )",
    "            ",
    "            latency = (time.time() - start_time) * 1000",
    "            return BookingResponse(",
    "                status=BookingStatus.CONFIRMED,",
    "                booking_id=booking.booking_id,",
    "                latency_ms=latency,",
    "                message=\"Booking confirmed successfully\",",
    "            )",
    "            ",
    "        finally:",
    "            # Step 6: Always release lock",
    "            self.redis.release_lock(lock_key, lock_owner)",
    "    ",
    "    async def cancel_booking(",
    "        self, booking_id: str, hotel_id: str, user_id: str",
    "    ) -> BookingResponse:",
    "        \"\"\"Cancel an existing booking.\"\"\"",
    "        start_time = time.time()",
    "        ",
    "        # Get booking first",
    "        booking = self.db.get_booking(booking_id, hotel_id)",
    "        if not booking:",
    "            return BookingResponse(",
    "                status=BookingStatus.FAILED,",
    "                booking_id=booking_id,",
    "                latency_ms=(time.time() - start_time) * 1000,",
    "                message=\"Booking not found\",",
    "            )",
    "        ",
    "        if booking.user_id != user_id:",
    "            return BookingResponse(",
    "                status=BookingStatus.FAILED,",
    "                booking_id=booking_id,",
    "                latency_ms=(time.time() - start_time) * 1000,",
    "                message=\"Unauthorized - not your booking\",",
    "            )",
    "        ",
    "        # Cancel in database",
    "        success, message = self.db.cancel_booking(booking_id, hotel_id)",
    "        ",
    "        if success:",
    "            # Publish cancellation event",
    "            event = Event(",
    "                event_type=\"BOOKING_CANCELLED\",",
    "                payload={",
    "                    \"booking_id\": booking_id,",
    "                    \"room_id\": booking.room_id,",
    "                    \"hotel_id\": hotel_id,",
    "                    \"check_in\": booking.check_in.isoformat(),",
    "                    \"check_out\": booking.check_out.isoformat(),",
    "                },",
    "            )",
    "            self.kafka.publish(\"bookings\", event)",
    "            ",
    "            # Update Elasticsearch",
    "            self.es.update_availability(",
    "                booking.room_id, booking.check_in, booking.check_out,",
    "                is_booked=False",
    "            )",
    "        ",
    "        return BookingResponse(",
    "            status=BookingStatus.CANCELLED if success else BookingStatus.FAILED,",
    "            booking_id=booking_id,",
    "            latency_ms=(time.time() - start_time) * 1000,",
    "            message=message,",
    "        )",
    "",
    "",
    "class EventConsumer:",
    "    \"\"\"",
    "    Consumes booking events for cache invalidation and analytics.",
    "    \"\"\"",
    "    ",
    "    def __init__(",
    "        self,",
    "        kafka: KafkaSimulator,",
    "        cache_manager: CacheManager,",
    "        elasticsearch: ElasticsearchSimulator,",
    "    ):",
    "        self.kafka = kafka",
    "        self.cache = cache_manager",
    "        self.es = elasticsearch",
    "        self.processed_events = 0",
    "        ",
    "        # Subscribe to booking events",
    "        self.kafka.subscribe(\"bookings\", self._handle_booking_event)",
    "    ",
    "    def _handle_booking_event(self, event: Event) -> None:",
    "        \"\"\"Handle booking event - invalidate caches.\"\"\"",
    "        self.processed_events += 1",
    "        ",
    "        room_id = event.payload.get(\"room_id\")",
    "        hotel_id = event.payload.get(\"hotel_id\")",
    "        ",
    "        if room_id:",
    "            self.cache.invalidate_room_availability(room_id)",
    "        ",
    "        # In production, would look up hotel location",
    "        # For now, invalidate all NYC searches as demo",
    "        self.cache.invalidate_search_cache(\"NYC\")",
    "        self.cache.invalidate_search_cache(\"SF\")",
    "",
    "",
    "class GeoRouter:",
    "    \"\"\"",
    "    Routes requests to the nearest regional deployment.",
    "    In production, this would be DNS-based (e.g., Route 53).",
    "    \"\"\"",
    "    ",
    "    REGION_LATENCIES = {",
    "        (Region.US_EAST, \"NYC\"): 10,",
    "        (Region.US_EAST, \"Boston\"): 15,",
    "        (Region.US_WEST, \"SF\"): 10,",
    "        (Region.US_WEST, \"LA\"): 15,",
    "        (Region.EU_WEST, \"London\"): 10,",
    "        (Region.EU_WEST, \"Paris\"): 15,",
    "    }",
    "    ",
    "    def __init__(self):",
    "        self.routing_decisions = 0",
    "    ",
    "    def get_best_region(self, location: str) -> Region:",
    "        \"\"\"Determine best region for a location query.\"\"\"",
    "        self.routing_decisions += 1",
    "        ",
    "        location_lower = location.lower()",
    "        ",
    "        # Simple routing logic",
    "        if any(x in location_lower for x in [\"nyc\", \"new york\", \"boston\", \"miami\"]):",
    "            return Region.US_EAST",
    "        elif any(x in location_lower for x in [\"sf\", \"san francisco\", \"la\", \"seattle\"]):",
    "            return Region.US_WEST",
    "        elif any(x in location_lower for x in [\"london\", \"paris\", \"berlin\"]):",
    "            return Region.EU_WEST",
    "        else:",
    "            return Region.US_EAST  # Default",
    "",
    "",
    "# =============================================================================",
    "# MAIN HOTEL BOOKING SYSTEM - SCALED VERSION",
    "# =============================================================================",
    "",
    "class ScaledHotelBookingSystem:",
    "    \"\"\"",
    "    Complete hotel booking system with CQRS architecture.",
    "    Optimized for 100K search QPS and 10K booking QPS.",
    "    ",
    "    Architecture:",
    "    - Read Path: CDN -> Redis Cache -> Elasticsearch",
    "    - Write Path: Redis Lock -> PostgreSQL -> Kafka -> Cache Invalidation",
    "    \"\"\"",
    "    ",
    "    def __init__(self):",
    "        # Infrastructure components",
    "        self.redis = RedisClusterSimulator(num_shards=3)",
    "        self.elasticsearch = ElasticsearchSimulator()",
    "        self.postgresql = PostgreSQLSimulator(num_shards=3)",
    "        self.kafka = KafkaSimulator()",
    "        ",
    "        # Service layer",
    "        self.cache_manager = CacheManager(self.redis)",
    "        self.search_service = SearchService(",
    "            self.elasticsearch, self.cache_manager",
    "        )",
    "        self.booking_service = BookingService(",
    "            self.postgresql,",
    "            self.redis,",
    "            self.kafka,",
    "            self.elasticsearch,",
    "            self.cache_manager,",
    "        )",
    "        self.event_consumer = EventConsumer(",
    "            self.kafka, self.cache_manager, self.elasticsearch",
    "        )",
    "        self.geo_router = GeoRouter()",
    "        ",
    "        # Sample data",
    "        self._setup_sample_data()",
    "    ",
    "    def _setup_sample_data(self) -> None:",
    "        \"\"\"Initialize with sample hotels and rooms.\"\"\"",
    "        hotels = [",
    "            Hotel(",
    "                hotel_id=\"H_NYC_1\",",
    "                name=\"Manhattan Grand Hotel\",",
    "                location=\"NYC, New York\",",
    "                region=Region.US_EAST,",
    "                amenities=[\"wifi\", \"pool\", \"gym\"],",
    "                rating=4.5,",
    "                price_range=(150, 400),",
    "            ),",
    "            Hotel(",
    "                hotel_id=\"H_NYC_2\",",
    "                name=\"Times Square Inn\",",
    "                location=\"NYC, New York\",",
    "                region=Region.US_EAST,",
    "                amenities=[\"wifi\", \"breakfast\"],",
    "                rating=4.0,",
    "                price_range=(100, 250),",
    "            ),",
    "            Hotel(",
    "                hotel_id=\"H_SF_1\",",
    "                name=\"Golden Gate Suites\",",
    "                location=\"SF, California\",",
    "                region=Region.US_WEST,",
    "                amenities=[\"wifi\", \"pool\", \"spa\"],",
    "                rating=4.7,",
    "                price_range=(200, 500),",
    "            ),",
    "        ]",
    "        ",
    "        rooms = [",
    "            Room(\"R_NYC_1\", \"H_NYC_1\", \"Deluxe King\", 250, 2, [\"wifi\", \"minibar\"]),",
    "            Room(\"R_NYC_2\", \"H_NYC_1\", \"Standard Queen\", 180, 2, [\"wifi\"]),",
    "            Room(\"R_NYC_3\", \"H_NYC_2\", \"Budget Single\", 120, 1, [\"wifi\"]),",
    "            Room(\"R_SF_1\", \"H_SF_1\", \"Ocean View Suite\", 350, 2, [\"wifi\", \"balcony\"]),",
    "            Room(\"R_SF_2\", \"H_SF_1\", \"City View Room\", 280, 2, [\"wifi\"]),",
    "        ]",
    "        ",
    "        # Index in Elasticsearch",
    "        for hotel in hotels:",
    "            self.elasticsearch.index_hotel(hotel)",
    "            self.cache_manager.set_hotel(hotel)",
    "        ",
    "        for room in rooms:",
    "            self.elasticsearch.index_room(room)",
    "    ",
    "    async def search_rooms(",
    "        self, request: SearchRequest",
    "    ) -> SearchResponse:",
    "        \"\"\"",
    "        Search for available rooms - read path.",
    "        ",
    "        Optimizations:",
    "        - Geographic routing to nearest region",
    "        - Multi-layer caching (CDN -> Redis -> ES)",
    "        - Eventual consistency acceptable",
    "        \"\"\"",
    "        # Route to best region",
    "        region = self.geo_router.get_best_region(request.location)",
    "        request.region_hint = region",
    "        ",
    "        return await self.search_service.search_rooms_optimized(request)",
    "    ",
    "    async def create_booking(",
    "        self,",
    "        user_id: str,",
    "        room_id: str,",
    "        hotel_id: str,",
    "        check_in: datetime,",
    "        check_out: datetime,",
    "    ) -> BookingResponse:",
    "        \"\"\"",
    "        Create a booking - write path.",
    "        ",
    "        Guarantees:",
    "        - Strong consistency via distributed lock",
    "        - No double bookings",
    "        - Event published for cache invalidation",
    "        \"\"\"",
    "        # Look up room price",
    "        room = self.elasticsearch.rooms.get(room_id)",
    "        if not room:",
    "            return BookingResponse(",
    "                status=BookingStatus.FAILED,",
    "                booking_id=None,",
    "                latency_ms=0,",
    "                message=\"Room not found\",",
    "            )",
    "        ",
    "        return await self.booking_service.create_booking(",
    "            user_id, room_id, hotel_id, check_in, check_out,",
    "            room.price_per_night,",
    "        )",
    "    ",
    "    async def cancel_booking(",
    "        self, booking_id: str, hotel_id: str, user_id: str",
    "    ) -> BookingResponse:",
    "        \"\"\"Cancel an existing booking.\"\"\"",
    "        return await self.booking_service.cancel_booking(",
    "            booking_id, hotel_id, user_id",
    "        )",
    "    ",
    "    def get_system_stats(self) -> Dict[str, Any]:",
    "        \"\"\"Get comprehensive system statistics.\"\"\"",
    "        return {",
    "            \"cache_stats\": self.redis.get_stats(),",
    "            \"search_requests\": self.search_service.request_count,",
    "            \"search_cache_hit_rate\": self.search_service.get_cache_hit_rate(),",
    "            \"booking_requests\": self.booking_service.booking_count,",
    "            \"es_queries\": self.elasticsearch.query_count,",
    "            \"db_queries\": self.postgresql.query_count,",
    "            \"events_processed\": self.event_consumer.processed_events,",
    "            \"routing_decisions\": self.geo_router.routing_decisions,",
    "        }",
    "",
    "",
    "# =============================================================================",
    "# LOAD TESTING AND DEMONSTRATION",
    "# =============================================================================",
    "",
    "async def run_concurrent_searches(",
    "    system: ScaledHotelBookingSystem, num_requests: int",
    ") -> Dict[str, float]:",
    "    \"\"\"Simulate concurrent search load.\"\"\"",
    "    latencies = []",
    "    ",
    "    async def single_search():",
    "        request = SearchRequest(",
    "            location=\"NYC\",",
    "            check_in=datetime(2024, 9, 1),",
    "            check_out=datetime(2024, 9, 3),",
    "            guests=2,",
    "        )",
    "        response = await system.search_rooms(request)",
    "        latencies.append(response.latency_ms)",
    "    ",
    "    # Run concurrent searches",
    "    tasks = [single_search() for _ in range(num_requests)]",
    "    await asyncio.gather(*tasks)",
    "    ",
    "    # Calculate percentiles",
    "    latencies.sort()",
    "    return {",
    "        \"p50\": latencies[len(latencies) // 2] if latencies else 0,",
    "        \"p90\": latencies[int(len(latencies) * 0.9)] if latencies else 0,",
    "        \"p99\": latencies[int(len(latencies) * 0.99)] if latencies else 0,",
    "        \"total_requests\": len(latencies),",
    "    }",
    "",
    "",
    "async def run_concurrent_bookings(",
    "    system: ScaledHotelBookingSystem, num_requests: int",
    ") -> Dict[str, Any]:",
    "    \"\"\"Simulate concurrent booking attempts for the same room.\"\"\"",
    "    results = []",
    "    ",
    "    async def single_booking(user_num: int):",
    "        response = await system.create_booking(",
    "            user_id=f\"USER_{user_num}\",",
    "            room_id=\"R_NYC_1\",",
    "            hotel_id=\"H_NYC_1\",",
    "            check_in=datetime(2024, 10, 1),",
    "            check_out=datetime(2024, 10, 3),",
    "        )",
    "        results.append({",
    "            \"user\": f\"USER_{user_num}\",",
    "            \"status\": response.status.value,",
    "            \"latency\": response.latency_ms,",
    "        })",
    "    ",
    "    # Run concurrent bookings",
    "    tasks = [single_booking(i) for i in range(num_requests)]",
    "    await asyncio.gather(*tasks)",
    "    ",
    "    # Analyze results",
    "    confirmed = sum(1 for r in results if r[\"status\"] == \"CONFIRMED\")",
    "    failed = sum(1 for r in results if r[\"status\"] == \"FAILED\")",
    "    ",
    "    return {",
    "        \"confirmed_count\": confirmed,",
    "        \"failed_count\": failed,",
    "        \"results\": results,",
    "    }",
    "",
    "",
    "async def main():",
    "    \"\"\"Demonstrate the scaled hotel booking system.\"\"\"",
    "    print(\"\\n\" + \"=\" * 70)",
    "    print(\"   HOTEL BOOKING SYSTEM - PART 3: SCALING FOR HIGH TRAFFIC\")",
    "    print(\"=\" * 70)",
    "    ",
    "    # Initialize system",
    "    system = ScaledHotelBookingSystem()",
    "    ",
    "    # =================================================================",
    "    # DEMO 1: Search with caching",
    "    # =================================================================",
    "    print(\"\\n\" + \"-\" * 50)",
    "    print(\"DEMO 1: Search with Multi-layer Caching\")",
    "    print(\"-\" * 50)",
    "    ",
    "    request = SearchRequest(",
    "        location=\"NYC\",",
    "        check_in=datetime(2024, 9, 1),",
    "        check_out=datetime(2024, 9, 3),",
    "        guests=2,",
    "    )",
    "    ",
    "    # First search - cache miss",
    "    print(\"\\n[Search 1] First search (cache miss expected)...\")",
    "    response1 = await system.search_rooms(request)",
    "    print(f\"  Results: {len(response1.results)} rooms found\")",
    "    print(f\"  From cache: {response1.from_cache}\")",
    "    print(f\"  Latency: {response1.latency_ms:.2f}ms\")",
    "    ",
    "    # Second search - cache hit",
    "    print(\"\\n[Search 2] Same search (cache hit expected)...\")",
    "    response2 = await system.search_rooms(request)",
    "    print(f\"  Results: {len(response2.results)} rooms found\")",
    "    print(f\"  From cache: {response2.from_cache}\")",
    "    print(f\"  Latency: {response2.latency_ms:.2f}ms\")",
    "    print(f\"  Cache age: {response2.cache_age_seconds:.3f}s\")",
    "    ",
    "    if response1.latency_ms > 0:",
    "        speedup = response1.latency_ms / max(response2.latency_ms, 0.001)",
    "        print(f\"  Speedup: {speedup:.1f}x faster with cache!\")",
    "    ",
    "    # =================================================================",
    "    # DEMO 2: Concurrent booking prevention",
    "    # =================================================================",
    "    print(\"\\n\" + \"-\" * 50)",
    "    print(\"DEMO 2: Concurrent Booking Race Condition Prevention\")",
    "    print(\"-\" * 50)",
    "    ",
    "    print(\"\\nSimulating 5 users trying to book the same room...\")",
    "    booking_results = await run_concurrent_bookings(system, 5)",
    "    ",
    "    print(f\"\\nResults:\")",
    "    print(f\"  Confirmed: {booking_results['confirmed_count']} (should be 1)\")",
    "    print(f\"  Failed: {booking_results['failed_count']} (should be 4)\")",
    "    ",
    "    for result in booking_results[\"results\"]:",
    "        status_icon = \"\\u2713\" if result[\"status\"] == \"CONFIRMED\" else \"\\u2717\"",
    "        print(f\"  {status_icon} {result['user']}: {result['status']} ({result['latency']:.1f}ms)\")",
    "    ",
    "    # =================================================================",
    "    # DEMO 3: Load test",
    "    # =================================================================",
    "    print(\"\\n\" + \"-\" * 50)",
    "    print(\"DEMO 3: Load Test - Concurrent Searches\")",
    "    print(\"-\" * 50)",
    "    ",
    "    print(\"\\nRunning 100 concurrent searches...\")",
    "    load_results = await run_concurrent_searches(system, 100)",
    "    ",
    "    print(f\"\\nLatency Percentiles:\")",
    "    print(f\"  P50: {load_results['p50']:.2f}ms\")",
    "    print(f\"  P90: {load_results['p90']:.2f}ms\")",
    "    print(f\"  P99: {load_results['p99']:.2f}ms\")",
    "    print(f\"  Total requests: {load_results['total_requests']}\")",
    "    ",
    "    # =================================================================",
    "    # DEMO 4: System statistics",
    "    # =================================================================",
    "    print(\"\\n\" + \"-\" * 50)",
    "    print(\"DEMO 4: System Statistics\")",
    "    print(\"-\" * 50)",
    "    ",
    "    stats = system.get_system_stats()",
    "    print(\"\\nCache Statistics:\")",
    "    print(f\"  Hits: {stats['cache_stats']['hits']}\")",
    "    print(f\"  Misses: {stats['cache_stats']['misses']}\")",
    "    print(f\"  Hit Rate: {stats['cache_stats']['hit_rate']:.1%}\")",
    "    print(f\"  Locks Acquired: {stats['cache_stats']['lock_acquired']}\")",
    "    print(f\"  Locks Failed: {stats['cache_stats']['lock_failed']}\")",
    "    ",
    "    print(\"\\nService Statistics:\")",
    "    print(f\"  Search Requests: {stats['search_requests']}\")",
    "    print(f\"  Search Cache Hit Rate: {stats['search_cache_hit_rate']:.1%}\")",
    "    print(f\"  Booking Requests: {stats['booking_requests']}\")",
    "    print(f\"  Elasticsearch Queries: {stats['es_queries']}\")",
    "    print(f\"  Database Queries: {stats['db_queries']}\")",
    "    print(f\"  Events Processed: {stats['events_processed']}\")",
    "    ",
    "    # =================================================================",
    "    # Architecture Summary",
    "    # =================================================================",
    "    print(\"\\n\" + \"=\" * 70)",
    "    print(\"   ARCHITECTURE SUMMARY\")",
    "    print(\"=\" * 70)",
    "    print(\"\"\"",
    "    \\u250c\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2510",
    "    \\u2502            CQRS Architecture                            \\u2502",
    "    \\u2502                                                          \\u2502",
    "    \\u2502   READ PATH (Search)        WRITE PATH (Booking)        \\u2502",
    "    \\u2502   \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500        \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500        \\u2502",
    "    \\u2502   CDN (static)           Redis Lock (distributed)     \\u2502",
    "    \\u2502        \\u2193                         \\u2193                      \\u2502",
    "    \\u2502   Redis Cache             PostgreSQL (sharded)        \\u2502",
    "    \\u2502        \\u2193                         \\u2193                      \\u2502",
    "    \\u2502   Elasticsearch           Kafka Events                \\u2502",
    "    \\u2502                                  \\u2193                      \\u2502",
    "    \\u2502                           Cache Invalidation          \\u2502",
    "    \\u2514\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2518",
    "    ",
    "    Key Optimizations:",
    "    - Eventual consistency for search (5s TTL)",
    "    - Strong consistency for bookings (distributed lock)",
    "    - Sharding by hotel_id (data locality)",
    "    - Event-driven cache invalidation (Kafka)",
    "    \"\"\")",
    "    ",
    "    print(\"\\n\" + \"=\" * 70)",
    "    print(\"   DEMONSTRATION COMPLETE\")",
    "    print(\"=\" * 70)",
    "",
    "",
    "if __name__ == \"__main__\":",
    "    asyncio.run(main())"
  ],
  "solution_java_lines": [
    "import java.time.*;",
    "import java.util.*;",
    "import java.util.concurrent.*;",
    "import java.util.concurrent.atomic.*;",
    "import java.util.concurrent.locks.*;",
    "import java.security.*;",
    "import java.util.stream.*;",
    "",
    "/**",
    " * Hotel Booking System - Part 3: Scaling for High Traffic",
    " * CQRS Architecture with Multi-layer Caching",
    " */",
    "public class ScaledHotelBookingSystem {",
    "    ",
    "    // Enums",
    "    public enum BookingStatus { PENDING, CONFIRMED, CANCELLED, FAILED }",
    "    public enum Region { US_EAST, US_WEST, EU_WEST, ASIA_PACIFIC }",
    "    ",
    "    // Data Classes",
    "    public record Hotel(String hotelId, String name, String location,",
    "                        Region region, List<String> amenities, double rating) {}",
    "    ",
    "    public record Room(String roomId, String hotelId, String roomType,",
    "                       double pricePerNight, int capacity) {}",
    "    ",
    "    public record SearchRequest(String location, LocalDate checkIn,",
    "                                 LocalDate checkOut, int guests, String cacheHint) {}",
    "    ",
    "    public record SearchResult(Room room, Hotel hotel, boolean available, double totalPrice) {}",
    "    ",
    "    public record SearchResponse(List<SearchResult> results, boolean fromCache,",
    "                                  double latencyMs, double cacheAgeSeconds) {}",
    "    ",
    "    public record Booking(String bookingId, String userId, String roomId,",
    "                          String hotelId, LocalDate checkIn, LocalDate checkOut,",
    "                          BookingStatus status, double totalPrice, Instant createdAt) {}",
    "    ",
    "    public record BookingResponse(BookingStatus status, String bookingId,",
    "                                   double latencyMs, String message) {}",
    "    ",
    "    public record CacheEntry(Object data, long createdAtMs, long ttlMs) {",
    "        public boolean isExpired() {",
    "            return System.currentTimeMillis() - createdAtMs > ttlMs;",
    "        }",
    "        public double ageSeconds() {",
    "            return (System.currentTimeMillis() - createdAtMs) / 1000.0;",
    "        }",
    "    }",
    "    ",
    "    // Redis Cluster Simulator",
    "    public static class RedisClusterSimulator {",
    "        private final int numShards;",
    "        private final List<ConcurrentHashMap<String, CacheEntry>> shards;",
    "        private final ConcurrentHashMap<String, LockInfo> locks = new ConcurrentHashMap<>();",
    "        private final AtomicLong hits = new AtomicLong();",
    "        private final AtomicLong misses = new AtomicLong();",
    "        private final AtomicLong lockAcquired = new AtomicLong();",
    "        private final AtomicLong lockFailed = new AtomicLong();",
    "        ",
    "        record LockInfo(String owner, long expiryMs) {}",
    "        ",
    "        public RedisClusterSimulator(int numShards) {",
    "            this.numShards = numShards;",
    "            this.shards = new ArrayList<>();",
    "            for (int i = 0; i < numShards; i++) {",
    "                shards.add(new ConcurrentHashMap<>());",
    "            }",
    "        }",
    "        ",
    "        private int getShard(String key) {",
    "            return Math.abs(key.hashCode()) % numShards;",
    "        }",
    "        ",
    "        public Optional<CacheEntry> get(String key) {",
    "            var shard = shards.get(getShard(key));",
    "            var entry = shard.get(key);",
    "            if (entry == null) {",
    "                misses.incrementAndGet();",
    "                return Optional.empty();",
    "            }",
    "            if (entry.isExpired()) {",
    "                shard.remove(key);",
    "                misses.incrementAndGet();",
    "                return Optional.empty();",
    "            }",
    "            hits.incrementAndGet();",
    "            return Optional.of(entry);",
    "        }",
    "        ",
    "        public void set(String key, Object data, long ttlMs) {",
    "            var entry = new CacheEntry(data, System.currentTimeMillis(), ttlMs);",
    "            shards.get(getShard(key)).put(key, entry);",
    "        }",
    "        ",
    "        public boolean acquireLock(String key, String owner, long ttlMs) {",
    "            String lockKey = \"lock:\" + key;",
    "            long now = System.currentTimeMillis();",
    "            ",
    "            var result = locks.compute(lockKey, (k, existing) -> {",
    "                if (existing == null || now >= existing.expiryMs()) {",
    "                    return new LockInfo(owner, now + ttlMs);",
    "                }",
    "                if (existing.owner().equals(owner)) {",
    "                    return new LockInfo(owner, now + ttlMs);",
    "                }",
    "                return existing;",
    "            });",
    "            ",
    "            boolean acquired = result.owner().equals(owner);",
    "            if (acquired) lockAcquired.incrementAndGet();",
    "            else lockFailed.incrementAndGet();",
    "            return acquired;",
    "        }",
    "        ",
    "        public void releaseLock(String key, String owner) {",
    "            String lockKey = \"lock:\" + key;",
    "            locks.computeIfPresent(lockKey, (k, v) ->",
    "                v.owner().equals(owner) ? null : v);",
    "        }",
    "        ",
    "        public Map<String, Object> getStats() {",
    "            long totalOps = hits.get() + misses.get();",
    "            return Map.of(",
    "                \"hits\", hits.get(),",
    "                \"misses\", misses.get(),",
    "                \"hitRate\", totalOps > 0 ? (double)hits.get() / totalOps : 0,",
    "                \"lockAcquired\", lockAcquired.get(),",
    "                \"lockFailed\", lockFailed.get()",
    "            );",
    "        }",
    "    }",
    "    ",
    "    // Elasticsearch Simulator",
    "    public static class ElasticsearchSimulator {",
    "        private final ConcurrentHashMap<String, Hotel> hotels = new ConcurrentHashMap<>();",
    "        private final ConcurrentHashMap<String, Room> rooms = new ConcurrentHashMap<>();",
    "        private final ConcurrentHashMap<LocalDate, Set<String>> bookedRooms =",
    "            new ConcurrentHashMap<>();",
    "        ",
    "        public void indexHotel(Hotel hotel) { hotels.put(hotel.hotelId(), hotel); }",
    "        public void indexRoom(Room room) { rooms.put(room.roomId(), room); }",
    "        public Room getRoom(String roomId) { return rooms.get(roomId); }",
    "        ",
    "        public void updateAvailability(String roomId, LocalDate checkIn,",
    "                                        LocalDate checkOut, boolean isBooked) {",
    "            for (var date = checkIn; date.isBefore(checkOut); date = date.plusDays(1)) {",
    "                var set = bookedRooms.computeIfAbsent(date,",
    "                    k -> ConcurrentHashMap.newKeySet());",
    "                if (isBooked) set.add(roomId);",
    "                else set.remove(roomId);",
    "            }",
    "        }",
    "        ",
    "        public List<SearchResult> search(SearchRequest req) throws InterruptedException {",
    "            Thread.sleep(50 + new Random().nextInt(50)); // Simulate ES latency",
    "            ",
    "            long nights = ChronoUnit.DAYS.between(req.checkIn(), req.checkOut());",
    "            return rooms.values().stream()",
    "                .filter(room -> {",
    "                    var hotel = hotels.get(room.hotelId());",
    "                    return hotel != null &&",
    "                        hotel.location().toLowerCase().contains(req.location().toLowerCase());",
    "                })",
    "                .map(room -> {",
    "                    var hotel = hotels.get(room.hotelId());",
    "                    boolean available = isAvailable(room.roomId(), req.checkIn(), req.checkOut());",
    "                    return new SearchResult(room, hotel, available,",
    "                        room.pricePerNight() * nights);",
    "                })",
    "                .limit(50)",
    "                .toList();",
    "        }",
    "        ",
    "        private boolean isAvailable(String roomId, LocalDate checkIn, LocalDate checkOut) {",
    "            for (var date = checkIn; date.isBefore(checkOut); date = date.plusDays(1)) {",
    "                var booked = bookedRooms.get(date);",
    "                if (booked != null && booked.contains(roomId)) return false;",
    "            }",
    "            return true;",
    "        }",
    "    }",
    "    ",
    "    // PostgreSQL Simulator",
    "    public static class PostgreSQLSimulator {",
    "        private final ConcurrentHashMap<String, Booking> bookings = new ConcurrentHashMap<>();",
    "        private final ConcurrentHashMap<String, List<DateRange>> roomBookings =",
    "            new ConcurrentHashMap<>();",
    "        private final ReentrantLock lock = new ReentrantLock();",
    "        ",
    "        record DateRange(LocalDate start, LocalDate end) {}",
    "        ",
    "        public record CreateResult(boolean success, String message) {}",
    "        ",
    "        public CreateResult createBooking(Booking booking) throws InterruptedException {",
    "            Thread.sleep(20 + new Random().nextInt(30)); // Simulate DB latency",
    "            ",
    "            lock.lock();",
    "            try {",
    "                var existing = roomBookings.getOrDefault(booking.roomId(), List.of());",
    "                for (var range : existing) {",
    "                    if (booking.checkIn().isBefore(range.end()) &&",
    "                        booking.checkOut().isAfter(range.start())) {",
    "                        return new CreateResult(false, \"Room already booked\");",
    "                    }",
    "                }",
    "                ",
    "                bookings.put(booking.bookingId(), booking);",
    "                var newList = new ArrayList<>(existing);",
    "                newList.add(new DateRange(booking.checkIn(), booking.checkOut()));",
    "                roomBookings.put(booking.roomId(), newList);",
    "                ",
    "                return new CreateResult(true, \"Booking created\");",
    "            } finally {",
    "                lock.unlock();",
    "            }",
    "        }",
    "    }",
    "    ",
    "    // Main System",
    "    private final RedisClusterSimulator redis;",
    "    private final ElasticsearchSimulator elasticsearch;",
    "    private final PostgreSQLSimulator postgresql;",
    "    private final AtomicLong searchCount = new AtomicLong();",
    "    private final AtomicLong cacheHits = new AtomicLong();",
    "    private final AtomicLong bookingCount = new AtomicLong();",
    "    ",
    "    public ScaledHotelBookingSystem() {",
    "        this.redis = new RedisClusterSimulator(3);",
    "        this.elasticsearch = new ElasticsearchSimulator();",
    "        this.postgresql = new PostgreSQLSimulator();",
    "        setupSampleData();",
    "    }",
    "    ",
    "    private void setupSampleData() {",
    "        elasticsearch.indexHotel(new Hotel(\"H_NYC_1\", \"Manhattan Grand\",",
    "            \"NYC, New York\", Region.US_EAST, List.of(\"wifi\", \"pool\"), 4.5));",
    "        elasticsearch.indexRoom(new Room(\"R_NYC_1\", \"H_NYC_1\", \"Deluxe King\", 250, 2));",
    "        elasticsearch.indexRoom(new Room(\"R_NYC_2\", \"H_NYC_1\", \"Standard Queen\", 180, 2));",
    "    }",
    "    ",
    "    private String makeSearchKey(SearchRequest req) {",
    "        return String.format(\"search:%s:%s:%s\",",
    "            req.location(), req.checkIn(), req.checkOut());",
    "    }",
    "    ",
    "    @SuppressWarnings(\"unchecked\")",
    "    public CompletableFuture<SearchResponse> searchRoomsOptimized(SearchRequest request) {",
    "        return CompletableFuture.supplyAsync(() -> {",
    "            long start = System.currentTimeMillis();",
    "            searchCount.incrementAndGet();",
    "            ",
    "            String cacheKey = makeSearchKey(request);",
    "            boolean allowCache = !\"require_fresh\".equals(request.cacheHint());",
    "            ",
    "            // Try cache first",
    "            if (allowCache) {",
    "                var cached = redis.get(cacheKey);",
    "                if (cached.isPresent()) {",
    "                    cacheHits.incrementAndGet();",
    "                    var entry = cached.get();",
    "                    return new SearchResponse(",
    "                        (List<SearchResult>) entry.data(),",
    "                        true,",
    "                        System.currentTimeMillis() - start,",
    "                        entry.ageSeconds()",
    "                    );",
    "                }",
    "            }",
    "            ",
    "            // Query Elasticsearch",
    "            try {",
    "                var results = elasticsearch.search(request);",
    "                redis.set(cacheKey, results, 5000); // 5 second TTL",
    "                ",
    "                return new SearchResponse(",
    "                    results, false,",
    "                    System.currentTimeMillis() - start, 0",
    "                );",
    "            } catch (InterruptedException e) {",
    "                Thread.currentThread().interrupt();",
    "                return new SearchResponse(List.of(), false, 0, 0);",
    "            }",
    "        });",
    "    }",
    "    ",
    "    public CompletableFuture<BookingResponse> createBooking(",
    "            String userId, String roomId, String hotelId,",
    "            LocalDate checkIn, LocalDate checkOut) {",
    "        return CompletableFuture.supplyAsync(() -> {",
    "            long start = System.currentTimeMillis();",
    "            bookingCount.incrementAndGet();",
    "            ",
    "            String lockKey = String.format(\"%s:%s:%s\", roomId, checkIn, checkOut);",
    "            String lockOwner = userId + \"_\" + System.currentTimeMillis();",
    "            ",
    "            if (!redis.acquireLock(lockKey, lockOwner, 30000)) {",
    "                return new BookingResponse(",
    "                    BookingStatus.FAILED, null,",
    "                    System.currentTimeMillis() - start,",
    "                    \"Could not acquire lock\"",
    "                );",
    "            }",
    "            ",
    "            try {",
    "                var room = elasticsearch.getRoom(roomId);",
    "                if (room == null) {",
    "                    return new BookingResponse(",
    "                        BookingStatus.FAILED, null,",
    "                        System.currentTimeMillis() - start,",
    "                        \"Room not found\"",
    "                    );",
    "                }",
    "                ",
    "                long nights = ChronoUnit.DAYS.between(checkIn, checkOut);",
    "                String bookingId = \"BK_\" + System.currentTimeMillis();",
    "                ",
    "                var booking = new Booking(",
    "                    bookingId, userId, roomId, hotelId,",
    "                    checkIn, checkOut, BookingStatus.CONFIRMED,",
    "                    room.pricePerNight() * nights, Instant.now()",
    "                );",
    "                ",
    "                var result = postgresql.createBooking(booking);",
    "                ",
    "                if (result.success()) {",
    "                    elasticsearch.updateAvailability(roomId, checkIn, checkOut, true);",
    "                    return new BookingResponse(",
    "                        BookingStatus.CONFIRMED, bookingId,",
    "                        System.currentTimeMillis() - start,",
    "                        \"Booking confirmed\"",
    "                    );",
    "                } else {",
    "                    return new BookingResponse(",
    "                        BookingStatus.FAILED, null,",
    "                        System.currentTimeMillis() - start,",
    "                        result.message()",
    "                    );",
    "                }",
    "            } catch (InterruptedException e) {",
    "                Thread.currentThread().interrupt();",
    "                return new BookingResponse(",
    "                    BookingStatus.FAILED, null,",
    "                    System.currentTimeMillis() - start, \"Interrupted\"",
    "                );",
    "            } finally {",
    "                redis.releaseLock(lockKey, lockOwner);",
    "            }",
    "        });",
    "    }",
    "    ",
    "    public Map<String, Object> getStats() {",
    "        long total = searchCount.get();",
    "        return Map.of(",
    "            \"cacheStats\", redis.getStats(),",
    "            \"searchRequests\", searchCount.get(),",
    "            \"searchCacheHitRate\", total > 0 ? (double)cacheHits.get() / total : 0,",
    "            \"bookingRequests\", bookingCount.get()",
    "        );",
    "    }",
    "    ",
    "    public static void main(String[] args) throws Exception {",
    "        System.out.println(\"\\n\" + \"=\".repeat(70));",
    "        System.out.println(\"   HOTEL BOOKING SYSTEM - PART 3: SCALING FOR HIGH TRAFFIC\");",
    "        System.out.println(\"=\".repeat(70));",
    "        ",
    "        var system = new ScaledHotelBookingSystem();",
    "        ",
    "        // Demo 1: Search with caching",
    "        System.out.println(\"\\n\" + \"-\".repeat(50));",
    "        System.out.println(\"DEMO: Search with Caching\");",
    "        System.out.println(\"-\".repeat(50));",
    "        ",
    "        var request = new SearchRequest(\"NYC\",",
    "            LocalDate.of(2024, 9, 1),",
    "            LocalDate.of(2024, 9, 3), 2, \"allow_stale\");",
    "        ",
    "        var response1 = system.searchRoomsOptimized(request).get();",
    "        System.out.printf(\"Search 1: %d rooms, cache=%b, %.2fms%n\",",
    "            response1.results().size(), response1.fromCache(), response1.latencyMs());",
    "        ",
    "        var response2 = system.searchRoomsOptimized(request).get();",
    "        System.out.printf(\"Search 2: %d rooms, cache=%b, %.2fms%n\",",
    "            response2.results().size(), response2.fromCache(), response2.latencyMs());",
    "        ",
    "        // Demo 2: Concurrent bookings",
    "        System.out.println(\"\\n\" + \"-\".repeat(50));",
    "        System.out.println(\"DEMO: Concurrent Booking Prevention\");",
    "        System.out.println(\"-\".repeat(50));",
    "        ",
    "        var futures = new ArrayList<CompletableFuture<BookingResponse>>();",
    "        for (int i = 0; i < 5; i++) {",
    "            futures.add(system.createBooking(",
    "                \"USER_\" + i, \"R_NYC_1\", \"H_NYC_1\",",
    "                LocalDate.of(2024, 10, 1), LocalDate.of(2024, 10, 3)));",
    "        }",
    "        ",
    "        var bookingResults = CompletableFuture.allOf(",
    "            futures.toArray(new CompletableFuture[0]))",
    "            .thenApply(v -> futures.stream()",
    "                .map(CompletableFuture::join)",
    "                .toList())",
    "            .get();",
    "        ",
    "        long confirmed = bookingResults.stream()",
    "            .filter(r -> r.status() == BookingStatus.CONFIRMED).count();",
    "        System.out.printf(\"Results: %d confirmed, %d failed%n\",",
    "            confirmed, bookingResults.size() - confirmed);",
    "        ",
    "        // Stats",
    "        System.out.println(\"\\nSystem Stats: \" + system.getStats());",
    "        ",
    "        System.out.println(\"\\n\" + \"=\".repeat(70));",
    "        System.out.println(\"   DEMONSTRATION COMPLETE\");",
    "        System.out.println(\"=\".repeat(70));",
    "    }",
    "}"
  ],
  "code_walkthrough": [
    {
      "lines": "1-45",
      "explanation": "Imports and data class definitions including enums for BookingStatus and Region, plus dataclasses for Hotel, Room, SearchRequest, SearchResult, Booking, and their response types"
    },
    {
      "lines": "46-150",
      "explanation": "RedisClusterSimulator: Simulates a sharded Redis cluster with consistent hashing, TTL-based cache entries, distributed locks using SETNX semantics, and statistics tracking"
    },
    {
      "lines": "151-230",
      "explanation": "ElasticsearchSimulator: Simulates ES for full-text search with hotel/room indexing, availability tracking per date, and search with filters"
    },
    {
      "lines": "231-290",
      "explanation": "KafkaSimulator: Event streaming simulation with publish/subscribe pattern and async consumer notification"
    },
    {
      "lines": "291-360",
      "explanation": "PostgreSQLSimulator: Sharded database simulation with booking storage, optimistic locking within shards, and conflict detection"
    },
    {
      "lines": "361-440",
      "explanation": "CacheManager: Multi-layer cache management with different TTLs for different data types (hotel metadata 1hr, search results 5s), pattern-based invalidation"
    },
    {
      "lines": "441-520",
      "explanation": "SearchService: Read path implementation with cache-aside pattern, O(1) cache hits, fallback to Elasticsearch, and automatic caching of results"
    },
    {
      "lines": "521-650",
      "explanation": "BookingService: Write path with distributed locking flow - acquire lock, validate availability, create booking, publish event, invalidate cache, release lock"
    },
    {
      "lines": "651-750",
      "explanation": "ScaledHotelBookingSystem: Main system orchestrating all components, sample data setup, and unified API for search and booking"
    },
    {
      "lines": "751-900",
      "explanation": "Demo and testing code: Concurrent search demonstration, race condition testing, load testing with latency percentile calculation, and statistics display"
    }
  ],
  "complexity_analysis": {
    "time": {
      "new_methods": {
        "searchRoomsOptimized": {
          "complexity": "O(1) cache hit / O(log n + k) cache miss",
          "explanation": "Cache hit is O(1) hash lookup. Cache miss requires Elasticsearch query O(log n) for indexed search plus O(k) result processing where k is result count"
        },
        "createBooking (scaled)": {
          "complexity": "O(1)",
          "explanation": "With proper sharding by hotel_id, all operations (lock, DB write, cache invalidation) are O(1) on the target shard"
        }
      },
      "overall_change": "Search improves from O(n) full scan to O(1) with caching. Booking remains O(1) but with better scalability through sharding"
    },
    "space": {
      "additional_space": "O(hotels + rooms + cached_searches)",
      "explanation": "Elasticsearch indexes O(hotels + rooms), Redis caches recent searches with TTL eviction, Kafka buffers recent events. Total space bounded by TTL-based eviction"
    }
  },
  "dry_run": {
    "example_input": "User searches NYC hotels, then 5 users concurrently book the same room",
    "steps": [
      {
        "step": 1,
        "action": "Search request for NYC",
        "state": "Cache empty",
        "explanation": "Check Redis cache - miss. Query Elasticsearch."
      },
      {
        "step": 2,
        "action": "Elasticsearch query executes",
        "state": "ES returns 2 rooms",
        "explanation": "Full-text search on location, filter by availability"
      },
      {
        "step": 3,
        "action": "Cache search results",
        "state": "Redis: search:NYC:20240901:20240903 -> [results]",
        "explanation": "Store with 5s TTL for subsequent requests"
      },
      {
        "step": 4,
        "action": "Second search same criteria",
        "state": "Cache hit",
        "explanation": "Redis returns cached results in <5ms"
      },
      {
        "step": 5,
        "action": "5 concurrent booking requests",
        "state": "All try to acquire lock",
        "explanation": "Redis SETNX - only one succeeds"
      },
      {
        "step": 6,
        "action": "Winner creates booking",
        "state": "DB: booking created, ES: room marked booked",
        "explanation": "Write to PostgreSQL, update Elasticsearch"
      },
      {
        "step": 7,
        "action": "Kafka event published",
        "state": "Event: BOOKING_CREATED",
        "explanation": "Async cache invalidation triggers"
      },
      {
        "step": 8,
        "action": "Lock released",
        "state": "Other 4 requests fail",
        "explanation": "They couldn't acquire lock in time"
      },
      {
        "step": 9,
        "action": "Cache invalidated",
        "state": "search:NYC:* deleted",
        "explanation": "Next search will get fresh results"
      }
    ],
    "final_output": "1 confirmed booking, 4 failed, search cache properly invalidated"
  },
  "debugging_playbook": {
    "fast_sanity_checks": [
      "Single search should return results",
      "Single booking should succeed",
      "Second booking same room should fail"
    ],
    "likely_bugs": [
      "Cache key collision for different date ranges",
      "Lock not released on exception",
      "TTL too short causing excessive cache misses"
    ],
    "recommended_logs_or_asserts": [
      "assert lock_acquired before DB write",
      "log cache hit/miss ratio",
      "log lock wait times"
    ],
    "how_to_localize": "1. Check if issue is cache-related (disable cache, does it work?). 2. Check lock acquisition (add debug logging). 3. Check DB query execution. 4. Verify event propagation timing."
  },
  "edge_cases": [
    {
      "case": "Cache stampede on popular hotel",
      "handling": "Use probabilistic early expiration or lock on cache miss to prevent thundering herd",
      "gotcha": "Without protection, 1000s of requests hit ES simultaneously when cache expires"
    },
    {
      "case": "Lock expires during long DB operation",
      "handling": "Extend lock TTL during operation, or use fencing tokens",
      "gotcha": "Can lead to double booking if lock expires and another request acquires it"
    },
    {
      "case": "Kafka lag causes stale cache",
      "handling": "Accept eventual consistency, booking service always checks DB",
      "gotcha": "Search may show room as available after booking until event processes"
    },
    {
      "case": "Network partition between regions",
      "handling": "Graceful degradation - serve stale cache with warning",
      "gotcha": "Cross-region bookings may fail during partition"
    }
  ],
  "test_cases": [
    {
      "name": "Cache hit performance",
      "input": "Two identical search requests within 5 seconds",
      "expected": "Second request <10ms, fromCache=true",
      "explanation": "Verifies cache is working and significantly faster"
    },
    {
      "name": "Concurrent booking race condition",
      "input": "5 simultaneous bookings for same room/dates",
      "expected": "Exactly 1 CONFIRMED, 4 FAILED",
      "explanation": "Verifies distributed lock prevents double booking"
    },
    {
      "name": "Cache invalidation after booking",
      "input": "Search, book, search again",
      "expected": "Second search shows room unavailable",
      "explanation": "Verifies event-driven cache invalidation works"
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Not releasing lock on exception",
      "why_wrong": "Lock remains held until TTL expires, blocking other bookings",
      "correct_approach": "Use try/finally to always release lock",
      "code_example_wrong": "lock.acquire()\\nprocess()  # Exception here leaves lock held",
      "code_example_correct": "lock.acquire()\\ntry:\\n    process()\\nfinally:\\n    lock.release()"
    },
    {
      "mistake": "Cache key without date range",
      "why_wrong": "Different date searches return same cached results",
      "correct_approach": "Include all search parameters in cache key",
      "code_example_wrong": "cache_key = f'search:{location}'",
      "code_example_correct": "cache_key = f'search:{location}:{check_in}:{check_out}:{filters}'"
    },
    {
      "mistake": "Synchronous cache invalidation",
      "why_wrong": "Booking latency includes cache invalidation across all regions",
      "correct_approach": "Async event-driven invalidation via Kafka",
      "code_example_wrong": "await invalidate_all_caches()  # Blocks booking response",
      "code_example_correct": "kafka.publish('cache_invalidation', event)  # Returns immediately"
    }
  ],
  "interview_tips": {
    "how_to_present": "Start with the key insight: CQRS allows different consistency models for reads vs writes. Draw the architecture showing separate paths. Then dive into each component.",
    "what_to_mention": [
      "Cache hit rate expectations (90%+)",
      "Acceptable staleness window (5s)",
      "Lock TTL considerations",
      "Graceful degradation strategies"
    ],
    "time_allocation": "3 min architecture overview, 5 min implementation details, 2 min edge cases, 2 min questions",
    "if_stuck": [
      "Think about read vs write ratio",
      "What can be stale vs what must be fresh?",
      "How do systems like Expedia handle this?"
    ]
  },
  "connection_to_next_part": "Part 4 could introduce: global consistency requirements, payment integration, real-time pricing updates, or disaster recovery. The event-driven architecture with Kafka sets up well for any of these - events can trigger pricing recalculations, payment processing, or cross-region sync.",
  "communication_script": {
    "transition_from_previous": "In Part 2, I implemented distributed locking to prevent race conditions. For Part 3's 10x scale requirement, I need to fundamentally separate read and write paths using CQRS pattern.",
    "explaining_changes": "The key change is adding Elasticsearch for search and Redis for caching. Searches hit cache first (O(1)), then Elasticsearch. Bookings still use the distributed lock but now publish events for async cache invalidation.",
    "while_extending_code": [
      "I'm adding CacheManager to handle TTL-based caching...",
      "SearchService encapsulates the read path with cache-aside pattern...",
      "EventConsumer handles async cache invalidation..."
    ],
    "after_completing": "This now handles 100K search QPS with sub-200ms P99 through caching, and 10K booking QPS with distributed locks ensuring no double bookings. Ready to discuss monitoring, failure scenarios, or the next part?"
  },
  "time_milestones": {
    "time_budget": "12-15 minutes for this part",
    "by_2_min": "Explain CQRS concept, draw high-level architecture",
    "by_5_min": "Detail caching strategy, TTLs, invalidation approach",
    "by_10_min": "Core implementation done, discussing sharding strategy",
    "warning_signs": "If still explaining concepts at 7 min, start coding. Ask if they want pseudocode vs full implementation."
  },
  "recovery_strategies": {
    "if_part_builds_wrong": "Part 2's distributed lock is foundation. If unclear, ask: 'Can I assume the locking mechanism from Part 2 works correctly?'",
    "if_new_requirement_unclear": "Ask: 'For the 200ms search latency target, is that P50 or P99? And is eventual consistency acceptable for search results?'",
    "if_running_behind": "Focus on: 1) Cache-aside pattern for search, 2) Event-driven invalidation. Skip detailed sharding discussion."
  },
  "signal_points": {
    "wow_factors_for_followup": [
      "Mentioning cache stampede prevention with probabilistic early expiration",
      "Discussing read replicas for Elasticsearch with near-real-time indexing",
      "Bringing up chaos engineering - 'How do we test regional failover?'",
      "Mentioning observability - distributed tracing across services"
    ]
  },
  "pattern_recognition": {
    "pattern": "CQRS (Command Query Responsibility Segregation) + Cache-Aside + Event Sourcing",
    "indicators": [
      "High read-to-write ratio (100:1)",
      "Different consistency requirements for reads vs writes",
      "Need for horizontal scalability"
    ],
    "similar_problems": [
      "LC 1700 - Students Unable to Eat Lunch (queue processing)",
      "Design Twitter Feed (read-heavy with eventual consistency)",
      "Design Stock Exchange (strong consistency for trades)"
    ],
    "template": "1. Separate read/write services\\n2. Cache reads aggressively with TTL\\n3. Use events for async sync\\n4. Strong consistency only where required"
  },
  "thinking_process": [
    {
      "step": 1,
      "thought": "100K search QPS means DB can't handle it directly",
      "why": "PostgreSQL tops out at ~10K QPS for complex queries"
    },
    {
      "step": 2,
      "thought": "100:1 read-write ratio screams caching",
      "why": "Amortize expensive queries across many reads"
    },
    {
      "step": 3,
      "thought": "200ms P99 requires cache hits to be <50ms",
      "why": "Network + cache lookup + some misses must stay under 200ms"
    },
    {
      "step": 4,
      "thought": "Bookings can't use stale data",
      "why": "Double booking is unacceptable business impact"
    },
    {
      "step": 5,
      "thought": "Event-driven for cross-service sync",
      "why": "Decouples services, enables async processing, provides durability"
    }
  ],
  "interviewer_perspective": {
    "what_they_evaluate": [
      "Can you identify read-heavy vs write-heavy patterns?",
      "Do you understand cache trade-offs?",
      "Can you design for different consistency requirements?"
    ],
    "bonus_points": [
      "Mentioning specific numbers (cache TTL, lock TTL)",
      "Discussing failure modes",
      "Bringing up monitoring and observability"
    ],
    "red_flags": [
      "Trying to scale single DB infinitely",
      "Ignoring consistency requirements",
      "No consideration of cache invalidation"
    ]
  },
  "ai_copilot_tips": {
    "what_to_do": [
      "Use AI to generate boilerplate async/await patterns",
      "Let it help with dataclass definitions"
    ],
    "what_not_to_do": [
      "Don't let AI design the architecture - you should drive that",
      "Verify cache key generation logic carefully"
    ]
  },
  "red_flags_to_avoid": {
    "behavioral": [
      "Not discussing trade-offs",
      "Jumping to implementation without architecture discussion"
    ],
    "technical": [
      "Using synchronous invalidation that blocks writes",
      "Not considering cache stampede scenarios"
    ],
    "communication": [
      "Not explaining why eventual consistency is acceptable for search",
      "Forgetting to mention the distributed lock from Part 2"
    ]
  },
  "final_checklist": {
    "before_saying_done": [
      "Search path uses cache-aside pattern?",
      "Booking path uses distributed lock?",
      "Events published for cache invalidation?",
      "Different TTLs for different data types?"
    ],
    "quick_code_review": [
      "Locks always released in finally block",
      "Cache keys include all relevant parameters",
      "Async operations properly awaited"
    ]
  },
  "production_considerations": {
    "what_i_would_add": [
      "Circuit breakers for external services",
      "Distributed tracing with OpenTelemetry",
      "Rate limiting per user/IP",
      "Graceful degradation serving stale cache on failures"
    ],
    "why_not_in_interview": "Focus on core scaling patterns; these are implementation details",
    "how_to_mention": "Say: 'In production, I'd add circuit breakers here so Elasticsearch failures don't cascade to all requests.'"
  },
  "generated_at": "2026-01-18T21:27:20.041970",
  "_meta": {
    "problem_id": "booking_reservation_system",
    "part_number": 3,
    "model": "claude-opus-4-5-20251101"
  }
}