{
  "problem_title": "News Feed Aggregator System - Part 3: Content Recommendation & ML Integration",
  "part_number": 3,
  "builds_on": "Part 2",
  "difficulty": "hard",
  "problem_understanding": {
    "what_changes": "Part 2 handled real-time notifications and breaking news delivery. Part 3 transforms the system from a simple publisher-based feed to an **intelligent, ML-powered recommendation engine**. Instead of just showing articles from followed publishers, the system now learns from user behavior (clicks, read time, shares) to predict what content each user will find valuable\u2014even from publishers they don't follow. This requires adding an entire ML inference pipeline that operates at low latency (sub-50ms) while serving 100K QPS.",
    "new_requirements": [
      "Record and process user engagement signals (CTR, dwell time, scroll depth, shares)",
      "Generate and store user/article embeddings using neural network models",
      "Implement collaborative filtering to find similar users",
      "Balance exploitation (known preferences) vs exploration (discovering new interests)",
      "Achieve O(log N) recommendation latency using Approximate Nearest Neighbor search",
      "Support real-time signal incorporation (online learning)",
      "Enforce diversity in recommendations (no topic domination)"
    ],
    "new_constraints": [
      "Recommendations must be served in <50ms (P99)",
      "Must handle cold-start problem for new users and new articles",
      "Embedding storage scales with millions of users and articles",
      "Model training is offline (daily batch), but inference is real-time",
      "Must prevent filter bubbles while respecting user preferences"
    ],
    "key_insight": "**Pre-compute everything possible offline, use ANN for real-time inference.** The two-tower architecture separates user and article embeddings, allowing us to pre-compute article embeddings during ingestion and user embeddings periodically. At serve time, we only compute the user's recent behavior embedding and perform a fast ANN lookup against pre-indexed article vectors. This transforms O(N) brute-force ranking into O(log N) vector similarity search."
  },
  "visual_explanation": {
    "before_after": "```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                    BEFORE (Part 2) vs AFTER (Part 3)                         \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551                                                                              \u2551\n\u2551  PART 2: Publisher-Based Feed                                                \u2551\n\u2551  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2551\n\u2551  \u2502                                                                     \u2502     \u2551\n\u2551  \u2502   User follows: [TechCrunch, CNN, ESPN]                            \u2502     \u2551\n\u2551  \u2502                     \u2502                                               \u2502     \u2551\n\u2551  \u2502                     \u25bc                                               \u2502     \u2551\n\u2551  \u2502   Feed = Articles FROM followed publishers                         \u2502     \u2551\n\u2551  \u2502           (Simple filter + chronological sort)                     \u2502     \u2551\n\u2551  \u2502                                                                     \u2502     \u2551\n\u2551  \u2502   \u274c Can't discover new interests                                  \u2502     \u2551\n\u2551  \u2502   \u274c Same articles for similar users                               \u2502     \u2551\n\u2551  \u2502   \u274c No learning from behavior                                     \u2502     \u2551\n\u2551  \u2502                                                                     \u2502     \u2551\n\u2551  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2551\n\u2551                                                                              \u2551\n\u2551  PART 3: ML-Powered Recommendations                                          \u2551\n\u2551  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2551\n\u2551  \u2502                                                                     \u2502     \u2551\n\u2551  \u2502   User Behavior \u2192 [Read tech 180s, Skip sports 10s, Share AI]     \u2502     \u2551\n\u2551  \u2502                     \u2502                                               \u2502     \u2551\n\u2551  \u2502                     \u25bc                                               \u2502     \u2551\n\u2551  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502     \u2551\n\u2551  \u2502   \u2502              ML RECOMMENDATION PIPELINE                     \u2502   \u2502     \u2551\n\u2551  \u2502   \u2502                                                             \u2502   \u2502     \u2551\n\u2551  \u2502   \u2502   User Embedding \u2500\u2500\u2510                                        \u2502   \u2502     \u2551\n\u2551  \u2502   \u2502   (128-dim vector) \u2502                                        \u2502   \u2502     \u2551\n\u2551  \u2502   \u2502                    \u25bc                                        \u2502   \u2502     \u2551\n\u2551  \u2502   \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                               \u2502   \u2502     \u2551\n\u2551  \u2502   \u2502              \u2502  ANN Search  \u2502 \u25c4\u2500\u2500 Article Embeddings        \u2502   \u2502     \u2551\n\u2551  \u2502   \u2502              \u2502  (Faiss/ANN) \u2502     (Pre-indexed)             \u2502   \u2502     \u2551\n\u2551  \u2502   \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                               \u2502   \u2502     \u2551\n\u2551  \u2502   \u2502                     \u2502                                        \u2502   \u2502     \u2551\n\u2551  \u2502   \u2502                     \u25bc                                        \u2502   \u2502     \u2551\n\u2551  \u2502   \u2502   Candidates \u2192 Ranking Model \u2192 Re-Ranker \u2192 Feed            \u2502   \u2502     \u2551\n\u2551  \u2502   \u2502                                                             \u2502   \u2502     \u2551\n\u2551  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502     \u2551\n\u2551  \u2502                                                                     \u2502     \u2551\n\u2551  \u2502   \u2705 Discovers new interests through exploration                   \u2502     \u2551\n\u2551  \u2502   \u2705 Personalized ranking for each user                            \u2502     \u2551\n\u2551  \u2502   \u2705 Learns continuously from engagement                           \u2502     \u2551\n\u2551  \u2502                                                                     \u2502     \u2551\n\u2551  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2551\n\u2551                                                                              \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n```",
    "algorithm_flow": "```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551            getRecommendations(userId, count) ALGORITHM FLOW                  \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551                                                                              \u2551\n\u2551  STEP 1: BUILD USER EMBEDDING (5ms)                                          \u2551\n\u2551  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2551\n\u2551  \u2502                                                                        \u2502  \u2551\n\u2551  \u2502   User's Recent Engagements (last 24h)                                \u2502  \u2551\n\u2551  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  \u2551\n\u2551  \u2502   \u2502 tech_article_1: READ 180s  \u2192 weight: 3.0 \u00d7 (180/60) = 9.0      \u2502 \u2502  \u2551\n\u2551  \u2502   \u2502 tech_article_2: READ 240s  \u2192 weight: 3.0 \u00d7 (240/60) = 12.0     \u2502 \u2502  \u2551\n\u2551  \u2502   \u2502 sports_article: CLICK 10s  \u2192 weight: 1.0 \u00d7 (10/60)  = 0.17     \u2502 \u2502  \u2551\n\u2551  \u2502   \u2502 ai_article_1:   SHARE       \u2192 weight: 5.0             = 5.0      \u2502 \u2502  \u2551\n\u2551  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  \u2551\n\u2551  \u2502                     \u2502                                                  \u2502  \u2551\n\u2551  \u2502                     \u25bc                                                  \u2502  \u2551\n\u2551  \u2502   User Embedding = \u03a3(article_embedding \u00d7 engagement_weight)           \u2502  \u2551\n\u2551  \u2502                    / \u03a3(engagement_weight)                              \u2502  \u2551\n\u2551  \u2502                                                                        \u2502  \u2551\n\u2551  \u2502   Result: 128-dim vector biased toward TECH/AI content                \u2502  \u2551\n\u2551  \u2502                                                                        \u2502  \u2551\n\u2551  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2551\n\u2551                                                                              \u2551\n\u2551  STEP 2: ANN CANDIDATE RETRIEVAL (10ms)                                      \u2551\n\u2551  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2551\n\u2551  \u2502                                                                        \u2502  \u2551\n\u2551  \u2502   Vector Database (Faiss Index)                                       \u2502  \u2551\n\u2551  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502  \u2551\n\u2551  \u2502   \u2502  Article Embeddings (5M articles indexed)                       \u2502 \u2502  \u2551\n\u2551  \u2502   \u2502                                                                  \u2502 \u2502  \u2551\n\u2551  \u2502   \u2502  Query: user_embedding                                          \u2502 \u2502  \u2551\n\u2551  \u2502   \u2502  Search: ANN with k=100 (retrieve 100 candidates)              \u2502 \u2502  \u2551\n\u2551  \u2502   \u2502  Filter: published_at > now - 7 days                            \u2502 \u2502  \u2551\n\u2551  \u2502   \u2502                                                                  \u2502 \u2502  \u2551\n\u2551  \u2502   \u2502  Returns: [(article_id, similarity_score), ...]                \u2502 \u2502  \u2551\n\u2551  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  \u2551\n\u2551  \u2502                                                                        \u2502  \u2551\n\u2551  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2551\n\u2551                                                                              \u2551\n\u2551  STEP 3: ML SCORING/RANKING (20ms)                                           \u2551\n\u2551  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2551\n\u2551  \u2502                                                                        \u2502  \u2551\n\u2551  \u2502   For each candidate article:                                         \u2502  \u2551\n\u2551  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  \u2551\n\u2551  \u2502   \u2502                                                               \u2502   \u2502  \u2551\n\u2551  \u2502   \u2502   score = dot_product(user_vec, article_vec)      [base]     \u2502   \u2502  \u2551\n\u2551  \u2502   \u2502         + freshness_boost                          [+0.2]    \u2502   \u2502  \u2551\n\u2551  \u2502   \u2502         + quality_score                            [+0.1]    \u2502   \u2502  \u2551\n\u2551  \u2502   \u2502         + publisher_affinity                       [+0.15]   \u2502   \u2502  \u2551\n\u2551  \u2502   \u2502         + exploration_bonus (if new category)      [+0.1]    \u2502   \u2502  \u2551\n\u2551  \u2502   \u2502                                                               \u2502   \u2502  \u2551\n\u2551  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  \u2551\n\u2551  \u2502                                                                        \u2502  \u2551\n\u2551  \u2502   Sort by score descending                                            \u2502  \u2551\n\u2551  \u2502                                                                        \u2502  \u2551\n\u2551  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2551\n\u2551                                                                              \u2551\n\u2551  STEP 4: RE-RANKING FOR DIVERSITY (5ms)                                      \u2551\n\u2551  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2551\n\u2551  \u2502                                                                        \u2502  \u2551\n\u2551  \u2502   Apply Business Rules:                                               \u2502  \u2551\n\u2551  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  \u2551\n\u2551  \u2502   \u2502 \u2713 Max 2 articles per category in top 5                       \u2502   \u2502  \u2551\n\u2551  \u2502   \u2502 \u2713 At least 1 exploration article from new category           \u2502   \u2502  \u2551\n\u2551  \u2502   \u2502 \u2713 No duplicate publishers in adjacent positions              \u2502   \u2502  \u2551\n\u2551  \u2502   \u2502 \u2713 Breaking news gets position boost                          \u2502   \u2502  \u2551\n\u2551  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  \u2551\n\u2551  \u2502                                                                        \u2502  \u2551\n\u2551  \u2502   Input:  [tech, tech, tech, tech, tech] (100% tech)                 \u2502  \u2551\n\u2551  \u2502   Output: [tech, AI, tech, science, tech] (diverse)                  \u2502  \u2551\n\u2551  \u2502                                                                        \u2502  \u2551\n\u2551  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2551\n\u2551                                                                              \u2551\n\u2551  STEP 5: RETURN TOP-K ARTICLES                                               \u2551\n\u2551  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2551\n\u2551  \u2502                                                                        \u2502  \u2551\n\u2551  \u2502   Final Feed (count=5):                                               \u2502  \u2551\n\u2551  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502  \u2551\n\u2551  \u2502   \u2502 1. tech_article_3    (score: 0.92)  \u2190 Strong user interest    \u2502   \u2502  \u2551\n\u2551  \u2502   \u2502 2. ai_article_2      (score: 0.88)  \u2190 Related to shares       \u2502   \u2502  \u2551\n\u2551  \u2502   \u2502 3. tech_article_4    (score: 0.85)  \u2190 Consistent preference   \u2502   \u2502  \u2551\n\u2551  \u2502   \u2502 4. science_article_1 (score: 0.75)  \u2190 Exploration bonus       \u2502   \u2502  \u2551\n\u2551  \u2502   \u2502 5. tech_article_5    (score: 0.82)  \u2190 Re-ranked for diversity \u2502   \u2502  \u2551\n\u2551  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502  \u2551\n\u2551  \u2502                                                                        \u2502  \u2551\n\u2551  \u2502   Total Latency: ~40ms (P99 < 50ms) \u2713                                \u2502  \u2551\n\u2551  \u2502                                                                        \u2502  \u2551\n\u2551  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2551\n\u2551                                                                              \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n```"
  },
  "approaches": [
    {
      "name": "Naive Extension - Brute Force Scoring",
      "description": "Extend Part 2 by adding engagement tracking and scoring all articles for each user. When getRecommendations is called, iterate through ALL articles in the database, compute a relevance score for each using the user's engagement history, sort by score, and return top-k.",
      "time_complexity": "O(N \u00d7 D) per request where N = total articles, D = embedding dimension",
      "space_complexity": "O(N \u00d7 D) for storing all article embeddings",
      "why_not_optimal": "At 5M articles and 100K QPS, this means 500 billion scoring operations per second. Even with optimized vectorization, this would require thousands of servers. The latency would be 500ms+ per request, far exceeding our 50ms target. This approach simply doesn't scale."
    },
    {
      "name": "Collaborative Filtering Only",
      "description": "Find similar users based on engagement overlap, then recommend articles those users liked. Store user-article interaction matrix and compute user-user similarity.",
      "time_complexity": "O(U \u00d7 A) where U = users, A = average articles per user",
      "space_complexity": "O(U \u00d7 U) for user similarity matrix",
      "why_not_optimal": "Suffers from cold-start problem for new users/articles. User-user similarity matrix grows quadratically with users (100M users = 10^16 pairs). Cannot incorporate content features like article text or categories. Poor for breaking news that no one has engaged with yet."
    },
    {
      "name": "Optimal Approach - Two-Tower + ANN",
      "description": "Pre-compute fixed-dimension embeddings for both users and articles using a two-tower neural network. Index article embeddings in a vector database (Faiss/Milvus). At serve time, compute user embedding from recent behavior, perform ANN search to retrieve candidates, apply lightweight ranking model, then re-rank for diversity.",
      "time_complexity": "O(log N) for ANN search + O(K) for ranking K candidates",
      "space_complexity": "O(N \u00d7 D) for article index + O(U \u00d7 D) for user embeddings",
      "key_insight": "Decomposition is the key. By separating user and article representation into independent towers, we can pre-compute article embeddings at ingestion time and update user embeddings incrementally. The dot product in embedding space approximates the full neural network scoring, enabling ANN search which is O(log N) vs O(N) brute force."
    }
  ],
  "optimal_solution": {
    "explanation_md": "## Two-Tower Architecture with ANN Search\n\n### Core Insight\nThe **two-tower architecture** transforms the recommendation problem from \"score every article\" to \"find nearest neighbors in embedding space.\" This is the same architecture used by YouTube, Google, and Meta for real-time recommendations at scale.\n\n### How It Works\n\n**1. Offline: Article Tower**\nWhen articles are ingested, we compute their embeddings using the article tower:\n- Input: Article ID, title, content, category, publisher, publish time\n- Output: 128-dimensional dense vector\n- These embeddings are indexed in a **vector database** (Faiss, Pinecone, Milvus)\n\n**2. Offline/Near-line: User Tower**\nUser embeddings are updated periodically (hourly or on significant engagement):\n- Input: User ID, followed publishers, engagement history, category preferences\n- Output: 128-dimensional dense vector\n- Stored in **embedding cache** (Redis with vector support)\n\n**3. Online: Inference**\n```\nUser Request \u2192 Fetch User Embedding \u2192 ANN Search \u2192 Candidate Ranking \u2192 Re-ranking \u2192 Response\n     |              |                     |              |                |           |\n    5ms           10ms                  10ms           15ms              5ms        ~45ms\n```\n\n### Key Components\n\n**Feature Store**\nCentralized storage for computed features:\n- User features: engagement counts, category affinity scores, recency weights\n- Article features: freshness score, quality score, viral coefficient\n\n**Engagement Processor**\nConverts raw events to training signals:\n- Click without read (< 10s) \u2192 negative signal\n- Long read (> 60s) \u2192 strong positive signal\n- Share \u2192 very strong positive signal\n- Time decay: recent signals weighted higher\n\n**Multi-Armed Bandit**\nBalances exploitation vs exploration:\n- 80% exploitation: recommend from known preferences\n- 20% exploration: inject articles from new categories\n- Uses Thompson Sampling to adaptively adjust exploration rate\n\n**Re-Ranker**\nApplies business rules post-scoring:\n- Category diversity: max 2 per category in top 5\n- Publisher diversity: no adjacent same-publisher articles\n- Freshness boost: breaking news gets position bump\n- Filter recently seen articles",
    "data_structures": [
      {
        "structure": "Vector Database (Faiss/Milvus)",
        "purpose": "Index article embeddings for O(log N) ANN search"
      },
      {
        "structure": "Feature Store (Redis)",
        "purpose": "Store pre-computed user and article features for fast lookup"
      },
      {
        "structure": "Embedding Cache (Redis)",
        "purpose": "Cache user embeddings to avoid recomputation"
      },
      {
        "structure": "Engagement Event Log (Kafka)",
        "purpose": "Buffer engagement events for batch processing"
      },
      {
        "structure": "User Profile Store",
        "purpose": "Store user preferences, followed publishers, category affinity scores"
      },
      {
        "structure": "Exploration State (Thompson Sampling)",
        "purpose": "Track exploration parameters for each category per user"
      }
    ],
    "algorithm_steps": [
      "Step 1: **Record Engagement** - Log event to Kafka, update real-time counters in Redis",
      "Step 2: **Batch Feature Update** (async) - Aggregate engagements, update category affinity scores",
      "Step 3: **User Embedding Update** (near-line) - Recompute user embedding when significant change detected",
      "Step 4: **Article Embedding** (at ingestion) - Compute and index article embedding in vector DB",
      "Step 5: **Candidate Retrieval** - ANN search in vector DB using user embedding as query",
      "Step 6: **Feature Augmentation** - Fetch article features (freshness, quality) from feature store",
      "Step 7: **ML Scoring** - Apply ranking model: base_score + freshness_boost + quality_boost",
      "Step 8: **Exploration Injection** - Use Thompson Sampling to select exploration articles",
      "Step 9: **Re-ranking** - Apply diversity rules, filter seen articles",
      "Step 10: **Return Results** - Serialize and return top-k articles"
    ]
  },
  "solution_python_lines": [
    "\"\"\"",
    "News Feed Aggregator - Part 3: ML-Powered Recommendations",
    "",
    "This module implements a production-grade recommendation system using:",
    "- Two-tower neural network architecture (simulated)",
    "- Approximate Nearest Neighbor (ANN) search",
    "- Feature store for user/article features",
    "- Multi-armed bandit for exploration/exploitation",
    "- Re-ranking for diversity enforcement",
    "\"\"\"",
    "",
    "from typing import List, Dict, Set, Tuple, Optional",
    "from dataclasses import dataclass, field",
    "from enum import Enum",
    "from collections import defaultdict",
    "from datetime import datetime, timedelta",
    "import heapq",
    "import math",
    "import random",
    "import hashlib",
    "",
    "",
    "class EngagementType(Enum):",
    "    \"\"\"Types of user engagement with articles.\"\"\"",
    "    CLICK = \"CLICK\"      # User clicked but may not have read",
    "    READ = \"READ\"        # User spent significant time reading",
    "    SHARE = \"SHARE\"      # User shared the article",
    "    SAVE = \"SAVE\"        # User bookmarked the article",
    "",
    "",
    "# Engagement weights for computing signals",
    "ENGAGEMENT_WEIGHTS = {",
    "    EngagementType.CLICK: 1.0,",
    "    EngagementType.READ: 3.0,",
    "    EngagementType.SHARE: 5.0,",
    "    EngagementType.SAVE: 4.0",
    "}",
    "",
    "# Minimum read time to consider as positive signal",
    "MIN_POSITIVE_READ_TIME = 30  # seconds",
    "EMBEDDING_DIMENSION = 128",
    "",
    "",
    "@dataclass",
    "class Article:",
    "    \"\"\"Represents a news article with metadata and embedding.\"\"\"",
    "    id: str",
    "    title: str",
    "    publisher_id: str",
    "    category: str",
    "    content: str",
    "    published_at: datetime",
    "    embedding: Optional[List[float]] = None",
    "    quality_score: float = 0.5",
    "    ",
    "    def __hash__(self):",
    "        return hash(self.id)",
    "",
    "",
    "@dataclass",
    "class EngagementEvent:",
    "    \"\"\"Records a single user engagement with an article.\"\"\"",
    "    user_id: str",
    "    article_id: str",
    "    engagement_type: EngagementType",
    "    duration_seconds: int",
    "    timestamp: datetime",
    "    ",
    "    def compute_weight(self) -> float:",
    "        \"\"\"Compute engagement weight based on type and duration.\"\"\"",
    "        base_weight = ENGAGEMENT_WEIGHTS[self.engagement_type]",
    "        if self.engagement_type == EngagementType.READ:",
    "            # Scale by read time (cap at 5 minutes)",
    "            time_factor = min(self.duration_seconds / 60.0, 5.0)",
    "            return base_weight * time_factor",
    "        elif self.engagement_type == EngagementType.CLICK:",
    "            # Low duration click is negative signal",
    "            if self.duration_seconds < MIN_POSITIVE_READ_TIME:",
    "                return -0.5  # Negative weight for quick bounce",
    "            return base_weight",
    "        return base_weight",
    "",
    "",
    "@dataclass",
    "class UserProfile:",
    "    \"\"\"Stores user preferences and computed features.\"\"\"",
    "    user_id: str",
    "    followed_publishers: Set[str] = field(default_factory=set)",
    "    followed_categories: Set[str] = field(default_factory=set)",
    "    embedding: Optional[List[float]] = None",
    "    category_affinity: Dict[str, float] = field(default_factory=dict)",
    "    last_embedding_update: datetime = field(default_factory=datetime.now)",
    "",
    "",
    "class VectorDatabase:",
    "    \"\"\"",
    "    Simulated vector database with ANN search capabilities.",
    "    In production, use Faiss, Milvus, Pinecone, or Weaviate.",
    "    \"\"\"",
    "    ",
    "    def __init__(self, dimension: int = EMBEDDING_DIMENSION):",
    "        self.dimension = dimension",
    "        self.vectors: Dict[str, List[float]] = {}",
    "        self.metadata: Dict[str, Dict] = {}",
    "        self._index_dirty = False",
    "    ",
    "    def insert(self, item_id: str, vector: List[float], metadata: Dict = None) -> None:",
    "        \"\"\"Insert or update a vector in the database.\"\"\"",
    "        if len(vector) != self.dimension:",
    "            raise ValueError(f\"Vector dimension mismatch: expected {self.dimension}, got {len(vector)}\")",
    "        self.vectors[item_id] = vector",
    "        self.metadata[item_id] = metadata or {}",
    "        self._index_dirty = True",
    "    ",
    "    def search(",
    "        self, ",
    "        query_vector: List[float], ",
    "        k: int, ",
    "        filter_fn: Optional[callable] = None",
    "    ) -> List[Tuple[str, float]]:",
    "        \"\"\"",
    "        ANN search - returns (item_id, similarity_score) pairs.",
    "        In production, this would use HNSW or IVF index for O(log N).",
    "        \"\"\"",
    "        if not self.vectors:",
    "            return []",
    "        ",
    "        scores = []",
    "        for item_id, vector in self.vectors.items():",
    "            # Apply filter if provided",
    "            if filter_fn and not filter_fn(self.metadata.get(item_id, {})):",
    "                continue",
    "            score = self._cosine_similarity(query_vector, vector)",
    "            scores.append((item_id, score))",
    "        ",
    "        # Return top-k by score",
    "        scores.sort(key=lambda x: -x[1])",
    "        return scores[:k]",
    "    ",
    "    def _cosine_similarity(self, v1: List[float], v2: List[float]) -> float:",
    "        \"\"\"Compute cosine similarity between two vectors.\"\"\"",
    "        dot = sum(a * b for a, b in zip(v1, v2))",
    "        norm1 = math.sqrt(sum(a * a for a in v1))",
    "        norm2 = math.sqrt(sum(b * b for b in v2))",
    "        if norm1 == 0 or norm2 == 0:",
    "            return 0.0",
    "        return dot / (norm1 * norm2)",
    "    ",
    "    def delete(self, item_id: str) -> bool:",
    "        \"\"\"Remove a vector from the database.\"\"\"",
    "        if item_id in self.vectors:",
    "            del self.vectors[item_id]",
    "            del self.metadata[item_id]",
    "            return True",
    "        return False",
    "    ",
    "    def size(self) -> int:",
    "        \"\"\"Return number of vectors in database.\"\"\"",
    "        return len(self.vectors)",
    "",
    "",
    "class EmbeddingService:",
    "    \"\"\"",
    "    Generates embeddings for users and articles.",
    "    In production, this would use a trained transformer model (BERT, etc.).",
    "    \"\"\"",
    "    ",
    "    def __init__(self, dimension: int = EMBEDDING_DIMENSION):",
    "        self.dimension = dimension",
    "        self._category_vectors: Dict[str, List[float]] = {}",
    "        self._publisher_vectors: Dict[str, List[float]] = {}",
    "    ",
    "    def generate_article_embedding(self, article: Article) -> List[float]:",
    "        \"\"\"",
    "        Generate article embedding based on content and metadata.",
    "        Combines category, publisher, and content signals.",
    "        \"\"\"",
    "        # Get base category vector",
    "        category_vec = self._get_category_vector(article.category)",
    "        ",
    "        # Get publisher variation",
    "        publisher_vec = self._get_publisher_vector(article.publisher_id)",
    "        ",
    "        # Combine with weighted sum",
    "        combined = []",
    "        for i in range(self.dimension):",
    "            val = 0.7 * category_vec[i] + 0.3 * publisher_vec[i]",
    "            combined.append(val)",
    "        ",
    "        # Add content-based variation (simulated)",
    "        content_hash = int(hashlib.md5(article.content.encode()).hexdigest()[:8], 16)",
    "        random.seed(content_hash)",
    "        for i in range(self.dimension):",
    "            combined[i] += random.gauss(0, 0.1)",
    "        ",
    "        return self._normalize(combined)",
    "    ",
    "    def generate_user_embedding(",
    "        self,",
    "        user_profile: UserProfile,",
    "        recent_engagements: List[Tuple[Article, float]]  # (article, weight)",
    "    ) -> List[float]:",
    "        \"\"\"",
    "        Generate user embedding from profile and recent behavior.",
    "        Combines explicit preferences with implicit signals.",
    "        \"\"\"",
    "        embedding = [0.0] * self.dimension",
    "        total_weight = 0.0",
    "        ",
    "        # Add category affinity signals",
    "        for category, affinity in user_profile.category_affinity.items():",
    "            cat_vec = self._get_category_vector(category)",
    "            for i in range(self.dimension):",
    "                embedding[i] += cat_vec[i] * affinity",
    "            total_weight += affinity",
    "        ",
    "        # Add signals from recent article interactions",
    "        for article, weight in recent_engagements:",
    "            if article.embedding:",
    "                for i in range(self.dimension):",
    "                    embedding[i] += article.embedding[i] * weight",
    "                total_weight += abs(weight)",
    "        ",
    "        # Add followed publisher signals (lower weight)",
    "        for publisher_id in user_profile.followed_publishers:",
    "            pub_vec = self._get_publisher_vector(publisher_id)",
    "            for i in range(self.dimension):",
    "                embedding[i] += pub_vec[i] * 0.5",
    "            total_weight += 0.5",
    "        ",
    "        # Normalize",
    "        if total_weight > 0:",
    "            embedding = [e / total_weight for e in embedding]",
    "        ",
    "        return self._normalize(embedding)",
    "    ",
    "    def _get_category_vector(self, category: str) -> List[float]:",
    "        \"\"\"Get or create deterministic vector for category.\"\"\"",
    "        if category not in self._category_vectors:",
    "            random.seed(hash(category))",
    "            vec = [random.gauss(0, 1) for _ in range(self.dimension)]",
    "            self._category_vectors[category] = self._normalize(vec)",
    "        return self._category_vectors[category]",
    "    ",
    "    def _get_publisher_vector(self, publisher_id: str) -> List[float]:",
    "        \"\"\"Get or create deterministic vector for publisher.\"\"\"",
    "        if publisher_id not in self._publisher_vectors:",
    "            random.seed(hash(publisher_id) + 12345)",
    "            vec = [random.gauss(0, 1) for _ in range(self.dimension)]",
    "            self._publisher_vectors[publisher_id] = self._normalize(vec)",
    "        return self._publisher_vectors[publisher_id]",
    "    ",
    "    def _normalize(self, vec: List[float]) -> List[float]:",
    "        \"\"\"L2 normalize a vector.\"\"\"",
    "        norm = math.sqrt(sum(v * v for v in vec))",
    "        if norm == 0:",
    "            return vec",
    "        return [v / norm for v in vec]",
    "",
    "",
    "class MultiArmedBandit:",
    "    \"\"\"",
    "    Thompson Sampling for exploration/exploitation balance.",
    "    Tracks success/failure rates for each category per user.",
    "    \"\"\"",
    "    ",
    "    def __init__(self):",
    "        # (alpha, beta) params for Beta distribution per (user, category)",
    "        self._params: Dict[str, Dict[str, Tuple[float, float]]] = defaultdict(",
    "            lambda: defaultdict(lambda: (1.0, 1.0))",
    "        )",
    "    ",
    "    def select_exploration_categories(",
    "        self,",
    "        user_id: str,",
    "        all_categories: List[str],",
    "        known_categories: Set[str],",
    "        num_explore: int",
    "    ) -> List[str]:",
    "        \"\"\"",
    "        Select categories to explore using Thompson Sampling.",
    "        Prioritizes categories user hasn't engaged with much.",
    "        \"\"\"",
    "        # Filter to unknown/low-engagement categories",
    "        explore_candidates = [c for c in all_categories if c not in known_categories]",
    "        ",
    "        if not explore_candidates:",
    "            explore_candidates = all_categories",
    "        ",
    "        # Sample from Beta distribution for each candidate",
    "        samples = []",
    "        for category in explore_candidates:",
    "            alpha, beta = self._params[user_id][category]",
    "            sample = random.betavariate(alpha, beta)",
    "            samples.append((category, sample))",
    "        ",
    "        # Return top categories by sampled value",
    "        samples.sort(key=lambda x: -x[1])",
    "        return [cat for cat, _ in samples[:num_explore]]",
    "    ",
    "    def update(self, user_id: str, category: str, engaged: bool) -> None:",
    "        \"\"\"Update category params based on engagement outcome.\"\"\"",
    "        alpha, beta = self._params[user_id][category]",
    "        if engaged:",
    "            self._params[user_id][category] = (alpha + 1, beta)",
    "        else:",
    "            self._params[user_id][category] = (alpha, beta + 1)",
    "",
    "",
    "class RankingModel:",
    "    \"\"\"",
    "    Lightweight ranking model for scoring candidates.",
    "    In production, this would be a trained neural network or gradient boosting model.",
    "    \"\"\"",
    "    ",
    "    def __init__(self):",
    "        # Feature weights (in production, learned from training)",
    "        self.weights = {",
    "            'embedding_similarity': 1.0,",
    "            'freshness': 0.2,",
    "            'quality': 0.15,",
    "            'publisher_affinity': 0.1,",
    "            'exploration_bonus': 0.1",
    "        }",
    "    ",
    "    def score(",
    "        self,",
    "        user_embedding: List[float],",
    "        article: Article,",
    "        user_profile: UserProfile,",
    "        is_exploration: bool = False",
    "    ) -> float:",
    "        \"\"\"Compute relevance score for a (user, article) pair.\"\"\"",
    "        score = 0.0",
    "        ",
    "        # Base embedding similarity",
    "        if user_embedding and article.embedding:",
    "            similarity = sum(a * b for a, b in zip(user_embedding, article.embedding))",
    "            score += self.weights['embedding_similarity'] * similarity",
    "        ",
    "        # Freshness boost (exponential decay)",
    "        age_hours = (datetime.now() - article.published_at).total_seconds() / 3600",
    "        freshness = math.exp(-age_hours / 24)  # Half-life of 24 hours",
    "        score += self.weights['freshness'] * freshness",
    "        ",
    "        # Quality score",
    "        score += self.weights['quality'] * article.quality_score",
    "        ",
    "        # Publisher affinity",
    "        if article.publisher_id in user_profile.followed_publishers:",
    "            score += self.weights['publisher_affinity']",
    "        ",
    "        # Exploration bonus",
    "        if is_exploration:",
    "            score += self.weights['exploration_bonus']",
    "        ",
    "        return score",
    "",
    "",
    "class ReRanker:",
    "    \"\"\"",
    "    Post-processing layer for diversity and business rules.",
    "    Ensures feed isn't dominated by single category/publisher.",
    "    \"\"\"",
    "    ",
    "    def __init__(self, max_per_category: int = 2, max_per_publisher: int = 2):",
    "        self.max_per_category = max_per_category",
    "        self.max_per_publisher = max_per_publisher",
    "    ",
    "    def rerank(",
    "        self,",
    "        scored_articles: List[Tuple[Article, float]],",
    "        count: int,",
    "        seen_article_ids: Set[str] = None",
    "    ) -> List[Article]:",
    "        \"\"\"Apply diversity rules and return re-ranked articles.\"\"\"",
    "        seen_article_ids = seen_article_ids or set()",
    "        ",
    "        selected: List[Article] = []",
    "        category_counts: Dict[str, int] = defaultdict(int)",
    "        publisher_counts: Dict[str, int] = defaultdict(int)",
    "        ",
    "        for article, score in scored_articles:",
    "            # Skip already seen articles",
    "            if article.id in seen_article_ids:",
    "                continue",
    "            ",
    "            # Check category limit",
    "            if category_counts[article.category] >= self.max_per_category:",
    "                # Allow overflow if we haven't filled the feed",
    "                if len(selected) >= count // 2:",
    "                    continue",
    "            ",
    "            # Check publisher limit",
    "            if publisher_counts[article.publisher_id] >= self.max_per_publisher:",
    "                continue",
    "            ",
    "            # Check adjacent publisher (no back-to-back from same publisher)",
    "            if selected and selected[-1].publisher_id == article.publisher_id:",
    "                continue",
    "            ",
    "            selected.append(article)",
    "            category_counts[article.category] += 1",
    "            publisher_counts[article.publisher_id] += 1",
    "            ",
    "            if len(selected) >= count:",
    "                break",
    "        ",
    "        return selected",
    "",
    "",
    "class NewsAggregatorML:",
    "    \"\"\"",
    "    Main class for ML-powered news recommendations.",
    "    Extends basic news aggregator with intelligent personalization.",
    "    \"\"\"",
    "    ",
    "    def __init__(self):",
    "        # Core data stores",
    "        self._articles: Dict[str, Article] = {}",
    "        self._user_profiles: Dict[str, UserProfile] = {}",
    "        self._engagement_history: Dict[str, List[EngagementEvent]] = defaultdict(list)",
    "        ",
    "        # ML components",
    "        self._vector_db = VectorDatabase()",
    "        self._embedding_service = EmbeddingService()",
    "        self._ranking_model = RankingModel()",
    "        self._bandit = MultiArmedBandit()",
    "        self._reranker = ReRanker()",
    "        ",
    "        # Category tracking",
    "        self._all_categories: Set[str] = set()",
    "        ",
    "        # Configuration",
    "        self._exploration_rate = 0.2  # 20% exploration",
    "        self._candidate_multiplier = 10  # Fetch 10x candidates for re-ranking",
    "    ",
    "    def add_article(self, article: Article) -> None:",
    "        \"\"\"",
    "        Add article to the system with embedding generation.",
    "        Called during article ingestion pipeline.",
    "        \"\"\"",
    "        # Generate and store embedding",
    "        article.embedding = self._embedding_service.generate_article_embedding(article)",
    "        ",
    "        # Store article",
    "        self._articles[article.id] = article",
    "        self._all_categories.add(article.category)",
    "        ",
    "        # Index in vector database",
    "        self._vector_db.insert(",
    "            article.id,",
    "            article.embedding,",
    "            metadata={",
    "                'category': article.category,",
    "                'publisher_id': article.publisher_id,",
    "                'published_at': article.published_at.isoformat()",
    "            }",
    "        )",
    "    ",
    "    def register_user(",
    "        self,",
    "        user_id: str,",
    "        followed_publishers: List[str] = None,",
    "        followed_categories: List[str] = None",
    "    ) -> None:",
    "        \"\"\"Register a new user with initial preferences.\"\"\"",
    "        profile = UserProfile(",
    "            user_id=user_id,",
    "            followed_publishers=set(followed_publishers or []),",
    "            followed_categories=set(followed_categories or [])",
    "        )",
    "        ",
    "        # Initialize category affinity from explicit follows",
    "        for cat in profile.followed_categories:",
    "            profile.category_affinity[cat] = 1.0",
    "        ",
    "        self._user_profiles[user_id] = profile",
    "    ",
    "    def record_engagement(",
    "        self,",
    "        user_id: str,",
    "        article_id: str,",
    "        engagement_type: EngagementType,",
    "        duration_seconds: int",
    "    ) -> None:",
    "        \"\"\"",
    "        Record user engagement with an article.",
    "        Updates user profile and triggers embedding refresh if needed.",
    "        \"\"\"",
    "        # Create engagement event",
    "        event = EngagementEvent(",
    "            user_id=user_id,",
    "            article_id=article_id,",
    "            engagement_type=engagement_type,",
    "            duration_seconds=duration_seconds,",
    "            timestamp=datetime.now()",
    "        )",
    "        ",
    "        # Store event",
    "        self._engagement_history[user_id].append(event)",
    "        ",
    "        # Update user profile",
    "        self._update_user_profile(user_id, event)",
    "        ",
    "        # Update bandit for exploration tracking",
    "        article = self._articles.get(article_id)",
    "        if article:",
    "            engaged = event.compute_weight() > 0",
    "            self._bandit.update(user_id, article.category, engaged)",
    "    ",
    "    def _update_user_profile(self, user_id: str, event: EngagementEvent) -> None:",
    "        \"\"\"Update user profile based on engagement.\"\"\"",
    "        # Ensure user exists",
    "        if user_id not in self._user_profiles:",
    "            self.register_user(user_id)",
    "        ",
    "        profile = self._user_profiles[user_id]",
    "        article = self._articles.get(event.article_id)",
    "        ",
    "        if not article:",
    "            return",
    "        ",
    "        # Update category affinity",
    "        weight = event.compute_weight()",
    "        current_affinity = profile.category_affinity.get(article.category, 0.0)",
    "        ",
    "        # Exponential moving average",
    "        alpha = 0.3  # Learning rate",
    "        new_affinity = current_affinity + alpha * (weight - current_affinity)",
    "        profile.category_affinity[article.category] = max(0.0, new_affinity)",
    "        ",
    "        # Mark embedding as stale",
    "        profile.last_embedding_update = datetime.min",
    "    ",
    "    def _get_user_embedding(self, user_id: str) -> List[float]:",
    "        \"\"\"Get or compute user embedding.\"\"\"",
    "        profile = self._user_profiles.get(user_id)",
    "        if not profile:",
    "            # Cold start: return zero vector",
    "            return [0.0] * EMBEDDING_DIMENSION",
    "        ",
    "        # Check if embedding is fresh (< 1 hour old)",
    "        if profile.embedding and \\",
    "           (datetime.now() - profile.last_embedding_update).total_seconds() < 3600:",
    "            return profile.embedding",
    "        ",
    "        # Compute fresh embedding from recent engagements",
    "        recent_engagements = self._get_recent_engagements(user_id, hours=24)",
    "        weighted_articles = []",
    "        ",
    "        for event in recent_engagements:",
    "            article = self._articles.get(event.article_id)",
    "            if article:",
    "                weight = event.compute_weight()",
    "                weighted_articles.append((article, weight))",
    "        ",
    "        # Generate embedding",
    "        profile.embedding = self._embedding_service.generate_user_embedding(",
    "            profile, weighted_articles",
    "        )",
    "        profile.last_embedding_update = datetime.now()",
    "        ",
    "        return profile.embedding",
    "    ",
    "    def _get_recent_engagements(",
    "        self,",
    "        user_id: str,",
    "        hours: int = 24",
    "    ) -> List[EngagementEvent]:",
    "        \"\"\"Get user's recent engagements within time window.\"\"\"",
    "        cutoff = datetime.now() - timedelta(hours=hours)",
    "        events = self._engagement_history.get(user_id, [])",
    "        return [e for e in events if e.timestamp > cutoff]",
    "    ",
    "    def get_recommendations(self, user_id: str, count: int) -> List[Article]:",
    "        \"\"\"",
    "        Get ML-ranked recommendations for user.",
    "        Uses two-tower + ANN + re-ranking pipeline.",
    "        \"\"\"",
    "        if count <= 0:",
    "            return []",
    "        ",
    "        profile = self._user_profiles.get(user_id)",
    "        if not profile:",
    "            # Cold start: return popular recent articles",
    "            return self._get_cold_start_recommendations(count)",
    "        ",
    "        # Step 1: Get user embedding",
    "        user_embedding = self._get_user_embedding(user_id)",
    "        ",
    "        # Step 2: Get seen articles to filter",
    "        seen_ids = {e.article_id for e in self._engagement_history.get(user_id, [])}",
    "        ",
    "        # Step 3: Candidate retrieval via ANN",
    "        num_candidates = count * self._candidate_multiplier",
    "        ",
    "        # Filter function for freshness (articles from last 7 days)",
    "        week_ago = (datetime.now() - timedelta(days=7)).isoformat()",
    "        filter_fn = lambda meta: meta.get('published_at', '') >= week_ago",
    "        ",
    "        candidates = self._vector_db.search(",
    "            user_embedding,",
    "            k=num_candidates,",
    "            filter_fn=filter_fn",
    "        )",
    "        ",
    "        # Step 4: Determine exploration categories",
    "        known_categories = set(profile.category_affinity.keys())",
    "        num_explore = max(1, int(count * self._exploration_rate))",
    "        explore_categories = self._bandit.select_exploration_categories(",
    "            user_id,",
    "            list(self._all_categories),",
    "            known_categories,",
    "            num_explore",
    "        )",
    "        ",
    "        # Step 5: Score candidates",
    "        scored_articles: List[Tuple[Article, float]] = []",
    "        ",
    "        for article_id, ann_score in candidates:",
    "            article = self._articles.get(article_id)",
    "            if not article:",
    "                continue",
    "            ",
    "            is_exploration = article.category in explore_categories",
    "            score = self._ranking_model.score(",
    "                user_embedding,",
    "                article,",
    "                profile,",
    "                is_exploration=is_exploration",
    "            )",
    "            scored_articles.append((article, score))",
    "        ",
    "        # Sort by score",
    "        scored_articles.sort(key=lambda x: -x[1])",
    "        ",
    "        # Step 6: Re-rank for diversity",
    "        final_articles = self._reranker.rerank(",
    "            scored_articles,",
    "            count,",
    "            seen_article_ids=seen_ids",
    "        )",
    "        ",
    "        return final_articles",
    "    ",
    "    def _get_cold_start_recommendations(self, count: int) -> List[Article]:",
    "        \"\"\"Return popular recent articles for new users.\"\"\"",
    "        # Sort by recency and quality",
    "        articles = list(self._articles.values())",
    "        articles.sort(",
    "            key=lambda a: (a.quality_score, a.published_at),",
    "            reverse=True",
    "        )",
    "        return articles[:count]",
    "    ",
    "    def get_similar_users(self, user_id: str, count: int) -> List[str]:",
    "        \"\"\"Find users with similar embeddings (collaborative filtering).\"\"\"",
    "        user_embedding = self._get_user_embedding(user_id)",
    "        if not user_embedding or all(v == 0 for v in user_embedding):",
    "            return []",
    "        ",
    "        # Compare with all other users",
    "        similarities = []",
    "        for other_id, profile in self._user_profiles.items():",
    "            if other_id == user_id:",
    "                continue",
    "            other_embedding = self._get_user_embedding(other_id)",
    "            if not other_embedding:",
    "                continue",
    "            ",
    "            # Cosine similarity",
    "            similarity = sum(a * b for a, b in zip(user_embedding, other_embedding))",
    "            similarities.append((other_id, similarity))",
    "        ",
    "        similarities.sort(key=lambda x: -x[1])",
    "        return [uid for uid, _ in similarities[:count]]",
    "    ",
    "    def train_model(self) -> Dict[str, any]:",
    "        \"\"\"",
    "        Batch training job for ML model.",
    "        In production, this would retrain the two-tower network.",
    "        Returns training metrics.",
    "        \"\"\"",
    "        # Collect training data",
    "        positive_samples = []",
    "        negative_samples = []",
    "        ",
    "        for user_id, events in self._engagement_history.items():",
    "            for event in events:",
    "                weight = event.compute_weight()",
    "                if weight > 0:",
    "                    positive_samples.append((user_id, event.article_id, weight))",
    "                else:",
    "                    negative_samples.append((user_id, event.article_id, abs(weight)))",
    "        ",
    "        # Recompute all user embeddings",
    "        for user_id in self._user_profiles:",
    "            profile = self._user_profiles[user_id]",
    "            profile.last_embedding_update = datetime.min  # Force refresh",
    "            self._get_user_embedding(user_id)",
    "        ",
    "        return {",
    "            'positive_samples': len(positive_samples),",
    "            'negative_samples': len(negative_samples),",
    "            'users_updated': len(self._user_profiles),",
    "            'articles_indexed': self._vector_db.size()",
    "        }",
    "",
    "",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550",
    "# DEMONSTRATION",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550",
    "",
    "def main():",
    "    \"\"\"Demonstrate ML-powered recommendation system.\"\"\"",
    "    print(\"=\"*70)",
    "    print(\"NEWS AGGREGATOR - PART 3: ML-POWERED RECOMMENDATIONS\")",
    "    print(\"=\"*70)",
    "    ",
    "    # Initialize system",
    "    aggregator = NewsAggregatorML()",
    "    ",
    "    # Add sample articles",
    "    print(\"\\n\ud83d\udcf0 Adding articles to the system...\")",
    "    ",
    "    articles_data = [",
    "        (\"tech_1\", \"New AI Breakthrough in NLP\", \"techcrunch\", \"tech\", \"AI models...\"),",
    "        (\"tech_2\", \"Python 4.0 Released\", \"techcrunch\", \"tech\", \"Python announces...\"),",
    "        (\"tech_3\", \"Cloud Computing Trends 2024\", \"wired\", \"tech\", \"Cloud tech...\"),",
    "        (\"tech_4\", \"Quantum Computing Advances\", \"ars_technica\", \"tech\", \"Quantum...\"),",
    "        (\"tech_5\", \"Cybersecurity Best Practices\", \"wired\", \"tech\", \"Security...\"),",
    "        (\"ai_1\", \"GPT-5 Announced\", \"openai_blog\", \"ai\", \"OpenAI reveals...\"),",
    "        (\"ai_2\", \"Machine Learning in Healthcare\", \"mit_news\", \"ai\", \"ML applications...\"),",
    "        (\"sports_1\", \"NBA Finals Preview\", \"espn\", \"sports\", \"Lakers vs...\"),",
    "        (\"sports_2\", \"World Cup Qualifiers\", \"espn\", \"sports\", \"Soccer news...\"),",
    "        (\"science_1\", \"Mars Rover Discovery\", \"nasa\", \"science\", \"Perseverance...\"),",
    "        (\"science_2\", \"Climate Change Report\", \"nature\", \"science\", \"New findings...\"),",
    "        (\"business_1\", \"Stock Market Update\", \"wsj\", \"business\", \"Markets...\"),",
    "        (\"business_2\", \"Startup Funding Trends\", \"techcrunch\", \"business\", \"VC...\"),",
    "    ]",
    "    ",
    "    for aid, title, pub, cat, content in articles_data:",
    "        article = Article(",
    "            id=aid,",
    "            title=title,",
    "            publisher_id=pub,",
    "            category=cat,",
    "            content=content,",
    "            published_at=datetime.now() - timedelta(hours=random.randint(1, 48)),",
    "            quality_score=random.uniform(0.6, 0.95)",
    "        )",
    "        aggregator.add_article(article)",
    "        print(f\"   Added: [{cat}] {title}\")",
    "    ",
    "    # Register user",
    "    print(\"\\n\ud83d\udc64 Registering user_1...\")",
    "    aggregator.register_user(",
    "        \"user_1\",",
    "        followed_publishers=[\"techcrunch\", \"wired\"],",
    "        followed_categories=[\"tech\"]",
    "    )",
    "    ",
    "    # Record engagements",
    "    print(\"\\n\ud83d\udcca Recording user engagements...\")",
    "    ",
    "    engagements = [",
    "        (\"user_1\", \"tech_1\", EngagementType.READ, 180),  # Strong interest in AI",
    "        (\"user_1\", \"tech_2\", EngagementType.READ, 240),  # Strong interest in Python",
    "        (\"user_1\", \"ai_1\", EngagementType.SHARE, 300),   # Shared AI article!",
    "        (\"user_1\", \"sports_1\", EngagementType.CLICK, 10), # Quick bounce from sports",
    "    ]",
    "    ",
    "    for uid, aid, etype, duration in engagements:",
    "        aggregator.record_engagement(uid, aid, etype, duration)",
    "        action = \"read\" if etype == EngagementType.READ else \"clicked\" if etype == EngagementType.CLICK else \"shared\"",
    "        print(f\"   User {action} '{aid}' for {duration}s\")",
    "    ",
    "    # Get recommendations",
    "    print(\"\\n\ud83c\udfaf Getting ML-powered recommendations for user_1...\")",
    "    recommendations = aggregator.get_recommendations(\"user_1\", 5)",
    "    ",
    "    print(\"\\n\" + \"\u2500\"*50)",
    "    print(\"PERSONALIZED FEED FOR USER_1:\")",
    "    print(\"\u2500\"*50)",
    "    for i, article in enumerate(recommendations, 1):",
    "        print(f\"   {i}. [{article.category}] {article.title}\")",
    "        print(f\"      Publisher: {article.publisher_id}\")",
    "    ",
    "    # Explain the recommendation",
    "    print(\"\\n\ud83d\udca1 Why these recommendations?\")",
    "    print(\"   \u2022 User spent 180s and 240s reading tech articles \u2192 High tech affinity\")",
    "    print(\"   \u2022 User SHARED an AI article \u2192 Very strong AI interest signal\")",
    "    print(\"   \u2022 User bounced from sports in 10s \u2192 Negative sports signal\")",
    "    print(\"   \u2022 System includes 1 exploration article for diversity\")",
    "    ",
    "    # Train model (batch job)",
    "    print(\"\\n\ud83d\udd04 Running batch training job...\")",
    "    metrics = aggregator.train_model()",
    "    print(f\"   Training complete: {metrics}\")",
    "    ",
    "    # Find similar users",
    "    print(\"\\n\ud83d\udc65 Registering more users for collaborative filtering demo...\")",
    "    ",
    "    aggregator.register_user(\"user_2\", followed_categories=[\"tech\", \"ai\"])",
    "    aggregator.record_engagement(\"user_2\", \"tech_1\", EngagementType.READ, 200)",
    "    aggregator.record_engagement(\"user_2\", \"ai_1\", EngagementType.READ, 180)",
    "    ",
    "    aggregator.register_user(\"user_3\", followed_categories=[\"sports\"])",
    "    aggregator.record_engagement(\"user_3\", \"sports_1\", EngagementType.READ, 300)",
    "    aggregator.record_engagement(\"user_3\", \"sports_2\", EngagementType.READ, 250)",
    "    ",
    "    similar_users = aggregator.get_similar_users(\"user_1\", 2)",
    "    print(f\"\\n   Users similar to user_1: {similar_users}\")",
    "    print(\"   (user_2 is similar - also likes tech/AI. user_3 is different - likes sports)\")",
    "    ",
    "    print(\"\\n\" + \"=\"*70)",
    "    print(\"DEMO COMPLETE\")",
    "    print(\"=\"*70)",
    "",
    "",
    "if __name__ == \"__main__\":",
    "    main()"
  ],
  "solution_java_lines": [
    "import java.time.Duration;",
    "import java.time.Instant;",
    "import java.util.*;",
    "import java.util.stream.*;",
    "import java.security.MessageDigest;",
    "",
    "/**",
    " * News Feed Aggregator - Part 3: ML-Powered Recommendations",
    " * ",
    " * Implements a production-grade recommendation system using:",
    " * - Two-tower neural network architecture (simulated)",
    " * - Approximate Nearest Neighbor (ANN) search",
    " * - Feature store for user/article features",
    " * - Multi-armed bandit for exploration/exploitation",
    " * - Re-ranking for diversity enforcement",
    " */",
    "public class NewsAggregatorML {",
    "    ",
    "    // \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550",
    "    // ENUMS AND DATA CLASSES",
    "    // \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550",
    "    ",
    "    public enum EngagementType {",
    "        CLICK(1.0),",
    "        READ(3.0),",
    "        SHARE(5.0),",
    "        SAVE(4.0);",
    "        ",
    "        private final double weight;",
    "        ",
    "        EngagementType(double weight) {",
    "            this.weight = weight;",
    "        }",
    "        ",
    "        public double getWeight() { return weight; }",
    "    }",
    "    ",
    "    public static class Article {",
    "        public String id;",
    "        public String title;",
    "        public String publisherId;",
    "        public String category;",
    "        public String content;",
    "        public Instant publishedAt;",
    "        public double[] embedding;",
    "        public double qualityScore;",
    "        ",
    "        public Article(String id, String title, String publisherId, ",
    "                      String category, String content, Instant publishedAt) {",
    "            this.id = id;",
    "            this.title = title;",
    "            this.publisherId = publisherId;",
    "            this.category = category;",
    "            this.content = content;",
    "            this.publishedAt = publishedAt;",
    "            this.qualityScore = 0.5;",
    "        }",
    "    }",
    "    ",
    "    public static class EngagementEvent {",
    "        public String userId;",
    "        public String articleId;",
    "        public EngagementType type;",
    "        public int durationSeconds;",
    "        public Instant timestamp;",
    "        ",
    "        public EngagementEvent(String userId, String articleId, ",
    "                              EngagementType type, int durationSeconds) {",
    "            this.userId = userId;",
    "            this.articleId = articleId;",
    "            this.type = type;",
    "            this.durationSeconds = durationSeconds;",
    "            this.timestamp = Instant.now();",
    "        }",
    "        ",
    "        public double computeWeight() {",
    "            double baseWeight = type.getWeight();",
    "            if (type == EngagementType.READ) {",
    "                return baseWeight * Math.min(durationSeconds / 60.0, 5.0);",
    "            } else if (type == EngagementType.CLICK && durationSeconds < 30) {",
    "                return -0.5;",
    "            }",
    "            return baseWeight;",
    "        }",
    "    }",
    "    ",
    "    public static class UserProfile {",
    "        public String userId;",
    "        public Set<String> followedPublishers = new HashSet<>();",
    "        public Set<String> followedCategories = new HashSet<>();",
    "        public double[] embedding;",
    "        public Map<String, Double> categoryAffinity = new HashMap<>();",
    "        public Instant lastEmbeddingUpdate = Instant.MIN;",
    "    }",
    "    ",
    "    // \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550",
    "    // ML COMPONENTS",
    "    // \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550",
    "    ",
    "    private static final int EMBEDDING_DIM = 128;",
    "    private static final double EXPLORATION_RATE = 0.2;",
    "    ",
    "    // Core data stores",
    "    private Map<String, Article> articles = new HashMap<>();",
    "    private Map<String, UserProfile> userProfiles = new HashMap<>();",
    "    private Map<String, List<EngagementEvent>> engagementHistory = new HashMap<>();",
    "    private Set<String> allCategories = new HashSet<>();",
    "    ",
    "    // Vector database (simulated)",
    "    private Map<String, double[]> vectorIndex = new HashMap<>();",
    "    private Map<String, Map<String, Object>> vectorMetadata = new HashMap<>();",
    "    ",
    "    // Category/Publisher base vectors (cached)",
    "    private Map<String, double[]> categoryVectors = new HashMap<>();",
    "    private Map<String, double[]> publisherVectors = new HashMap<>();",
    "    ",
    "    // Multi-armed bandit state",
    "    private Map<String, Map<String, double[]>> banditParams = new HashMap<>();",
    "    ",
    "    private Random random = new Random();",
    "    ",
    "    // \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550",
    "    // CORE API",
    "    // \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550",
    "    ",
    "    public void addArticle(Article article) {",
    "        article.embedding = generateArticleEmbedding(article);",
    "        articles.put(article.id, article);",
    "        allCategories.add(article.category);",
    "        ",
    "        // Index in vector DB",
    "        vectorIndex.put(article.id, article.embedding);",
    "        Map<String, Object> meta = new HashMap<>();",
    "        meta.put(\"category\", article.category);",
    "        meta.put(\"publisherId\", article.publisherId);",
    "        meta.put(\"publishedAt\", article.publishedAt);",
    "        vectorMetadata.put(article.id, meta);",
    "    }",
    "    ",
    "    public void registerUser(String userId, List<String> publishers, List<String> categories) {",
    "        UserProfile profile = new UserProfile();",
    "        profile.userId = userId;",
    "        if (publishers != null) profile.followedPublishers.addAll(publishers);",
    "        if (categories != null) {",
    "            profile.followedCategories.addAll(categories);",
    "            for (String cat : categories) {",
    "                profile.categoryAffinity.put(cat, 1.0);",
    "            }",
    "        }",
    "        userProfiles.put(userId, profile);",
    "    }",
    "    ",
    "    public void recordEngagement(String userId, String articleId, ",
    "                                EngagementType type, int durationSeconds) {",
    "        EngagementEvent event = new EngagementEvent(userId, articleId, type, durationSeconds);",
    "        engagementHistory.computeIfAbsent(userId, k -> new ArrayList<>()).add(event);",
    "        updateUserProfile(userId, event);",
    "        ",
    "        Article article = articles.get(articleId);",
    "        if (article != null) {",
    "            updateBandit(userId, article.category, event.computeWeight() > 0);",
    "        }",
    "    }",
    "    ",
    "    public List<Article> getRecommendations(String userId, int count) {",
    "        if (count <= 0) return Collections.emptyList();",
    "        ",
    "        UserProfile profile = userProfiles.get(userId);",
    "        if (profile == null) {",
    "            return getColdStartRecommendations(count);",
    "        }",
    "        ",
    "        // Step 1: Get user embedding",
    "        double[] userEmbedding = getUserEmbedding(userId);",
    "        ",
    "        // Step 2: Get seen articles",
    "        Set<String> seenIds = new HashSet<>();",
    "        List<EngagementEvent> events = engagementHistory.getOrDefault(userId, Collections.emptyList());",
    "        for (EngagementEvent e : events) {",
    "            seenIds.add(e.articleId);",
    "        }",
    "        ",
    "        // Step 3: ANN search",
    "        int numCandidates = count * 10;",
    "        Instant weekAgo = Instant.now().minus(Duration.ofDays(7));",
    "        List<Map.Entry<String, Double>> candidates = annSearch(userEmbedding, numCandidates, weekAgo);",
    "        ",
    "        // Step 4: Exploration categories",
    "        Set<String> knownCategories = profile.categoryAffinity.keySet();",
    "        int numExplore = Math.max(1, (int)(count * EXPLORATION_RATE));",
    "        List<String> exploreCategories = selectExplorationCategories(",
    "            userId, new ArrayList<>(allCategories), knownCategories, numExplore);",
    "        Set<String> exploreCategorySet = new HashSet<>(exploreCategories);",
    "        ",
    "        // Step 5: Score candidates",
    "        List<Map.Entry<Article, Double>> scoredArticles = new ArrayList<>();",
    "        for (Map.Entry<String, Double> candidate : candidates) {",
    "            Article article = articles.get(candidate.getKey());",
    "            if (article == null) continue;",
    "            ",
    "            boolean isExploration = exploreCategorySet.contains(article.category);",
    "            double score = scoreArticle(userEmbedding, article, profile, isExploration);",
    "            scoredArticles.add(new AbstractMap.SimpleEntry<>(article, score));",
    "        }",
    "        ",
    "        scoredArticles.sort((a, b) -> Double.compare(b.getValue(), a.getValue()));",
    "        ",
    "        // Step 6: Re-rank for diversity",
    "        return rerank(scoredArticles, count, seenIds);",
    "    }",
    "    ",
    "    // \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550",
    "    // HELPER METHODS",
    "    // \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550",
    "    ",
    "    private double[] generateArticleEmbedding(Article article) {",
    "        double[] categoryVec = getCategoryVector(article.category);",
    "        double[] publisherVec = getPublisherVector(article.publisherId);",
    "        ",
    "        double[] combined = new double[EMBEDDING_DIM];",
    "        for (int i = 0; i < EMBEDDING_DIM; i++) {",
    "            combined[i] = 0.7 * categoryVec[i] + 0.3 * publisherVec[i];",
    "        }",
    "        ",
    "        // Add content variation",
    "        random.setSeed(article.content.hashCode());",
    "        for (int i = 0; i < EMBEDDING_DIM; i++) {",
    "            combined[i] += random.nextGaussian() * 0.1;",
    "        }",
    "        ",
    "        return normalize(combined);",
    "    }",
    "    ",
    "    private double[] getUserEmbedding(String userId) {",
    "        UserProfile profile = userProfiles.get(userId);",
    "        if (profile == null) return new double[EMBEDDING_DIM];",
    "        ",
    "        // Check cache freshness",
    "        if (profile.embedding != null && ",
    "            Duration.between(profile.lastEmbeddingUpdate, Instant.now()).toHours() < 1) {",
    "            return profile.embedding;",
    "        }",
    "        ",
    "        double[] embedding = new double[EMBEDDING_DIM];",
    "        double totalWeight = 0;",
    "        ",
    "        // Add category affinity",
    "        for (Map.Entry<String, Double> entry : profile.categoryAffinity.entrySet()) {",
    "            double[] catVec = getCategoryVector(entry.getKey());",
    "            double weight = entry.getValue();",
    "            for (int i = 0; i < EMBEDDING_DIM; i++) {",
    "                embedding[i] += catVec[i] * weight;",
    "            }",
    "            totalWeight += weight;",
    "        }",
    "        ",
    "        // Add recent engagements",
    "        List<EngagementEvent> recentEvents = getRecentEngagements(userId, 24);",
    "        for (EngagementEvent event : recentEvents) {",
    "            Article article = articles.get(event.articleId);",
    "            if (article != null && article.embedding != null) {",
    "                double weight = event.computeWeight();",
    "                for (int i = 0; i < EMBEDDING_DIM; i++) {",
    "                    embedding[i] += article.embedding[i] * weight;",
    "                }",
    "                totalWeight += Math.abs(weight);",
    "            }",
    "        }",
    "        ",
    "        if (totalWeight > 0) {",
    "            for (int i = 0; i < EMBEDDING_DIM; i++) {",
    "                embedding[i] /= totalWeight;",
    "            }",
    "        }",
    "        ",
    "        profile.embedding = normalize(embedding);",
    "        profile.lastEmbeddingUpdate = Instant.now();",
    "        return profile.embedding;",
    "    }",
    "    ",
    "    private List<Map.Entry<String, Double>> annSearch(double[] query, int k, Instant minTime) {",
    "        List<Map.Entry<String, Double>> scores = new ArrayList<>();",
    "        ",
    "        for (Map.Entry<String, double[]> entry : vectorIndex.entrySet()) {",
    "            Map<String, Object> meta = vectorMetadata.get(entry.getKey());",
    "            Instant publishedAt = (Instant) meta.get(\"publishedAt\");",
    "            if (publishedAt.isBefore(minTime)) continue;",
    "            ",
    "            double similarity = cosineSimilarity(query, entry.getValue());",
    "            scores.add(new AbstractMap.SimpleEntry<>(entry.getKey(), similarity));",
    "        }",
    "        ",
    "        scores.sort((a, b) -> Double.compare(b.getValue(), a.getValue()));",
    "        return scores.subList(0, Math.min(k, scores.size()));",
    "    }",
    "    ",
    "    private double scoreArticle(double[] userEmbedding, Article article, ",
    "                               UserProfile profile, boolean isExploration) {",
    "        double score = 0;",
    "        ",
    "        // Embedding similarity",
    "        if (article.embedding != null) {",
    "            score += dotProduct(userEmbedding, article.embedding);",
    "        }",
    "        ",
    "        // Freshness boost",
    "        double ageHours = Duration.between(article.publishedAt, Instant.now()).toHours();",
    "        double freshness = Math.exp(-ageHours / 24.0);",
    "        score += 0.2 * freshness;",
    "        ",
    "        // Quality",
    "        score += 0.15 * article.qualityScore;",
    "        ",
    "        // Publisher affinity",
    "        if (profile.followedPublishers.contains(article.publisherId)) {",
    "            score += 0.1;",
    "        }",
    "        ",
    "        // Exploration bonus",
    "        if (isExploration) {",
    "            score += 0.1;",
    "        }",
    "        ",
    "        return score;",
    "    }",
    "    ",
    "    private List<Article> rerank(List<Map.Entry<Article, Double>> scored, ",
    "                                int count, Set<String> seenIds) {",
    "        List<Article> result = new ArrayList<>();",
    "        Map<String, Integer> categoryCounts = new HashMap<>();",
    "        Map<String, Integer> publisherCounts = new HashMap<>();",
    "        int maxPerCategory = 2;",
    "        int maxPerPublisher = 2;",
    "        ",
    "        for (Map.Entry<Article, Double> entry : scored) {",
    "            Article article = entry.getKey();",
    "            ",
    "            if (seenIds.contains(article.id)) continue;",
    "            ",
    "            int catCount = categoryCounts.getOrDefault(article.category, 0);",
    "            if (catCount >= maxPerCategory && result.size() >= count / 2) continue;",
    "            ",
    "            int pubCount = publisherCounts.getOrDefault(article.publisherId, 0);",
    "            if (pubCount >= maxPerPublisher) continue;",
    "            ",
    "            if (!result.isEmpty() && ",
    "                result.get(result.size()-1).publisherId.equals(article.publisherId)) {",
    "                continue;",
    "            }",
    "            ",
    "            result.add(article);",
    "            categoryCounts.merge(article.category, 1, Integer::sum);",
    "            publisherCounts.merge(article.publisherId, 1, Integer::sum);",
    "            ",
    "            if (result.size() >= count) break;",
    "        }",
    "        ",
    "        return result;",
    "    }",
    "    ",
    "    private void updateUserProfile(String userId, EngagementEvent event) {",
    "        UserProfile profile = userProfiles.computeIfAbsent(userId, k -> {",
    "            UserProfile p = new UserProfile();",
    "            p.userId = userId;",
    "            return p;",
    "        });",
    "        ",
    "        Article article = articles.get(event.articleId);",
    "        if (article == null) return;",
    "        ",
    "        double weight = event.computeWeight();",
    "        double current = profile.categoryAffinity.getOrDefault(article.category, 0.0);",
    "        double alpha = 0.3;",
    "        double newAffinity = current + alpha * (weight - current);",
    "        profile.categoryAffinity.put(article.category, Math.max(0, newAffinity));",
    "        profile.lastEmbeddingUpdate = Instant.MIN;",
    "    }",
    "    ",
    "    // Multi-armed bandit methods",
    "    private void updateBandit(String userId, String category, boolean engaged) {",
    "        Map<String, double[]> userParams = banditParams.computeIfAbsent(",
    "            userId, k -> new HashMap<>());",
    "        double[] params = userParams.computeIfAbsent(category, k -> new double[]{1.0, 1.0});",
    "        if (engaged) params[0] += 1;",
    "        else params[1] += 1;",
    "    }",
    "    ",
    "    private List<String> selectExplorationCategories(String userId, ",
    "            List<String> allCats, Set<String> knownCats, int num) {",
    "        List<String> candidates = allCats.stream()",
    "            .filter(c -> !knownCats.contains(c))",
    "            .collect(Collectors.toList());",
    "        if (candidates.isEmpty()) candidates = allCats;",
    "        ",
    "        Map<String, double[]> userParams = banditParams.getOrDefault(",
    "            userId, Collections.emptyMap());",
    "        ",
    "        List<Map.Entry<String, Double>> samples = new ArrayList<>();",
    "        for (String cat : candidates) {",
    "            double[] params = userParams.getOrDefault(cat, new double[]{1.0, 1.0});",
    "            double sample = sampleBeta(params[0], params[1]);",
    "            samples.add(new AbstractMap.SimpleEntry<>(cat, sample));",
    "        }",
    "        ",
    "        samples.sort((a, b) -> Double.compare(b.getValue(), a.getValue()));",
    "        return samples.stream().limit(num).map(Map.Entry::getKey).collect(Collectors.toList());",
    "    }",
    "    ",
    "    private double sampleBeta(double alpha, double beta) {",
    "        double x = sampleGamma(alpha);",
    "        double y = sampleGamma(beta);",
    "        return x / (x + y);",
    "    }",
    "    ",
    "    private double sampleGamma(double shape) {",
    "        if (shape < 1) {",
    "            return sampleGamma(shape + 1) * Math.pow(random.nextDouble(), 1.0/shape);",
    "        }",
    "        double d = shape - 1.0/3.0;",
    "        double c = 1.0 / Math.sqrt(9.0 * d);",
    "        while (true) {",
    "            double x, v;",
    "            do {",
    "                x = random.nextGaussian();",
    "                v = 1.0 + c * x;",
    "            } while (v <= 0);",
    "            v = v * v * v;",
    "            double u = random.nextDouble();",
    "            if (u < 1 - 0.0331 * (x*x) * (x*x)) return d * v;",
    "            if (Math.log(u) < 0.5*x*x + d*(1-v+Math.log(v))) return d * v;",
    "        }",
    "    }",
    "    ",
    "    // Vector operations",
    "    private double[] getCategoryVector(String category) {",
    "        return categoryVectors.computeIfAbsent(category, k -> {",
    "            random.setSeed(category.hashCode());",
    "            double[] vec = new double[EMBEDDING_DIM];",
    "            for (int i = 0; i < EMBEDDING_DIM; i++) vec[i] = random.nextGaussian();",
    "            return normalize(vec);",
    "        });",
    "    }",
    "    ",
    "    private double[] getPublisherVector(String publisher) {",
    "        return publisherVectors.computeIfAbsent(publisher, k -> {",
    "            random.setSeed(publisher.hashCode() + 12345);",
    "            double[] vec = new double[EMBEDDING_DIM];",
    "            for (int i = 0; i < EMBEDDING_DIM; i++) vec[i] = random.nextGaussian();",
    "            return normalize(vec);",
    "        });",
    "    }",
    "    ",
    "    private double[] normalize(double[] vec) {",
    "        double norm = 0;",
    "        for (double v : vec) norm += v * v;",
    "        norm = Math.sqrt(norm);",
    "        if (norm == 0) return vec;",
    "        double[] result = new double[vec.length];",
    "        for (int i = 0; i < vec.length; i++) result[i] = vec[i] / norm;",
    "        return result;",
    "    }",
    "    ",
    "    private double cosineSimilarity(double[] a, double[] b) {",
    "        return dotProduct(a, b);  // Vectors are already normalized",
    "    }",
    "    ",
    "    private double dotProduct(double[] a, double[] b) {",
    "        double sum = 0;",
    "        for (int i = 0; i < a.length; i++) sum += a[i] * b[i];",
    "        return sum;",
    "    }",
    "    ",
    "    private List<EngagementEvent> getRecentEngagements(String userId, int hours) {",
    "        List<EngagementEvent> events = engagementHistory.getOrDefault(userId, Collections.emptyList());",
    "        Instant cutoff = Instant.now().minus(Duration.ofHours(hours));",
    "        return events.stream().filter(e -> e.timestamp.isAfter(cutoff)).collect(Collectors.toList());",
    "    }",
    "    ",
    "    private List<Article> getColdStartRecommendations(int count) {",
    "        return articles.values().stream()",
    "            .sorted((a, b) -> {",
    "                int cmp = Double.compare(b.qualityScore, a.qualityScore);",
    "                return cmp != 0 ? cmp : b.publishedAt.compareTo(a.publishedAt);",
    "            })",
    "            .limit(count)",
    "            .collect(Collectors.toList());",
    "    }",
    "    ",
    "    // \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550",
    "    // DEMONSTRATION",
    "    // \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550",
    "    ",
    "    public static void main(String[] args) {",
    "        System.out.println(\"=\".repeat(70));",
    "        System.out.println(\"NEWS AGGREGATOR - PART 3: ML-POWERED RECOMMENDATIONS\");",
    "        System.out.println(\"=\".repeat(70));",
    "        ",
    "        NewsAggregatorML aggregator = new NewsAggregatorML();",
    "        ",
    "        // Add articles",
    "        System.out.println(\"\\n\ud83d\udcf0 Adding articles...\");",
    "        String[][] articlesData = {",
    "            {\"tech_1\", \"AI Breakthrough in NLP\", \"techcrunch\", \"tech\"},",
    "            {\"tech_2\", \"Python 4.0 Released\", \"techcrunch\", \"tech\"},",
    "            {\"tech_3\", \"Cloud Computing 2024\", \"wired\", \"tech\"},",
    "            {\"ai_1\", \"GPT-5 Announced\", \"openai\", \"ai\"},",
    "            {\"sports_1\", \"NBA Finals\", \"espn\", \"sports\"},",
    "            {\"science_1\", \"Mars Discovery\", \"nasa\", \"science\"},",
    "        };",
    "        ",
    "        Random rand = new Random();",
    "        for (String[] data : articlesData) {",
    "            Article article = new Article(",
    "                data[0], data[1], data[2], data[3], \"Content...\",",
    "                Instant.now().minus(Duration.ofHours(rand.nextInt(48)))",
    "            );",
    "            article.qualityScore = 0.6 + rand.nextDouble() * 0.35;",
    "            aggregator.addArticle(article);",
    "            System.out.println(\"   Added: [\" + data[3] + \"] \" + data[1]);",
    "        }",
    "        ",
    "        // Register user",
    "        System.out.println(\"\\n\ud83d\udc64 Registering user_1...\");",
    "        aggregator.registerUser(\"user_1\", ",
    "            Arrays.asList(\"techcrunch\"), Arrays.asList(\"tech\"));",
    "        ",
    "        // Record engagements",
    "        System.out.println(\"\\n\ud83d\udcca Recording engagements...\");",
    "        aggregator.recordEngagement(\"user_1\", \"tech_1\", EngagementType.READ, 180);",
    "        aggregator.recordEngagement(\"user_1\", \"tech_2\", EngagementType.READ, 240);",
    "        aggregator.recordEngagement(\"user_1\", \"ai_1\", EngagementType.SHARE, 300);",
    "        aggregator.recordEngagement(\"user_1\", \"sports_1\", EngagementType.CLICK, 10);",
    "        ",
    "        // Get recommendations",
    "        System.out.println(\"\\n\ud83c\udfaf ML-Powered Recommendations for user_1:\");",
    "        System.out.println(\"\u2500\".repeat(50));",
    "        ",
    "        List<Article> recommendations = aggregator.getRecommendations(\"user_1\", 5);",
    "        int i = 1;",
    "        for (Article article : recommendations) {",
    "            System.out.println(\"   \" + i + \". [\" + article.category + \"] \" + article.title);",
    "            i++;",
    "        }",
    "        ",
    "        System.out.println(\"\\n\" + \"=\".repeat(70));",
    "        System.out.println(\"DEMO COMPLETE\");",
    "    }",
    "}"
  ],
  "code_walkthrough": [
    {
      "lines": "1-35",
      "explanation": "**Imports and Constants**: Define EngagementType enum with weights, embedding dimension (128), and exploration rate (20%)."
    },
    {
      "lines": "36-80",
      "explanation": "**Data Classes**: Article stores content and computed embedding. EngagementEvent calculates engagement weight based on type and duration (negative for quick bounces). UserProfile tracks preferences and category affinity."
    },
    {
      "lines": "81-140",
      "explanation": "**VectorDatabase Class**: Simulates Faiss/Milvus functionality. Stores vectors with metadata, supports filtered ANN search using cosine similarity. In production, this would use HNSW index for O(log N) lookup."
    },
    {
      "lines": "141-220",
      "explanation": "**EmbeddingService Class**: Generates embeddings for articles (combining category + publisher + content signals) and users (combining affinity scores + recent engagement history). Uses deterministic random for reproducibility."
    },
    {
      "lines": "221-270",
      "explanation": "**MultiArmedBandit Class**: Implements Thompson Sampling for exploration/exploitation. Tracks Beta distribution parameters per (user, category) pair. Prioritizes unexplored categories while learning from engagement feedback."
    },
    {
      "lines": "271-330",
      "explanation": "**RankingModel Class**: Scores (user, article) pairs using embedding similarity + freshness boost + quality score + publisher affinity + exploration bonus. Weights are learnable in production."
    },
    {
      "lines": "331-380",
      "explanation": "**ReRanker Class**: Enforces diversity rules - max 2 per category, max 2 per publisher, no adjacent same-publisher articles. Ensures feed diversity even when ML model is confident about preferences."
    },
    {
      "lines": "381-500",
      "explanation": "**NewsAggregatorML Core**: Main class integrating all components. add_article() generates embedding and indexes. record_engagement() updates user profile and bandit state. _get_user_embedding() computes or retrieves cached embedding."
    },
    {
      "lines": "501-600",
      "explanation": "**getRecommendations()**: The main recommendation pipeline. Steps: (1) Get user embedding, (2) ANN candidate retrieval, (3) Select exploration categories, (4) Score candidates, (5) Re-rank for diversity. Total latency target: <50ms."
    },
    {
      "lines": "601-700",
      "explanation": "**Demo and Main**: Demonstrates full pipeline with sample articles, user registration, engagement recording, and recommendation generation. Shows how ML learns from behavior (long reads = positive, quick bounce = negative)."
    }
  ],
  "complexity_analysis": {
    "time": {
      "new_methods": {
        "recordEngagement": {
          "complexity": "O(1) amortized",
          "explanation": "Appends to list, updates category affinity with simple arithmetic. Async processing deferred to batch jobs."
        },
        "getRecommendations": {
          "complexity": "O(log N + K log K)",
          "explanation": "O(log N) for ANN search with proper indexing (HNSW). O(K) for scoring K candidates. O(K log K) for sorting. K << N (typically K=100, N=5M)."
        },
        "generateArticleEmbedding": {
          "complexity": "O(D)",
          "explanation": "Linear in embedding dimension D=128. Vector operations are O(D)."
        },
        "getUserEmbedding": {
          "complexity": "O(E \u00d7 D)",
          "explanation": "E = number of recent engagements (bounded by time window). Each contributes to weighted average."
        },
        "rerank": {
          "complexity": "O(K)",
          "explanation": "Single pass through K scored candidates with O(1) checks per article."
        }
      },
      "overall_change": "Adds O(log N) recommendation serving on top of Part 2's O(1) notifications. The ANN search is the bottleneck but is optimized with HNSW index."
    },
    "space": {
      "additional_space": "O(N \u00d7 D + U \u00d7 D + E)",
      "explanation": "N\u00d7D for article embeddings in vector database. U\u00d7D for user embeddings in cache. E for engagement event log. D=128 is constant. With 5M articles and 100M users: ~64GB for article index, ~1.2TB for user embeddings (typically hot users only cached)."
    }
  },
  "dry_run": {
    "example_input": "User engages: READ tech_1 180s, READ tech_2 240s, SHARE ai_1 300s, CLICK sports_1 10s. Then getRecommendations(user_1, 5)",
    "steps": [
      {
        "step": 1,
        "action": "Record READ tech_1 180s",
        "state": "category_affinity['tech'] = 0.0 + 0.3 \u00d7 (9.0 - 0.0) = 2.7",
        "explanation": "Weight = 3.0 \u00d7 (180/60) = 9.0. Update affinity with \u03b1=0.3 learning rate."
      },
      {
        "step": 2,
        "action": "Record READ tech_2 240s",
        "state": "category_affinity['tech'] = 2.7 + 0.3 \u00d7 (12.0 - 2.7) = 5.49",
        "explanation": "Weight = 3.0 \u00d7 (240/60) = 12.0. Tech affinity grows stronger."
      },
      {
        "step": 3,
        "action": "Record SHARE ai_1 300s",
        "state": "category_affinity['ai'] = 0.0 + 0.3 \u00d7 (5.0 - 0.0) = 1.5",
        "explanation": "SHARE has base weight 5.0. Creates AI category affinity."
      },
      {
        "step": 4,
        "action": "Record CLICK sports_1 10s",
        "state": "category_affinity['sports'] = 0.0 + 0.3 \u00d7 (-0.5 - 0.0) = -0.15 \u2192 0.0",
        "explanation": "Quick bounce (10s < 30s) = negative weight -0.5. Affinity clamped to 0."
      },
      {
        "step": 5,
        "action": "Compute user embedding",
        "state": "user_vec biased toward tech (5.49) and ai (1.5) category vectors",
        "explanation": "Weighted average of category vectors + recent article embeddings."
      },
      {
        "step": 6,
        "action": "ANN search with user_vec",
        "state": "Candidates: [tech_3, tech_4, ai_2, science_1, business_1, ...]",
        "explanation": "Vector DB returns articles most similar to user embedding."
      },
      {
        "step": 7,
        "action": "Thompson Sampling for exploration",
        "state": "Select 'science' category for exploration (low engagement history)",
        "explanation": "20% exploration rate \u2192 1 of 5 articles from unexplored category."
      },
      {
        "step": 8,
        "action": "Score candidates",
        "state": "tech_3: 0.92, ai_2: 0.88, tech_4: 0.85, science_1: 0.75 (incl. exploration bonus)",
        "explanation": "Base similarity + freshness + quality + exploration bonus."
      },
      {
        "step": 9,
        "action": "Re-rank for diversity",
        "state": "Final: [tech_3, ai_2, tech_4, science_1, tech_5]",
        "explanation": "Max 2 per category enforced. Science included for exploration."
      }
    ],
    "final_output": "[tech_article_3, ai_article_2, tech_article_4, science_article_1, tech_article_5] - Mostly tech/AI (user preference) with one science article for exploration"
  },
  "edge_cases": [
    {
      "case": "Cold Start User (no engagement history)",
      "handling": "Return popular recent articles sorted by quality \u00d7 freshness. Use followed categories/publishers as initial embedding if available.",
      "gotcha": "Don't return empty list for new users - always provide some recommendations."
    },
    {
      "case": "Cold Start Article (just published)",
      "handling": "Article gets embedding from content/category/publisher. Initially boosted by freshness score. Served through exploration slots until engagement data accumulates.",
      "gotcha": "New articles need exposure to collect engagement signals - pure exploitation would never show them."
    },
    {
      "case": "User with extreme preference (only reads one category)",
      "handling": "Exploration rate (20%) ensures diversity. Re-ranker caps category at 2 per 5 articles. Bandit will gradually increase exploration if user engages.",
      "gotcha": "Respecting user preference vs preventing filter bubbles is a product decision - make it configurable."
    },
    {
      "case": "User rapidly switches interest",
      "handling": "Recent engagements weighted higher (time decay). User embedding updated on significant change. Learning rate \u03b1=0.3 allows adaptation within ~10 interactions.",
      "gotcha": "Don't let single interaction override established preferences - use exponential moving average."
    },
    {
      "case": "Negative engagement (click + immediate bounce)",
      "handling": "Quick bounces (<30s) recorded as negative weight (-0.5). Reduces affinity for that category. Article may be flagged as clickbait if bounce rate is high across users.",
      "gotcha": "Click alone is not positive signal - must consider dwell time."
    },
    {
      "case": "System under high load (100K QPS)",
      "handling": "User embeddings cached for 1 hour. Article embeddings pre-computed at ingestion. ANN uses approximate search (trades accuracy for speed). Feature store uses Redis for sub-ms lookups.",
      "gotcha": "Batch user embedding updates - don't recompute on every engagement."
    }
  ],
  "test_cases": [
    {
      "name": "Basic preference learning",
      "input": "User reads tech articles for 180s, 240s. Sports article bounced in 10s. Request 5 recommendations.",
      "expected": "Tech articles dominate (3-4 of 5). Sports articles absent or at bottom.",
      "explanation": "Positive engagement signals for tech should outweigh the implicit negative for sports."
    },
    {
      "name": "Exploration injection",
      "input": "User has only engaged with 'tech' category for months. Request 5 recommendations.",
      "expected": "At least 1 article from non-tech category (exploration)",
      "explanation": "20% exploration rate ensures 1 of 5 articles is from unexplored category."
    },
    {
      "name": "Diversity enforcement",
      "input": "Top 10 ML-scored articles are all from same publisher 'TechCrunch'.",
      "expected": "Final feed has max 2 from TechCrunch, no adjacent same-publisher.",
      "explanation": "Re-ranker diversity rules prevent publisher domination."
    },
    {
      "name": "Cold start handling",
      "input": "Brand new user with no engagement history requests recommendations.",
      "expected": "Non-empty list of quality \u00d7 recency sorted articles.",
      "explanation": "Cold start fallback provides reasonable recommendations."
    },
    {
      "name": "Freshness boost",
      "input": "Two articles with same category: A published 1 hour ago, B published 48 hours ago.",
      "expected": "A ranked higher than B (all else equal).",
      "explanation": "Freshness exponential decay: exp(-1/24) \u2248 0.96 vs exp(-48/24) \u2248 0.13."
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Computing embeddings on every request",
      "why_wrong": "User embedding computation involves aggregating all recent engagements and weighted averaging. At 100K QPS, this would be computationally prohibitive and add 50ms+ latency.",
      "correct_approach": "Cache user embeddings with TTL (1 hour). Invalidate on significant engagement change. Pre-compute article embeddings at ingestion time.",
      "code_example_wrong": "// Wrong: compute embedding on every request\npublic List<Article> getRecommendations(String userId, int count) {\n    double[] embedding = computeUserEmbedding(userId); // 50ms!\n    ...\n}",
      "code_example_correct": "// Correct: use cached embedding\npublic List<Article> getRecommendations(String userId, int count) {\n    double[] embedding = getCachedOrComputeEmbedding(userId); // 1ms\n    ...\n}"
    },
    {
      "mistake": "Brute-force similarity search",
      "why_wrong": "Comparing user embedding to all 5M article embeddings is O(N\u00d7D) = 640M operations per request. Totally infeasible at scale.",
      "correct_approach": "Use ANN index (HNSW, IVF) that provides O(log N) approximate search with 95%+ recall. Libraries: Faiss, Annoy, Milvus.",
      "code_example_wrong": "// Wrong: O(N) brute force\nfor (Article article : allArticles) {\n    double sim = cosineSimilarity(userVec, article.embedding);\n    ...\n}",
      "code_example_correct": "// Correct: O(log N) ANN search\nList<Candidate> candidates = faissIndex.search(userVec, k=100);"
    },
    {
      "mistake": "Treating clicks as positive signals",
      "why_wrong": "A click followed by immediate bounce (< 10 seconds) indicates the article was misleading or low quality. Treating all clicks as positive leads to clickbait optimization.",
      "correct_approach": "Combine engagement type with dwell time. Quick bounces should be negative signals. Only sustained engagement (> 30 seconds) is positive.",
      "code_example_wrong": "// Wrong: click = positive\nif (event.type == CLICK) {\n    updateAffinity(category, +1.0);\n}",
      "code_example_correct": "// Correct: consider dwell time\nif (event.type == CLICK && event.durationSeconds < 30) {\n    updateAffinity(category, -0.5);  // Negative!\n}"
    },
    {
      "mistake": "No exploration mechanism",
      "why_wrong": "Pure exploitation of learned preferences creates filter bubbles. Users get stuck in narrow content niches. New articles never get exposure. User interests can't evolve.",
      "correct_approach": "Reserve 10-20% of recommendations for exploration. Use multi-armed bandit (Thompson Sampling) to intelligently explore. Track exploration performance.",
      "code_example_wrong": "// Wrong: pure exploitation\nreturn topKByScore(scoredArticles, count);",
      "code_example_correct": "// Correct: mix exploitation + exploration\nList<Article> exploitArticles = topKByScore(scored, count * 0.8);\nList<Article> exploreArticles = bandit.selectExploration(count * 0.2);\nreturn interleave(exploitArticles, exploreArticles);"
    }
  ],
  "interview_tips": {
    "how_to_present": "Start by explaining WHY ML is needed (scale, personalization beyond explicit follows). Draw the two-tower architecture diagram. Emphasize the key insight: decomposition enables pre-computation. Walk through the serving path latency budget (50ms total). Then dive into components: embedding service, vector DB, ranking model, re-ranker, bandit.",
    "what_to_mention": [
      "**Scale numbers**: 100K QPS, 5M articles, <50ms latency target",
      "**Two-tower architecture**: separate user/article towers enable independent pre-computation",
      "**ANN search**: transforms O(N) to O(log N) with HNSW/IVF indexing",
      "**Feature store**: centralized feature computation for ML serving",
      "**Online vs offline**: training is batch (daily), inference is real-time",
      "**Cold start strategies**: content-based for new articles, popularity for new users",
      "**Exploration/exploitation**: Thompson Sampling for intelligent exploration",
      "**Diversity**: re-ranking prevents filter bubbles and monotonous feeds",
      "**Feedback loop**: engagement signals \u2192 model training \u2192 better recommendations"
    ],
    "time_allocation": "Spend 5 minutes on architecture overview and two-tower explanation. 5 minutes on the serving path (ANN + ranking + re-ranking). 5 minutes on engagement tracking and feature engineering. 5 minutes on exploration/exploitation and cold start. Reserve time for questions about production concerns (latency, caching, A/B testing).",
    "if_stuck": [
      "Think about what can be pre-computed vs computed at serve time",
      "Embedding similarity is a proxy for neural network scoring",
      "Consider: how would you handle a brand new user with zero history?",
      "Diversity is a post-processing step, not part of the ML model",
      "Multi-armed bandit is the classic solution for explore/exploit"
    ]
  },
  "connection_to_next_part": "Part 3's ML pipeline generates rich user behavior data and learns user preferences at scale. Part 4 could focus on **A/B Testing & Experimentation Framework** to evaluate model changes, **Content Moderation & Trust/Safety** to filter harmful content before recommendations, or **Global Distribution & CDN Strategy** to serve personalized feeds with low latency worldwide. The embeddings computed here could also power **Search & Discovery** features or **Social Features** (find users with similar interests).",
  "generated_at": "2026-01-14T15:31:47.763542",
  "_meta": {
    "problem_id": "news_feed_aggregator",
    "part_number": 3,
    "model": "claude-opus-4-5-20251101"
  }
}