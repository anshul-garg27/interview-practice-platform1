{
  "problem_title": "Design Instagram - Photo Sharing Platform - Part 3: Search and Explore",
  "part_number": 3,
  "builds_on": "Part 2",
  "difficulty": "hard",
  "problem_understanding": {
    "what_changes": "Part 3 adds content discovery capabilities on top of the existing photo sharing and stories functionality. We now need to support full-text search across users, hashtags, and locations, plus generate personalized Explore feeds using ML-based recommendations. This requires integrating search infrastructure (Elasticsearch), geo-spatial databases (PostGIS), and recommendation systems.",
    "new_requirements": [
      "Full-text search for users by username/name with relevance ranking",
      "Hashtag search with paginated results",
      "Location-based search using geo-spatial queries",
      "Personalized Explore feed with ML-based recommendations",
      "Real-time trending hashtags computation",
      "Real-time indexing of new content for search"
    ],
    "new_constraints": [
      "Search latency must be < 100ms p99",
      "Explore feed must balance freshness, engagement, and personalization",
      "Must handle billions of posts for search indexing",
      "Trending computation needs sliding window (hourly/daily)",
      "Geo-queries need sub-kilometer precision"
    ],
    "key_insight": "The crucial insight is separating RETRIEVAL (fast candidate generation using inverted indices) from RANKING (ML models for personalization). Search uses inverted indices for O(log n) lookups, while Explore uses a two-stage approach: first retrieve candidates from multiple sources, then apply ML ranking. Trending uses probabilistic data structures (Count-Min Sketch) for memory-efficient counting."
  },
  "visual_explanation": {
    "before_after": "```\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                    BEFORE (Part 2): Stories Only                   \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551                                                                    \u2551\n\u2551   User \u2500\u2500\u25ba Upload Photo \u2500\u2500\u25ba CDN Storage                           \u2551\n\u2551        \u2500\u2500\u25ba View Feed    \u2500\u2500\u25ba Feed Service                          \u2551\n\u2551        \u2500\u2500\u25ba View Stories \u2500\u2500\u25ba Stories Service                       \u2551\n\u2551                                                                    \u2551\n\u2551   No discovery mechanism beyond following users                    \u2551\n\u2551                                                                    \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551                 AFTER (Part 3): Search + Explore                   \u2551\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\n\u2551                                                                    \u2551\n\u2551                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2551\n\u2551                    \u2502    SEARCH SERVICE       \u2502                     \u2551\n\u2551                    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502                     \u2551\n\u2551                    \u2502  \u2502  User Index (ES)    \u2502\u2502                     \u2551\n\u2551                    \u2502  \u2502  Hashtag Index (ES) \u2502\u2502                     \u2551\n\u2551                    \u2502  \u2502  Location Index     \u2502\u2502                     \u2551\n\u2551                    \u2502  \u2502  (PostGIS)          \u2502\u2502                     \u2551\n\u2551                    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502                     \u2551\n\u2551                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2551\n\u2551                              \u2502                                     \u2551\n\u2551   User \u2500\u2500\u25ba Search Query \u2500\u2500\u2500\u2500\u2500\u2518                                     \u2551\n\u2551        \u2502                                                           \u2551\n\u2551        \u2502                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2551\n\u2551        \u2514\u25ba Explore \u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502   EXPLORE SERVICE      \u2502               \u2551\n\u2551                          \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502               \u2551\n\u2551                          \u2502  \u2502 Candidate Gen     \u2502 \u2502               \u2551\n\u2551                          \u2502  \u2502 \u2022 Similar Users   \u2502 \u2502               \u2551\n\u2551                          \u2502  \u2502 \u2022 Trending Posts  \u2502 \u2502               \u2551\n\u2551                          \u2502  \u2502 \u2022 Topic Clusters  \u2502 \u2502               \u2551\n\u2551                          \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502               \u2551\n\u2551                          \u2502            \u25bc           \u2502               \u2551\n\u2551                          \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502               \u2551\n\u2551                          \u2502  \u2502 ML Ranking        \u2502 \u2502               \u2551\n\u2551                          \u2502  \u2502 \u2022 Engagement Pred \u2502 \u2502               \u2551\n\u2551                          \u2502  \u2502 \u2022 User Affinity   \u2502 \u2502               \u2551\n\u2551                          \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502               \u2551\n\u2551                          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2551\n\u2551                                                                    \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n```",
    "algorithm_flow": "```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              SEARCH & EXPLORE ALGORITHM FLOW                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n                         USER SEARCH FLOW\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n  Step 1: Query Preprocessing\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502  Input: \"john smith\"                   \u2502\n  \u2502                                        \u2502\n  \u2502  \u2022 Normalize: lowercase, trim          \u2502\n  \u2502  \u2022 Tokenize: [\"john\", \"smith\"]         \u2502\n  \u2502  \u2022 Generate prefixes for autocomplete  \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n                    \u25bc\n  Step 2: Elasticsearch Query\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502  Multi-match query:                    \u2502\n  \u2502  \u2022 username (boost: 3.0)               \u2502\n  \u2502  \u2022 display_name (boost: 2.0)           \u2502\n  \u2502  \u2022 bio (boost: 1.0)                    \u2502\n  \u2502                                        \u2502\n  \u2502  Scoring factors:                      \u2502\n  \u2502  \u2022 Text relevance (BM25)               \u2502\n  \u2502  \u2022 Follower count (popularity)         \u2502\n  \u2502  \u2022 Verification status                 \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n                    \u25bc\n  Step 3: Social Graph Boost\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502  Boost scores for:                     \u2502\n  \u2502  \u2022 Users you follow: +2.0              \u2502\n  \u2502  \u2022 Users followed by friends: +1.5    \u2502\n  \u2502  \u2022 Same location/interests: +1.2      \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n                       EXPLORE FEED FLOW\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n  Stage 1: Candidate Generation (Retrieve ~10K candidates)\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502                                                                 \u2502\n  \u2502   Source 1: Interest-Based                                      \u2502\n  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                   \u2502\n  \u2502   \u2502 User's topic clusters   \u2502\u2500\u2500\u25ba Posts in those topics          \u2502\n  \u2502   \u2502 (ML embedding)          \u2502    (~3000 candidates)             \u2502\n  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                   \u2502\n  \u2502                                                                 \u2502\n  \u2502   Source 2: Social Signal                                       \u2502\n  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                   \u2502\n  \u2502   \u2502 Friends' engagements    \u2502\u2500\u2500\u25ba Posts friends liked            \u2502\n  \u2502   \u2502 (collaborative filter)  \u2502    (~3000 candidates)             \u2502\n  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                   \u2502\n  \u2502                                                                 \u2502\n  \u2502   Source 3: Trending                                            \u2502\n  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                   \u2502\n  \u2502   \u2502 High engagement posts   \u2502\u2500\u2500\u25ba Viral content                  \u2502\n  \u2502   \u2502 (real-time counters)    \u2502    (~2000 candidates)             \u2502\n  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                   \u2502\n  \u2502                                                                 \u2502\n  \u2502   Source 4: Geographic                                          \u2502\n  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                   \u2502\n  \u2502   \u2502 User's location         \u2502\u2500\u2500\u25ba Local popular posts            \u2502\n  \u2502   \u2502 (geo-index)             \u2502    (~2000 candidates)             \u2502\n  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                   \u2502\n  \u2502                                                                 \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n  Stage 2: Filtering (Reduce to ~1K candidates)\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502  Remove:                                                        \u2502\n  \u2502  \u2022 Already seen posts                                           \u2502\n  \u2502  \u2022 Blocked users' content                                       \u2502\n  \u2502  \u2022 Low-quality content (spam score > threshold)                 \u2502\n  \u2502  \u2022 Policy-violating content                                     \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502\n                              \u25bc\n  Stage 3: ML Ranking (Select top ~50)\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502                                                                 \u2502\n  \u2502  Feature Vector per (user, post):                               \u2502\n  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n  \u2502  \u2502 User Features:                                            \u2502  \u2502\n  \u2502  \u2502 \u2022 Interest embedding (128-dim)                            \u2502  \u2502\n  \u2502  \u2502 \u2022 Engagement history vector                               \u2502  \u2502\n  \u2502  \u2502 \u2022 Session context (time, device)                          \u2502  \u2502\n  \u2502  \u2502                                                           \u2502  \u2502\n  \u2502  \u2502 Post Features:                                            \u2502  \u2502\n  \u2502  \u2502 \u2022 Content embedding (256-dim)                             \u2502  \u2502\n  \u2502  \u2502 \u2022 Engagement stats (likes/comments/shares)                \u2502  \u2502\n  \u2502  \u2502 \u2022 Author features                                         \u2502  \u2502\n  \u2502  \u2502 \u2022 Age decay factor                                        \u2502  \u2502\n  \u2502  \u2502                                                           \u2502  \u2502\n  \u2502  \u2502 Cross Features:                                           \u2502  \u2502\n  \u2502  \u2502 \u2022 User-author affinity score                              \u2502  \u2502\n  \u2502  \u2502 \u2022 Topic match score                                       \u2502  \u2502\n  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n  \u2502                                                                 \u2502\n  \u2502  Model: Deep Neural Network                                     \u2502\n  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n  \u2502  \u2502 Input \u2500\u2500\u25ba Dense(512) \u2500\u2500\u25ba Dense(256) \u2500\u2500\u25ba Dense(128) \u2500\u2500\u25ba   \u2502  \u2502\n  \u2502  \u2502          \u2500\u2500\u25ba Multi-task heads:                            \u2502  \u2502\n  \u2502  \u2502              \u2022 P(like)                                    \u2502  \u2502\n  \u2502  \u2502              \u2022 P(comment)                                 \u2502  \u2502\n  \u2502  \u2502              \u2022 P(save)                                    \u2502  \u2502\n  \u2502  \u2502              \u2022 E[watch_time]                              \u2502  \u2502\n  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n  \u2502                                                                 \u2502\n  \u2502  Final Score = w1*P(like) + w2*P(comment) + w3*P(save) + ...   \u2502\n  \u2502                                                                 \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n                    TRENDING HASHTAGS ALGORITHM\n\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n  Using Count-Min Sketch + Sliding Window\n  \n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502                                                                 \u2502\n  \u2502   Time Window (last 1 hour, 15-min buckets):                   \u2502\n  \u2502                                                                 \u2502\n  \u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                \u2502\n  \u2502   \u2502 0-15 \u250215-30 \u250230-45 \u250245-60 \u2502  (minutes ago)                 \u2502\n  \u2502   \u2502 min  \u2502 min  \u2502 min  \u2502 min  \u2502                                \u2502\n  \u2502   \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524                                \u2502\n  \u2502   \u2502 CMS  \u2502 CMS  \u2502 CMS  \u2502 CMS  \u2502  Count-Min Sketch per bucket   \u2502\n  \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                \u2502\n  \u2502                                                                 \u2502\n  \u2502   To get count for #travel:                                     \u2502\n  \u2502   count = sum(CMS[bucket].estimate(\"travel\") for each bucket)  \u2502\n  \u2502                                                                 \u2502\n  \u2502   Trending Score = current_count / baseline_count              \u2502\n  \u2502   (baseline from same time window previous week)               \u2502\n  \u2502                                                                 \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```"
  },
  "approaches": [
    {
      "name": "Naive Extension",
      "description": "Simply scan the database for search queries using LIKE patterns, compute trending by counting all hashtags in recent posts, and generate explore feed by random sampling popular posts.",
      "time_complexity": "O(n) for search where n is total users/posts, O(n) for trending computation",
      "space_complexity": "O(1) additional space",
      "why_not_optimal": "Database LIKE queries are extremely slow at scale (full table scans). Computing trending by scanning all posts is prohibitively expensive with billions of posts. Random sampling doesn't provide personalization. This approach would result in 10+ second latencies and poor user experience."
    },
    {
      "name": "Optimal Approach",
      "description": "Use specialized infrastructure: Elasticsearch for full-text search with inverted indices (O(log n) lookups), PostGIS for geo-spatial queries with R-tree indices, Count-Min Sketch for memory-efficient trending computation, and a two-stage ML pipeline for personalized Explore feeds (candidate generation + ranking).",
      "time_complexity": "O(log n) for search, O(1) for trending lookups, O(k log k) for explore where k is candidate pool size",
      "space_complexity": "O(n) for search indices, O(1) per hashtag for trending (probabilistic), O(u*f) for user features where u=users, f=feature dimensions",
      "key_insight": "Separate RETRIEVAL from RANKING. Use inverted indices for fast retrieval, probabilistic data structures for real-time counting, and ML models only for final ranking of pre-filtered candidates. This achieves both speed and personalization."
    }
  ],
  "optimal_solution": {
    "explanation_md": "## Optimal Solution: Search & Explore Architecture\n\n### Key Design Decisions\n\n**1. Search Infrastructure**\n\nWe use **Elasticsearch** for user and hashtag search because:\n- Inverted indices provide O(log n) lookups\n- BM25 algorithm handles relevance scoring\n- Supports prefix matching for autocomplete\n- Horizontally scalable via sharding\n\nFor **location search**, we use **PostGIS** (or Elasticsearch geo_point) because:\n- R-tree indices for efficient range queries\n- Supports complex geo operations (radius, polygon)\n- Optimized for lat/lng queries\n\n**2. Real-time Indexing Pipeline**\n\nWhen a new post is created:\n```\nPost Created \u2192 Kafka \u2192 Index Workers \u2192 Elasticsearch\n                    \u2193\n              Hashtag Extractor \u2192 Trending Counter (Redis)\n                    \u2193\n              Geo Encoder \u2192 Location Index\n```\n\n**3. Trending Computation**\n\nWe use **Count-Min Sketch** because:\n- O(1) increment and query operations\n- Fixed memory regardless of unique hashtag count\n- Probabilistic with configurable error bounds\n- Perfect for \"heavy hitter\" detection\n\nCombined with **sliding window buckets**:\n- Divide time into buckets (e.g., 15-minute intervals)\n- Each bucket has its own CMS\n- Sum across recent buckets for trending score\n- Expire old buckets periodically\n\n**4. Explore Feed Generation**\n\nTwo-stage approach:\n\n**Stage 1: Candidate Generation** (fast, recall-focused)\n- Interest-based: Posts matching user's topic clusters\n- Collaborative: Posts liked by similar users\n- Trending: Viral content across platform\n- Geographic: Popular local content\n\n**Stage 2: ML Ranking** (accurate, precision-focused)\n- Deep neural network predicting engagement\n- Multi-task learning: like, comment, save, watch_time\n- Real-time feature computation\n- Personalized per user\n\n### Why This Works\n\n1. **Latency**: Search in <100ms via inverted indices\n2. **Scalability**: Each component scales independently\n3. **Freshness**: Real-time indexing pipeline\n4. **Personalization**: ML ranking for explore\n5. **Memory Efficiency**: Probabilistic structures for trending",
    "data_structures": [
      {
        "structure": "Elasticsearch Index",
        "purpose": "Full-text search with inverted indices for users and hashtags"
      },
      {
        "structure": "PostGIS/Geo Index",
        "purpose": "R-tree based geo-spatial indexing for location queries"
      },
      {
        "structure": "Count-Min Sketch",
        "purpose": "Probabilistic counting for trending hashtags with O(1) operations"
      },
      {
        "structure": "Redis Sorted Sets",
        "purpose": "Maintain top-K trending hashtags per region"
      },
      {
        "structure": "Feature Store",
        "purpose": "Store precomputed ML features for users and posts"
      },
      {
        "structure": "Inverted Index (Hashtag\u2192Posts)",
        "purpose": "Fast lookup of posts by hashtag"
      },
      {
        "structure": "User Embedding Cache",
        "purpose": "Cached interest vectors for candidate generation"
      }
    ],
    "algorithm_steps": [
      "Step 1: Index new content in real-time via Kafka consumers",
      "Step 2: For user search, query Elasticsearch with multi-match and apply social graph boosting",
      "Step 3: For hashtag search, use inverted index lookup with pagination via cursor",
      "Step 4: For location search, use geo_distance query on PostGIS/Elasticsearch",
      "Step 5: For trending, increment Count-Min Sketch on hashtag use, maintain top-K in sorted set",
      "Step 6: For explore, generate candidates from 4 sources (interest, social, trending, geo)",
      "Step 7: Filter candidates (already seen, blocked, low quality)",
      "Step 8: Apply ML ranking model to score remaining candidates",
      "Step 9: Return top-N posts with diversity sampling"
    ]
  },
  "solution_python_lines": [
    "\"\"\"",
    "Instagram Search & Explore System - Part 3",
    "Production-quality implementation with search, trending, and ML-based explore.",
    "\"\"\"",
    "",
    "from typing import List, Dict, Set, Optional, Tuple, Any",
    "from dataclasses import dataclass, field",
    "from datetime import datetime, timedelta",
    "from collections import defaultdict",
    "from abc import ABC, abstractmethod",
    "import heapq",
    "import hashlib",
    "import math",
    "import random",
    "import time",
    "import uuid",
    "from enum import Enum",
    "",
    "",
    "# ============================================================================",
    "# DATA MODELS",
    "# ============================================================================",
    "",
    "@dataclass",
    "class User:",
    "    \"\"\"User model with search-relevant fields.\"\"\"",
    "    user_id: str",
    "    username: str",
    "    display_name: str",
    "    bio: str = \"\"",
    "    follower_count: int = 0",
    "    following_count: int = 0",
    "    is_verified: bool = False",
    "    profile_pic_url: str = \"\"",
    "    interests: List[str] = field(default_factory=list)",
    "    location: Optional[Tuple[float, float]] = None  # (lat, lng)",
    "",
    "",
    "@dataclass",
    "class Post:",
    "    \"\"\"Post model with indexable fields.\"\"\"",
    "    post_id: str",
    "    user_id: str",
    "    caption: str",
    "    image_url: str",
    "    hashtags: List[str] = field(default_factory=list)",
    "    location: Optional[Tuple[float, float]] = None",
    "    location_name: str = \"\"",
    "    created_at: datetime = field(default_factory=datetime.utcnow)",
    "    like_count: int = 0",
    "    comment_count: int = 0",
    "    share_count: int = 0",
    "    quality_score: float = 0.5",
    "",
    "",
    "@dataclass",
    "class SearchResult:",
    "    \"\"\"Generic search result with relevance score.\"\"\"",
    "    item: Any",
    "    relevance_score: float",
    "    match_type: str = \"exact\"",
    "",
    "",
    "@dataclass",
    "class ExploreFeed:",
    "    \"\"\"Explore feed response.\"\"\"",
    "    posts: List[Post]",
    "    cursor: Optional[str] = None",
    "    has_more: bool = True",
    "",
    "",
    "@dataclass",
    "class TrendingHashtag:",
    "    \"\"\"Trending hashtag with metadata.\"\"\"",
    "    hashtag: str",
    "    post_count: int",
    "    trend_score: float  # velocity vs baseline",
    "    region: str = \"global\"",
    "",
    "",
    "# ============================================================================",
    "# COUNT-MIN SKETCH FOR TRENDING",
    "# ============================================================================",
    "",
    "class CountMinSketch:",
    "    \"\"\"",
    "    Probabilistic data structure for frequency estimation.",
    "    Space-efficient counting with configurable error bounds.",
    "    \"\"\"",
    "    ",
    "    def __init__(self, width: int = 10000, depth: int = 7):",
    "        \"\"\"",
    "        Initialize Count-Min Sketch.",
    "        ",
    "        Args:",
    "            width: Number of counters per row (affects accuracy)",
    "            depth: Number of hash functions (affects confidence)",
    "        \"\"\"",
    "        self.width = width",
    "        self.depth = depth",
    "        self.table = [[0] * width for _ in range(depth)]",
    "        self.total_count = 0",
    "    ",
    "    def _hash(self, item: str, seed: int) -> int:",
    "        \"\"\"Generate hash for item with given seed.\"\"\"",
    "        hash_input = f\"{seed}:{item}\".encode('utf-8')",
    "        hash_value = int(hashlib.md5(hash_input).hexdigest(), 16)",
    "        return hash_value % self.width",
    "    ",
    "    def increment(self, item: str, count: int = 1) -> None:",
    "        \"\"\"Increment count for item. O(depth) = O(1).\"\"\"",
    "        for i in range(self.depth):",
    "            idx = self._hash(item, i)",
    "            self.table[i][idx] += count",
    "        self.total_count += count",
    "    ",
    "    def estimate(self, item: str) -> int:",
    "        \"\"\"",
    "        Estimate count for item. O(depth) = O(1).",
    "        Returns minimum across all hash positions (least overestimation).",
    "        \"\"\"",
    "        min_count = float('inf')",
    "        for i in range(self.depth):",
    "            idx = self._hash(item, i)",
    "            min_count = min(min_count, self.table[i][idx])",
    "        return min_count",
    "",
    "",
    "# ============================================================================",
    "# SLIDING WINDOW COUNTER FOR TRENDING",
    "# ============================================================================",
    "",
    "class SlidingWindowCounter:",
    "    \"\"\"",
    "    Time-bucketed counter using Count-Min Sketch.",
    "    Efficiently tracks trending items over sliding time windows.",
    "    \"\"\"",
    "    ",
    "    def __init__(self, window_minutes: int = 60, bucket_minutes: int = 15):",
    "        \"\"\"",
    "        Initialize sliding window counter.",
    "        ",
    "        Args:",
    "            window_minutes: Total window size",
    "            bucket_minutes: Size of each bucket",
    "        \"\"\"",
    "        self.window_minutes = window_minutes",
    "        self.bucket_minutes = bucket_minutes",
    "        self.num_buckets = window_minutes // bucket_minutes",
    "        self.buckets: Dict[int, CountMinSketch] = {}",
    "        self.bucket_timestamps: Dict[int, datetime] = {}",
    "    ",
    "    def _get_bucket_key(self, timestamp: datetime) -> int:",
    "        \"\"\"Get bucket key for given timestamp.\"\"\"",
    "        minutes_since_epoch = int(timestamp.timestamp() / 60)",
    "        return minutes_since_epoch // self.bucket_minutes",
    "    ",
    "    def _get_or_create_bucket(self, bucket_key: int) -> CountMinSketch:",
    "        \"\"\"Get existing bucket or create new one.\"\"\"",
    "        if bucket_key not in self.buckets:",
    "            self.buckets[bucket_key] = CountMinSketch()",
    "            self.bucket_timestamps[bucket_key] = datetime.utcnow()",
    "        return self.buckets[bucket_key]",
    "    ",
    "    def _cleanup_old_buckets(self) -> None:",
    "        \"\"\"Remove buckets outside the window.\"\"\"",
    "        current_key = self._get_bucket_key(datetime.utcnow())",
    "        min_key = current_key - self.num_buckets",
    "        old_keys = [k for k in self.buckets if k < min_key]",
    "        for k in old_keys:",
    "            del self.buckets[k]",
    "            if k in self.bucket_timestamps:",
    "                del self.bucket_timestamps[k]",
    "    ",
    "    def increment(self, item: str, count: int = 1) -> None:",
    "        \"\"\"Increment count for item in current bucket.\"\"\"",
    "        bucket_key = self._get_bucket_key(datetime.utcnow())",
    "        bucket = self._get_or_create_bucket(bucket_key)",
    "        bucket.increment(item, count)",
    "        ",
    "        # Periodic cleanup",
    "        if random.random() < 0.01:",
    "            self._cleanup_old_buckets()",
    "    ",
    "    def get_count(self, item: str) -> int:",
    "        \"\"\"Get total count across all active buckets.\"\"\"",
    "        self._cleanup_old_buckets()",
    "        current_key = self._get_bucket_key(datetime.utcnow())",
    "        min_key = current_key - self.num_buckets",
    "        ",
    "        total = 0",
    "        for key in range(min_key, current_key + 1):",
    "            if key in self.buckets:",
    "                total += self.buckets[key].estimate(item)",
    "        return total",
    "",
    "",
    "# ============================================================================",
    "# INVERTED INDEX FOR SEARCH",
    "# ============================================================================",
    "",
    "class InvertedIndex:",
    "    \"\"\"",
    "    Simple inverted index for text search.",
    "    Production would use Elasticsearch.",
    "    \"\"\"",
    "    ",
    "    def __init__(self):",
    "        self.index: Dict[str, Set[str]] = defaultdict(set)  # term -> doc_ids",
    "        self.documents: Dict[str, Dict] = {}  # doc_id -> document",
    "        self.doc_term_freq: Dict[str, Dict[str, int]] = {}  # doc_id -> term -> freq",
    "    ",
    "    def _tokenize(self, text: str) -> List[str]:",
    "        \"\"\"Tokenize text into searchable terms.\"\"\"",
    "        # Lowercase, split by non-alphanumeric, filter short tokens",
    "        import re",
    "        tokens = re.findall(r'\\w+', text.lower())",
    "        return [t for t in tokens if len(t) >= 2]",
    "    ",
    "    def add_document(self, doc_id: str, content: Dict[str, str], boost_fields: Dict[str, float] = None) -> None:",
    "        \"\"\"",
    "        Add document to index.",
    "        ",
    "        Args:",
    "            doc_id: Unique document identifier",
    "            content: Dictionary of field_name -> text content",
    "            boost_fields: Optional field boost weights",
    "        \"\"\"",
    "        self.documents[doc_id] = content",
    "        self.doc_term_freq[doc_id] = defaultdict(int)",
    "        ",
    "        for field_name, text in content.items():",
    "            if text:",
    "                for token in self._tokenize(text):",
    "                    self.index[token].add(doc_id)",
    "                    self.doc_term_freq[doc_id][token] += 1",
    "                    ",
    "                    # Add prefix tokens for autocomplete",
    "                    for i in range(2, len(token)):",
    "                        prefix = token[:i]",
    "                        self.index[f\"prefix:{prefix}\"].add(doc_id)",
    "    ",
    "    def search(self, query: str, limit: int = 10, prefix_match: bool = True) -> List[Tuple[str, float]]:",
    "        \"\"\"",
    "        Search for documents matching query.",
    "        Returns list of (doc_id, score) tuples sorted by relevance.",
    "        \"\"\"",
    "        tokens = self._tokenize(query)",
    "        if not tokens:",
    "            return []",
    "        ",
    "        # Collect candidate documents",
    "        candidates: Dict[str, float] = defaultdict(float)",
    "        ",
    "        for token in tokens:",
    "            # Exact match",
    "            if token in self.index:",
    "                for doc_id in self.index[token]:",
    "                    # Simple TF-IDF-like scoring",
    "                    tf = self.doc_term_freq[doc_id].get(token, 0)",
    "                    idf = math.log(1 + len(self.documents) / (1 + len(self.index[token])))",
    "                    candidates[doc_id] += tf * idf",
    "            ",
    "            # Prefix match",
    "            if prefix_match:",
    "                prefix_key = f\"prefix:{token}\"",
    "                if prefix_key in self.index:",
    "                    for doc_id in self.index[prefix_key]:",
    "                        candidates[doc_id] += 0.5  # Lower boost for prefix",
    "        ",
    "        # Sort by score and return top results",
    "        results = sorted(candidates.items(), key=lambda x: -x[1])",
    "        return results[:limit]",
    "    ",
    "    def remove_document(self, doc_id: str) -> None:",
    "        \"\"\"Remove document from index.\"\"\"",
    "        if doc_id not in self.documents:",
    "            return",
    "        ",
    "        content = self.documents[doc_id]",
    "        for text in content.values():",
    "            if text:",
    "                for token in self._tokenize(text):",
    "                    self.index[token].discard(doc_id)",
    "                    for i in range(2, len(token)):",
    "                        self.index[f\"prefix:{token[:i]}\"].discard(doc_id)",
    "        ",
    "        del self.documents[doc_id]",
    "        del self.doc_term_freq[doc_id]",
    "",
    "",
    "# ============================================================================",
    "# GEO-SPATIAL INDEX",
    "# ============================================================================",
    "",
    "class GeoIndex:",
    "    \"\"\"",
    "    Simple geo-spatial index using grid-based bucketing.",
    "    Production would use PostGIS or Elasticsearch geo_point.",
    "    \"\"\"",
    "    ",
    "    def __init__(self, grid_size_degrees: float = 0.1):",
    "        \"\"\"",
    "        Initialize geo index.",
    "        ",
    "        Args:",
    "            grid_size_degrees: Size of each grid cell (~11km at equator)",
    "        \"\"\"",
    "        self.grid_size = grid_size_degrees",
    "        self.grid: Dict[Tuple[int, int], Set[str]] = defaultdict(set)",
    "        self.locations: Dict[str, Tuple[float, float]] = {}",
    "    ",
    "    def _get_grid_cell(self, lat: float, lng: float) -> Tuple[int, int]:",
    "        \"\"\"Get grid cell for coordinates.\"\"\"",
    "        return (int(lat / self.grid_size), int(lng / self.grid_size))",
    "    ",
    "    def _haversine_distance(self, lat1: float, lng1: float, lat2: float, lng2: float) -> float:",
    "        \"\"\"Calculate distance between two points in kilometers.\"\"\"",
    "        R = 6371  # Earth's radius in km",
    "        ",
    "        lat1_rad = math.radians(lat1)",
    "        lat2_rad = math.radians(lat2)",
    "        delta_lat = math.radians(lat2 - lat1)",
    "        delta_lng = math.radians(lng2 - lng1)",
    "        ",
    "        a = math.sin(delta_lat / 2) ** 2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lng / 2) ** 2",
    "        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))",
    "        ",
    "        return R * c",
    "    ",
    "    def add_location(self, item_id: str, lat: float, lng: float) -> None:",
    "        \"\"\"Add item to geo index.\"\"\"",
    "        cell = self._get_grid_cell(lat, lng)",
    "        self.grid[cell].add(item_id)",
    "        self.locations[item_id] = (lat, lng)",
    "    ",
    "    def search_radius(self, lat: float, lng: float, radius_km: float, limit: int = 100) -> List[Tuple[str, float]]:",
    "        \"\"\"",
    "        Find items within radius of point.",
    "        Returns list of (item_id, distance) tuples sorted by distance.",
    "        \"\"\"",
    "        # Calculate grid cells to search",
    "        cell_radius = int(radius_km / (self.grid_size * 111)) + 1  # ~111km per degree",
    "        center_cell = self._get_grid_cell(lat, lng)",
    "        ",
    "        candidates = []",
    "        ",
    "        for di in range(-cell_radius, cell_radius + 1):",
    "            for dj in range(-cell_radius, cell_radius + 1):",
    "                cell = (center_cell[0] + di, center_cell[1] + dj)",
    "                for item_id in self.grid.get(cell, set()):",
    "                    item_lat, item_lng = self.locations[item_id]",
    "                    distance = self._haversine_distance(lat, lng, item_lat, item_lng)",
    "                    if distance <= radius_km:",
    "                        candidates.append((item_id, distance))",
    "        ",
    "        # Sort by distance and limit",
    "        candidates.sort(key=lambda x: x[1])",
    "        return candidates[:limit]",
    "    ",
    "    def remove_location(self, item_id: str) -> None:",
    "        \"\"\"Remove item from geo index.\"\"\"",
    "        if item_id in self.locations:",
    "            lat, lng = self.locations[item_id]",
    "            cell = self._get_grid_cell(lat, lng)",
    "            self.grid[cell].discard(item_id)",
    "            del self.locations[item_id]",
    "",
    "",
    "# ============================================================================",
    "# ML RANKING SIMULATOR",
    "# ============================================================================",
    "",
    "class MLRanker:",
    "    \"\"\"",
    "    Simulated ML ranking model for explore feed.",
    "    Production would use TensorFlow/PyTorch with feature store.",
    "    \"\"\"",
    "    ",
    "    def __init__(self):",
    "        self.user_interests: Dict[str, List[str]] = {}",
    "        self.post_embeddings: Dict[str, List[float]] = {}",
    "    ",
    "    def set_user_interests(self, user_id: str, interests: List[str]) -> None:",
    "        \"\"\"Set user interest profile.\"\"\"",
    "        self.user_interests[user_id] = interests",
    "    ",
    "    def compute_post_embedding(self, post: Post) -> List[float]:",
    "        \"\"\"Compute embedding for post (simplified).\"\"\"",
    "        # In production: use pre-trained vision/text models",
    "        embedding = [0.0] * 10",
    "        ",
    "        # Simple feature engineering",
    "        for i, hashtag in enumerate(post.hashtags[:5]):",
    "            embedding[i] = hash(hashtag) % 100 / 100.0",
    "        ",
    "        embedding[5] = min(post.like_count / 1000, 1.0)",
    "        embedding[6] = min(post.comment_count / 100, 1.0)",
    "        embedding[7] = post.quality_score",
    "        embedding[8] = 1.0 if post.location else 0.0",
    "        embedding[9] = len(post.caption) / 500.0",
    "        ",
    "        return embedding",
    "    ",
    "    def score_post_for_user(self, user_id: str, post: Post) -> float:",
    "        \"\"\"",
    "        Score post relevance for user.",
    "        Returns value between 0 and 1.",
    "        \"\"\"",
    "        base_score = 0.5",
    "        ",
    "        # Interest match",
    "        user_interests = self.user_interests.get(user_id, [])",
    "        if user_interests:",
    "            interest_set = set(user_interests)",
    "            hashtag_set = set(post.hashtags)",
    "            overlap = len(interest_set & hashtag_set)",
    "            base_score += overlap * 0.1",
    "        ",
    "        # Engagement signals",
    "        engagement_score = (",
    "            min(post.like_count / 1000, 1.0) * 0.3 +",
    "            min(post.comment_count / 100, 1.0) * 0.2",
    "        )",
    "        base_score += engagement_score",
    "        ",
    "        # Quality score",
    "        base_score += post.quality_score * 0.2",
    "        ",
    "        # Recency decay",
    "        age_hours = (datetime.utcnow() - post.created_at).total_seconds() / 3600",
    "        recency_factor = math.exp(-age_hours / 24)  # Half-life of ~24 hours",
    "        base_score *= (0.5 + 0.5 * recency_factor)",
    "        ",
    "        return min(base_score, 1.0)",
    "    ",
    "    def rank_posts(self, user_id: str, posts: List[Post]) -> List[Tuple[Post, float]]:",
    "        \"\"\"Rank posts for user by relevance score.\"\"\"",
    "        scored = [(post, self.score_post_for_user(user_id, post)) for post in posts]",
    "        scored.sort(key=lambda x: -x[1])",
    "        return scored",
    "",
    "",
    "# ============================================================================",
    "# MAIN SEARCH & EXPLORE SERVICE",
    "# ============================================================================",
    "",
    "class InstagramSearchExplore:",
    "    \"\"\"",
    "    Complete Search & Explore service for Instagram.",
    "    Integrates user search, hashtag search, location search,",
    "    trending computation, and personalized explore feed.",
    "    \"\"\"",
    "    ",
    "    def __init__(self):",
    "        # User storage and index",
    "        self.users: Dict[str, User] = {}",
    "        self.user_index = InvertedIndex()",
    "        ",
    "        # Post storage and indices",
    "        self.posts: Dict[str, Post] = {}",
    "        self.hashtag_index: Dict[str, Set[str]] = defaultdict(set)  # hashtag -> post_ids",
    "        self.user_posts: Dict[str, List[str]] = defaultdict(list)  # user_id -> post_ids",
    "        self.geo_index = GeoIndex()",
    "        ",
    "        # Social graph",
    "        self.following: Dict[str, Set[str]] = defaultdict(set)  # user_id -> following_ids",
    "        self.followers: Dict[str, Set[str]] = defaultdict(set)  # user_id -> follower_ids",
    "        ",
    "        # Trending computation",
    "        self.trending_counter = SlidingWindowCounter(window_minutes=60, bucket_minutes=15)",
    "        self.regional_trending: Dict[str, List[TrendingHashtag]] = {}  # region -> trending list",
    "        ",
    "        # ML ranking",
    "        self.ml_ranker = MLRanker()",
    "        ",
    "        # Explore feed cache",
    "        self.explore_cache: Dict[str, Tuple[ExploreFeed, datetime]] = {}",
    "        self.explore_cache_ttl = timedelta(minutes=5)",
    "        ",
    "        # User seen posts (for filtering)",
    "        self.user_seen_posts: Dict[str, Set[str]] = defaultdict(set)",
    "    ",
    "    # ========================================================================",
    "    # USER MANAGEMENT",
    "    # ========================================================================",
    "    ",
    "    def create_user(self, user: User) -> None:",
    "        \"\"\"Create user and index for search.\"\"\"",
    "        self.users[user.user_id] = user",
    "        ",
    "        # Index user for search",
    "        self.user_index.add_document(",
    "            user.user_id,",
    "            {",
    "                'username': user.username,",
    "                'display_name': user.display_name,",
    "                'bio': user.bio",
    "            }",
    "        )",
    "        ",
    "        # Set ML interests",
    "        if user.interests:",
    "            self.ml_ranker.set_user_interests(user.user_id, user.interests)",
    "    ",
    "    def follow_user(self, follower_id: str, followee_id: str) -> bool:",
    "        \"\"\"Create follow relationship.\"\"\"",
    "        if follower_id == followee_id:",
    "            return False",
    "        ",
    "        self.following[follower_id].add(followee_id)",
    "        self.followers[followee_id].add(follower_id)",
    "        ",
    "        # Update follower counts",
    "        if followee_id in self.users:",
    "            self.users[followee_id].follower_count = len(self.followers[followee_id])",
    "        if follower_id in self.users:",
    "            self.users[follower_id].following_count = len(self.following[follower_id])",
    "        ",
    "        return True",
    "    ",
    "    # ========================================================================",
    "    # POST MANAGEMENT",
    "    # ========================================================================",
    "    ",
    "    def create_post(self, post: Post) -> None:",
    "        \"\"\"Create post and index for search.\"\"\"",
    "        self.posts[post.post_id] = post",
    "        self.user_posts[post.user_id].append(post.post_id)",
    "        ",
    "        # Index hashtags",
    "        for hashtag in post.hashtags:",
    "            normalized = hashtag.lower().strip('#')",
    "            self.hashtag_index[normalized].add(post.post_id)",
    "            # Update trending counter",
    "            self.trending_counter.increment(normalized)",
    "        ",
    "        # Index location",
    "        if post.location:",
    "            lat, lng = post.location",
    "            self.geo_index.add_location(post.post_id, lat, lng)",
    "        ",
    "        # Invalidate explore caches (simplified)",
    "        self.explore_cache.clear()",
    "    ",
    "    def engage_with_post(self, user_id: str, post_id: str, engagement_type: str) -> None:",
    "        \"\"\"Record user engagement with post.\"\"\"",
    "        if post_id not in self.posts:",
    "            return",
    "        ",
    "        post = self.posts[post_id]",
    "        if engagement_type == 'like':",
    "            post.like_count += 1",
    "        elif engagement_type == 'comment':",
    "            post.comment_count += 1",
    "        elif engagement_type == 'share':",
    "            post.share_count += 1",
    "        ",
    "        # Track seen",
    "        self.user_seen_posts[user_id].add(post_id)",
    "        ",
    "        # Re-increment trending for engaged hashtags",
    "        for hashtag in post.hashtags:",
    "            self.trending_counter.increment(hashtag.lower().strip('#'))",
    "    ",
    "    # ========================================================================",
    "    # SEARCH FUNCTIONALITY",
    "    # ========================================================================",
    "    ",
    "    def search_users(self, query: str, limit: int = 10, requesting_user_id: str = None) -> List[User]:",
    "        \"\"\"",
    "        Search users by username or display name.",
    "        ",
    "        Args:",
    "            query: Search query string",
    "            limit: Maximum results to return",
    "            requesting_user_id: Optional user making request (for social boosting)",
    "        ",
    "        Returns:",
    "            List of matching users sorted by relevance",
    "        \"\"\"",
    "        if not query or not query.strip():",
    "            return []",
    "        ",
    "        # Get candidates from inverted index",
    "        results = self.user_index.search(query.strip(), limit=limit * 2)",
    "        ",
    "        # Apply social graph boosting if requesting user is known",
    "        if requesting_user_id and requesting_user_id in self.following:",
    "            following_set = self.following[requesting_user_id]",
    "            friends_following: Set[str] = set()",
    "            for friend_id in following_set:",
    "                friends_following.update(self.following.get(friend_id, set()))",
    "            ",
    "            boosted_results = []",
    "            for user_id, score in results:",
    "                boost = 1.0",
    "                if user_id in following_set:",
    "                    boost = 3.0  # Strong boost for users you follow",
    "                elif user_id in friends_following:",
    "                    boost = 1.5  # Medium boost for friends-of-friends",
    "                boosted_results.append((user_id, score * boost))",
    "            ",
    "            boosted_results.sort(key=lambda x: -x[1])",
    "            results = boosted_results",
    "        ",
    "        # Apply verification and follower count boosting",
    "        final_results = []",
    "        for user_id, score in results[:limit]:",
    "            if user_id in self.users:",
    "                user = self.users[user_id]",
    "                adjusted_score = score",
    "                if user.is_verified:",
    "                    adjusted_score *= 1.5",
    "                adjusted_score += math.log1p(user.follower_count) * 0.1",
    "                final_results.append((user, adjusted_score))",
    "        ",
    "        final_results.sort(key=lambda x: -x[1])",
    "        return [user for user, _ in final_results[:limit]]",
    "    ",
    "    def search_hashtag(self, hashtag: str, limit: int = 20, cursor: str = None) -> Tuple[List[Post], str]:",
    "        \"\"\"",
    "        Search posts by hashtag with pagination.",
    "        ",
    "        Args:",
    "            hashtag: Hashtag to search (with or without #)",
    "            limit: Maximum posts to return",
    "            cursor: Pagination cursor from previous request",
    "        ",
    "        Returns:",
    "            Tuple of (list of posts, next cursor)",
    "        \"\"\"",
    "        normalized = hashtag.lower().strip('#')",
    "        ",
    "        if normalized not in self.hashtag_index:",
    "            return [], None",
    "        ",
    "        post_ids = list(self.hashtag_index[normalized])",
    "        ",
    "        # Sort by engagement (likes + comments) descending",
    "        def post_score(pid: str) -> float:",
    "            if pid not in self.posts:",
    "                return 0",
    "            p = self.posts[pid]",
    "            recency = (datetime.utcnow() - p.created_at).total_seconds()",
    "            engagement = p.like_count + p.comment_count * 2",
    "            return engagement / (1 + recency / 86400)  # Decay over days",
    "        ",
    "        post_ids.sort(key=post_score, reverse=True)",
    "        ",
    "        # Handle pagination",
    "        start_idx = 0",
    "        if cursor:",
    "            try:",
    "                start_idx = int(cursor)",
    "            except ValueError:",
    "                start_idx = 0",
    "        ",
    "        end_idx = start_idx + limit",
    "        result_ids = post_ids[start_idx:end_idx]",
    "        ",
    "        posts = [self.posts[pid] for pid in result_ids if pid in self.posts]",
    "        ",
    "        next_cursor = str(end_idx) if end_idx < len(post_ids) else None",
    "        ",
    "        return posts, next_cursor",
    "    ",
    "    def search_location(self, lat: float, lng: float, radius_km: float = 5.0, limit: int = 50) -> List[Post]:",
    "        \"\"\"",
    "        Search posts near a location.",
    "        ",
    "        Args:",
    "            lat: Latitude of center point",
    "            lng: Longitude of center point",
    "            radius_km: Search radius in kilometers",
    "            limit: Maximum posts to return",
    "        ",
    "        Returns:",
    "            List of posts near location, sorted by distance",
    "        \"\"\"",
    "        results = self.geo_index.search_radius(lat, lng, radius_km, limit=limit * 2)",
    "        ",
    "        posts = []",
    "        for post_id, distance in results:",
    "            if post_id in self.posts:",
    "                posts.append((self.posts[post_id], distance))",
    "        ",
    "        # Sort by combination of distance and engagement",
    "        def location_score(item: Tuple[Post, float]) -> float:",
    "            post, distance = item",
    "            engagement = post.like_count + post.comment_count * 2",
    "            return -distance + math.log1p(engagement) * 0.5",
    "        ",
    "        posts.sort(key=location_score, reverse=True)",
    "        ",
    "        return [post for post, _ in posts[:limit]]",
    "    ",
    "    # ========================================================================",
    "    # TRENDING HASHTAGS",
    "    # ========================================================================",
    "    ",
    "    def get_trending_hashtags(self, region: str = \"global\", limit: int = 10) -> List[TrendingHashtag]:",
    "        \"\"\"",
    "        Get trending hashtags for a region.",
    "        ",
    "        Args:",
    "            region: Geographic region or 'global'",
    "            limit: Number of trending hashtags to return",
    "        ",
    "        Returns:",
    "            List of trending hashtags with scores",
    "        \"\"\"",
    "        # Get all known hashtags (in production, maintain a top-K structure)",
    "        all_hashtags = list(self.hashtag_index.keys())",
    "        ",
    "        # Calculate trending scores",
    "        scored_hashtags = []",
    "        for hashtag in all_hashtags:",
    "            current_count = self.trending_counter.get_count(hashtag)",
    "            total_posts = len(self.hashtag_index[hashtag])",
    "            ",
    "            if current_count > 0:",
    "                # Trend score: recent activity relative to total",
    "                # Higher score = more recent activity",
    "                trend_score = current_count / max(1, math.log1p(total_posts))",
    "                scored_hashtags.append(TrendingHashtag(",
    "                    hashtag=f\"#{hashtag}\",",
    "                    post_count=total_posts,",
    "                    trend_score=trend_score,",
    "                    region=region",
    "                ))",
    "        ",
    "        # Sort by trend score",
    "        scored_hashtags.sort(key=lambda x: -x.trend_score)",
    "        ",
    "        return scored_hashtags[:limit]",
    "    ",
    "    # ========================================================================",
    "    # EXPLORE FEED",
    "    # ========================================================================",
    "    ",
    "    def _generate_candidates(self, user_id: str, num_candidates: int = 500) -> List[str]:",
    "        \"\"\"",
    "        Generate candidate posts for explore feed.",
    "        Multi-source retrieval strategy.",
    "        \"\"\"",
    "        candidates: Set[str] = set()",
    "        seen_posts = self.user_seen_posts.get(user_id, set())",
    "        following_set = self.following.get(user_id, set())",
    "        ",
    "        # Source 1: Interest-based (posts with matching hashtags)",
    "        user = self.users.get(user_id)",
    "        if user and user.interests:",
    "            for interest in user.interests:",
    "                matching_posts = self.hashtag_index.get(interest.lower(), set())",
    "                for post_id in list(matching_posts)[:100]:",
    "                    if post_id not in seen_posts:",
    "                        candidates.add(post_id)",
    "        ",
    "        # Source 2: Social signal (posts from friends-of-friends)",
    "        for friend_id in list(following_set)[:20]:",
    "            for fof_id in list(self.following.get(friend_id, set()))[:10]:",
    "                if fof_id not in following_set and fof_id != user_id:",
    "                    for post_id in self.user_posts.get(fof_id, [])[-5:]:",
    "                        if post_id not in seen_posts:",
    "                            candidates.add(post_id)",
    "        ",
    "        # Source 3: Trending posts (high engagement)",
    "        all_posts = list(self.posts.keys())",
    "        trending_posts = sorted(",
    "            all_posts,",
    "            key=lambda pid: self.posts[pid].like_count + self.posts[pid].comment_count * 2,",
    "            reverse=True",
    "        )[:200]",
    "        for post_id in trending_posts:",
    "            if post_id not in seen_posts:",
    "                candidates.add(post_id)",
    "        ",
    "        # Source 4: Geographic (if user has location)",
    "        if user and user.location:",
    "            lat, lng = user.location",
    "            nearby = self.geo_index.search_radius(lat, lng, radius_km=50, limit=100)",
    "            for post_id, _ in nearby:",
    "                if post_id not in seen_posts:",
    "                    candidates.add(post_id)",
    "        ",
    "        # Filter out posts from followed users (explore is for discovery)",
    "        candidates = {",
    "            pid for pid in candidates",
    "            if pid in self.posts and self.posts[pid].user_id not in following_set",
    "        }",
    "        ",
    "        return list(candidates)[:num_candidates]",
    "    ",
    "    def get_explore_feed(self, user_id: str, page_size: int = 20) -> ExploreFeed:",
    "        \"\"\"",
    "        Get personalized explore feed for user.",
    "        ",
    "        Args:",
    "            user_id: User requesting explore content",
    "            page_size: Number of posts to return",
    "        ",
    "        Returns:",
    "            ExploreFeed with personalized recommendations",
    "        \"\"\"",
    "        # Check cache",
    "        cache_key = user_id",
    "        if cache_key in self.explore_cache:",
    "            cached_feed, cached_time = self.explore_cache[cache_key]",
    "            if datetime.utcnow() - cached_time < self.explore_cache_ttl:",
    "                return cached_feed",
    "        ",
    "        # Generate candidates",
    "        candidate_ids = self._generate_candidates(user_id, num_candidates=500)",
    "        ",
    "        if not candidate_ids:",
    "            # Fallback: return popular posts",
    "            all_posts = list(self.posts.values())",
    "            all_posts.sort(key=lambda p: p.like_count + p.comment_count, reverse=True)",
    "            return ExploreFeed(posts=all_posts[:page_size], has_more=len(all_posts) > page_size)",
    "        ",
    "        # Get post objects",
    "        candidate_posts = [self.posts[pid] for pid in candidate_ids if pid in self.posts]",
    "        ",
    "        # ML Ranking",
    "        ranked_posts = self.ml_ranker.rank_posts(user_id, candidate_posts)",
    "        ",
    "        # Select top posts with diversity",
    "        selected_posts = []",
    "        seen_authors: Set[str] = set()",
    "        seen_hashtags: Set[str] = set()",
    "        ",
    "        for post, score in ranked_posts:",
    "            # Diversity constraint: max 2 posts per author",
    "            author_count = sum(1 for p in selected_posts if p.user_id == post.user_id)",
    "            if author_count >= 2:",
    "                continue",
    "            ",
    "            # Add some variety in hashtags",
    "            post_hashtags = set(h.lower() for h in post.hashtags)",
    "            if seen_hashtags and len(post_hashtags & seen_hashtags) == len(post_hashtags):",
    "                # All hashtags already seen, skip with 50% probability",
    "                if random.random() < 0.5:",
    "                    continue",
    "            ",
    "            selected_posts.append(post)",
    "            seen_authors.add(post.user_id)",
    "            seen_hashtags.update(post_hashtags)",
    "            ",
    "            if len(selected_posts) >= page_size:",
    "                break",
    "        ",
    "        # Mark as seen",
    "        for post in selected_posts:",
    "            self.user_seen_posts[user_id].add(post.post_id)",
    "        ",
    "        # Create feed response",
    "        feed = ExploreFeed(",
    "            posts=selected_posts,",
    "            cursor=str(len(selected_posts)),",
    "            has_more=len(ranked_posts) > page_size",
    "        )",
    "        ",
    "        # Cache result",
    "        self.explore_cache[cache_key] = (feed, datetime.utcnow())",
    "        ",
    "        return feed",
    "    ",
    "    # ========================================================================",
    "    # STATISTICS",
    "    # ========================================================================",
    "    ",
    "    def get_stats(self) -> Dict[str, Any]:",
    "        \"\"\"Get system statistics.\"\"\"",
    "        return {",
    "            'total_users': len(self.users),",
    "            'total_posts': len(self.posts),",
    "            'total_hashtags': len(self.hashtag_index),",
    "            'indexed_locations': len(self.geo_index.locations),",
    "            'explore_cache_size': len(self.explore_cache)",
    "        }",
    "",
    "",
    "# ============================================================================",
    "# DEMO AND TESTING",
    "# ============================================================================",
    "",
    "def main():",
    "    \"\"\"Demonstrate Search & Explore functionality.\"\"\"",
    "    print(\"=\"*70)",
    "    print(\"INSTAGRAM SEARCH & EXPLORE - PART 3 DEMO\")",
    "    print(\"=\"*70)",
    "    ",
    "    # Initialize service",
    "    service = InstagramSearchExplore()",
    "    ",
    "    # Create sample users",
    "    print(\"\\n\ud83d\udcdd Creating users...\")",
    "    users = [",
    "        User('u1', 'john_doe', 'John Doe', 'Travel photographer', 50000, 200, True, interests=['travel', 'photography']),",
    "        User('u2', 'jane_smith', 'Jane Smith', 'Food blogger', 30000, 150, False, interests=['food', 'cooking']),",
    "        User('u3', 'john_traveler', 'John Traveler', 'World explorer', 10000, 100, False, interests=['travel', 'adventure']),",
    "        User('u4', 'photo_jane', 'Jane Photo', 'Portrait photographer', 20000, 80, True, interests=['photography', 'portraits']),",
    "        User('u5', 'foodie_mike', 'Mike Foods', 'Restaurant reviewer', 15000, 90, False, interests=['food', 'restaurants'], location=(40.7128, -74.0060)),",
    "        User('u6', 'adventure_sara', 'Sara Adventures', 'Outdoor enthusiast', 8000, 60, False, interests=['adventure', 'hiking'], location=(34.0522, -118.2437)),",
    "    ]",
    "    for user in users:",
    "        service.create_user(user)",
    "        print(f\"  \u2713 Created user: {user.username}\")",
    "    ",
    "    # Create follow relationships",
    "    print(\"\\n\ud83d\udd17 Creating follow relationships...\")",
    "    service.follow_user('u1', 'u2')",
    "    service.follow_user('u1', 'u4')",
    "    service.follow_user('u2', 'u5')",
    "    service.follow_user('u3', 'u1')",
    "    service.follow_user('u5', 'u2')",
    "    print(\"  \u2713 Follow graph created\")",
    "    ",
    "    # Create sample posts",
    "    print(\"\\n\ud83d\udcf8 Creating posts...\")",
    "    posts = [",
    "        Post('p1', 'u1', 'Amazing sunset in Bali! #travel #sunset #bali', 'img1.jpg', ['travel', 'sunset', 'bali'], (40.7128, -74.0060), 'New York', like_count=5000, comment_count=200),",
    "        Post('p2', 'u1', 'Street photography in Tokyo #photography #tokyo #street', 'img2.jpg', ['photography', 'tokyo', 'street'], (35.6762, 139.6503), 'Tokyo', like_count=3000, comment_count=150),",
    "        Post('p3', 'u2', 'Best pizza in NYC! #food #pizza #nyc', 'img3.jpg', ['food', 'pizza', 'nyc'], (40.7580, -73.9855), 'Times Square', like_count=2000, comment_count=100),",
    "        Post('p4', 'u3', 'Hiking in the Alps #travel #adventure #hiking', 'img4.jpg', ['travel', 'adventure', 'hiking'], (46.8182, 8.2275), 'Swiss Alps', like_count=1500, comment_count=80),",
    "        Post('p5', 'u4', 'Portrait session today #photography #portraits #studio', 'img5.jpg', ['photography', 'portraits', 'studio'], like_count=4000, comment_count=180),",
    "        Post('p6', 'u5', 'Trying new ramen spot #food #ramen #japanese', 'img6.jpg', ['food', 'ramen', 'japanese'], (40.7589, -73.9851), 'Midtown', like_count=1800, comment_count=90),",
    "        Post('p7', 'u6', 'Mountain biking adventure #adventure #biking #outdoors', 'img7.jpg', ['adventure', 'biking', 'outdoors'], (34.1184, -118.3004), 'Griffith Park', like_count=1200, comment_count=60),",
    "        Post('p8', 'u1', 'Coffee culture in Melbourne #travel #coffee #melbourne', 'img8.jpg', ['travel', 'coffee', 'melbourne'], (-37.8136, 144.9631), 'Melbourne', like_count=2500, comment_count=120),",
    "    ]",
    "    for post in posts:",
    "        service.create_post(post)",
    "        print(f\"  \u2713 Created post: {post.post_id} by {post.user_id}\")",
    "    ",
    "    # Simulate some engagement",
    "    print(\"\\n\u2764\ufe0f Simulating engagement...\")",
    "    for _ in range(50):",
    "        service.trending_counter.increment('travel')",
    "        service.trending_counter.increment('photography')",
    "    for _ in range(30):",
    "        service.trending_counter.increment('food')",
    "        service.trending_counter.increment('adventure')",
    "    print(\"  \u2713 Engagement simulated\")",
    "    ",
    "    # Test User Search",
    "    print(\"\\n\" + \"=\"*70)",
    "    print(\"\ud83d\udd0d USER SEARCH TESTS\")",
    "    print(\"=\"*70)",
    "    ",
    "    print(\"\\nSearching for 'john':\")",
    "    results = service.search_users('john', limit=5)",
    "    for user in results:",
    "        print(f\"  \u2192 {user.username} ({user.display_name}) - {user.follower_count} followers\")",
    "    ",
    "    print(\"\\nSearching for 'photo':\")",
    "    results = service.search_users('photo', limit=5)",
    "    for user in results:",
    "        print(f\"  \u2192 {user.username} ({user.display_name})\")",
    "    ",
    "    print(\"\\nSearching for 'john' (with social boosting for u3):\")",
    "    results = service.search_users('john', limit=5, requesting_user_id='u3')",
    "    for user in results:",
    "        print(f\"  \u2192 {user.username} ({user.display_name})\")",
    "    ",
    "    # Test Hashtag Search",
    "    print(\"\\n\" + \"=\"*70)",
    "    print(\"#\ufe0f\u20e3 HASHTAG SEARCH TESTS\")",
    "    print(\"=\"*70)",
    "    ",
    "    print(\"\\nSearching for #travel:\")",
    "    posts_found, cursor = service.search_hashtag('travel', limit=5)",
    "    for post in posts_found:",
    "        print(f\"  \u2192 Post {post.post_id}: {post.caption[:40]}... ({post.like_count} likes)\")",
    "    ",
    "    print(\"\\nSearching for #photography:\")",
    "    posts_found, cursor = service.search_hashtag('photography', limit=5)",
    "    for post in posts_found:",
    "        print(f\"  \u2192 Post {post.post_id}: {post.caption[:40]}... ({post.like_count} likes)\")",
    "    ",
    "    # Test Location Search",
    "    print(\"\\n\" + \"=\"*70)",
    "    print(\"\ud83d\udccd LOCATION SEARCH TESTS\")",
    "    print(\"=\"*70)",
    "    ",
    "    print(\"\\nSearching near Times Square, NYC (40.7580, -73.9855):\")",
    "    location_results = service.search_location(40.7580, -73.9855, radius_km=10)",
    "    for post in location_results:",
    "        print(f\"  \u2192 Post {post.post_id}: {post.location_name} - {post.caption[:30]}...\")",
    "    ",
    "    # Test Trending Hashtags",
    "    print(\"\\n\" + \"=\"*70)",
    "    print(\"\ud83d\udcc8 TRENDING HASHTAGS\")",
    "    print(\"=\"*70)",
    "    ",
    "    print(\"\\nTop trending hashtags:\")",
    "    trending = service.get_trending_hashtags(limit=5)",
    "    for i, hashtag in enumerate(trending, 1):",
    "        print(f\"  {i}. {hashtag.hashtag} - {hashtag.post_count} posts (trend score: {hashtag.trend_score:.2f})\")",
    "    ",
    "    # Test Explore Feed",
    "    print(\"\\n\" + \"=\"*70)",
    "    print(\"\ud83c\udf1f EXPLORE FEED TESTS\")",
    "    print(\"=\"*70)",
    "    ",
    "    print(\"\\nExplore feed for u3 (interests: travel, adventure):\")",
    "    explore = service.get_explore_feed('u3', page_size=5)",
    "    for post in explore.posts:",
    "        print(f\"  \u2192 Post {post.post_id} by {post.user_id}: {post.caption[:35]}...\")",
    "        print(f\"      Hashtags: {post.hashtags}, Likes: {post.like_count}\")",
    "    ",
    "    print(\"\\nExplore feed for u5 (interests: food, restaurants):\")",
    "    explore = service.get_explore_feed('u5', page_size=5)",
    "    for post in explore.posts:",
    "        print(f\"  \u2192 Post {post.post_id} by {post.user_id}: {post.caption[:35]}...\")",
    "    ",
    "    # System Stats",
    "    print(\"\\n\" + \"=\"*70)",
    "    print(\"\ud83d\udcca SYSTEM STATISTICS\")",
    "    print(\"=\"*70)",
    "    stats = service.get_stats()",
    "    for key, value in stats.items():",
    "        print(f\"  {key}: {value}\")",
    "    ",
    "    print(\"\\n\" + \"=\"*70)",
    "    print(\"\u2705 DEMO COMPLETED SUCCESSFULLY\")",
    "    print(\"=\"*70)",
    "",
    "",
    "if __name__ == \"__main__\":",
    "    main()"
  ],
  "solution_java_lines": [
    "import java.util.*;",
    "import java.time.*;",
    "import java.security.MessageDigest;",
    "import java.nio.charset.StandardCharsets;",
    "import java.util.concurrent.ConcurrentHashMap;",
    "import java.util.stream.Collectors;",
    "",
    "/**",
    " * Instagram Search & Explore System - Part 3",
    " * Production-quality implementation with search, trending, and ML-based explore.",
    " */",
    "public class InstagramSearchExplore {",
    "",
    "    // ========================================================================",
    "    // DATA MODELS",
    "    // ========================================================================",
    "",
    "    public static class User {",
    "        public String userId;",
    "        public String username;",
    "        public String displayName;",
    "        public String bio;",
    "        public int followerCount;",
    "        public int followingCount;",
    "        public boolean isVerified;",
    "        public List<String> interests;",
    "        public double[] location; // [lat, lng]",
    "",
    "        public User(String userId, String username, String displayName, String bio,",
    "                    int followerCount, boolean isVerified, List<String> interests, double[] location) {",
    "            this.userId = userId;",
    "            this.username = username;",
    "            this.displayName = displayName;",
    "            this.bio = bio;",
    "            this.followerCount = followerCount;",
    "            this.isVerified = isVerified;",
    "            this.interests = interests != null ? interests : new ArrayList<>();",
    "            this.location = location;",
    "        }",
    "    }",
    "",
    "    public static class Post {",
    "        public String postId;",
    "        public String userId;",
    "        public String caption;",
    "        public List<String> hashtags;",
    "        public double[] location;",
    "        public String locationName;",
    "        public Instant createdAt;",
    "        public int likeCount;",
    "        public int commentCount;",
    "        public double qualityScore;",
    "",
    "        public Post(String postId, String userId, String caption, List<String> hashtags,",
    "                    double[] location, String locationName, int likeCount, int commentCount) {",
    "            this.postId = postId;",
    "            this.userId = userId;",
    "            this.caption = caption;",
    "            this.hashtags = hashtags != null ? hashtags : new ArrayList<>();",
    "            this.location = location;",
    "            this.locationName = locationName;",
    "            this.createdAt = Instant.now();",
    "            this.likeCount = likeCount;",
    "            this.commentCount = commentCount;",
    "            this.qualityScore = 0.5;",
    "        }",
    "    }",
    "",
    "    public static class TrendingHashtag {",
    "        public String hashtag;",
    "        public int postCount;",
    "        public double trendScore;",
    "",
    "        public TrendingHashtag(String hashtag, int postCount, double trendScore) {",
    "            this.hashtag = hashtag;",
    "            this.postCount = postCount;",
    "            this.trendScore = trendScore;",
    "        }",
    "    }",
    "",
    "    public static class ExploreFeed {",
    "        public List<Post> posts;",
    "        public String cursor;",
    "        public boolean hasMore;",
    "",
    "        public ExploreFeed(List<Post> posts, String cursor, boolean hasMore) {",
    "            this.posts = posts;",
    "            this.cursor = cursor;",
    "            this.hasMore = hasMore;",
    "        }",
    "    }",
    "",
    "    // ========================================================================",
    "    // COUNT-MIN SKETCH",
    "    // ========================================================================",
    "",
    "    public static class CountMinSketch {",
    "        private final int width;",
    "        private final int depth;",
    "        private final int[][] table;",
    "",
    "        public CountMinSketch(int width, int depth) {",
    "            this.width = width;",
    "            this.depth = depth;",
    "            this.table = new int[depth][width];",
    "        }",
    "",
    "        private int hash(String item, int seed) {",
    "            try {",
    "                MessageDigest md = MessageDigest.getInstance(\"MD5\");",
    "                String input = seed + \":\" + item;",
    "                byte[] hash = md.digest(input.getBytes(StandardCharsets.UTF_8));",
    "                int value = Math.abs(java.nio.ByteBuffer.wrap(hash).getInt());",
    "                return value % width;",
    "            } catch (Exception e) {",
    "                return Math.abs((seed + item).hashCode()) % width;",
    "            }",
    "        }",
    "",
    "        public void increment(String item, int count) {",
    "            for (int i = 0; i < depth; i++) {",
    "                int idx = hash(item, i);",
    "                table[i][idx] += count;",
    "            }",
    "        }",
    "",
    "        public int estimate(String item) {",
    "            int minCount = Integer.MAX_VALUE;",
    "            for (int i = 0; i < depth; i++) {",
    "                int idx = hash(item, i);",
    "                minCount = Math.min(minCount, table[i][idx]);",
    "            }",
    "            return minCount;",
    "        }",
    "    }",
    "",
    "    // ========================================================================",
    "    // SLIDING WINDOW COUNTER",
    "    // ========================================================================",
    "",
    "    public static class SlidingWindowCounter {",
    "        private final int windowMinutes;",
    "        private final int bucketMinutes;",
    "        private final int numBuckets;",
    "        private final Map<Long, CountMinSketch> buckets;",
    "",
    "        public SlidingWindowCounter(int windowMinutes, int bucketMinutes) {",
    "            this.windowMinutes = windowMinutes;",
    "            this.bucketMinutes = bucketMinutes;",
    "            this.numBuckets = windowMinutes / bucketMinutes;",
    "            this.buckets = new ConcurrentHashMap<>();",
    "        }",
    "",
    "        private long getBucketKey(Instant timestamp) {",
    "            long minutesSinceEpoch = timestamp.getEpochSecond() / 60;",
    "            return minutesSinceEpoch / bucketMinutes;",
    "        }",
    "",
    "        public void increment(String item) {",
    "            long bucketKey = getBucketKey(Instant.now());",
    "            buckets.computeIfAbsent(bucketKey, k -> new CountMinSketch(10000, 7));",
    "            buckets.get(bucketKey).increment(item, 1);",
    "            cleanupOldBuckets();",
    "        }",
    "",
    "        public int getCount(String item) {",
    "            long currentKey = getBucketKey(Instant.now());",
    "            long minKey = currentKey - numBuckets;",
    "            int total = 0;",
    "            for (long key = minKey; key <= currentKey; key++) {",
    "                if (buckets.containsKey(key)) {",
    "                    total += buckets.get(key).estimate(item);",
    "                }",
    "            }",
    "            return total;",
    "        }",
    "",
    "        private void cleanupOldBuckets() {",
    "            if (Math.random() > 0.01) return;",
    "            long currentKey = getBucketKey(Instant.now());",
    "            long minKey = currentKey - numBuckets;",
    "            buckets.keySet().removeIf(k -> k < minKey);",
    "        }",
    "    }",
    "",
    "    // ========================================================================",
    "    // INVERTED INDEX",
    "    // ========================================================================",
    "",
    "    public static class InvertedIndex {",
    "        private final Map<String, Set<String>> index;",
    "        private final Map<String, Map<String, String>> documents;",
    "",
    "        public InvertedIndex() {",
    "            this.index = new ConcurrentHashMap<>();",
    "            this.documents = new ConcurrentHashMap<>();",
    "        }",
    "",
    "        private List<String> tokenize(String text) {",
    "            if (text == null) return Collections.emptyList();",
    "            return Arrays.stream(text.toLowerCase().split(\"\\\\W+\"))",
    "                    .filter(t -> t.length() >= 2)",
    "                    .collect(Collectors.toList());",
    "        }",
    "",
    "        public void addDocument(String docId, Map<String, String> content) {",
    "            documents.put(docId, content);",
    "            for (String text : content.values()) {",
    "                if (text != null) {",
    "                    for (String token : tokenize(text)) {",
    "                        index.computeIfAbsent(token, k -> ConcurrentHashMap.newKeySet()).add(docId);",
    "                        for (int i = 2; i < token.length(); i++) {",
    "                            String prefix = \"prefix:\" + token.substring(0, i);",
    "                            index.computeIfAbsent(prefix, k -> ConcurrentHashMap.newKeySet()).add(docId);",
    "                        }",
    "                    }",
    "                }",
    "            }",
    "        }",
    "",
    "        public List<String> search(String query, int limit) {",
    "            Map<String, Double> scores = new HashMap<>();",
    "            for (String token : tokenize(query)) {",
    "                Set<String> matches = index.getOrDefault(token, Collections.emptySet());",
    "                double idf = Math.log(1 + documents.size() / (1.0 + matches.size()));",
    "                for (String docId : matches) {",
    "                    scores.merge(docId, idf, Double::sum);",
    "                }",
    "                Set<String> prefixMatches = index.getOrDefault(\"prefix:\" + token, Collections.emptySet());",
    "                for (String docId : prefixMatches) {",
    "                    scores.merge(docId, 0.5, Double::sum);",
    "                }",
    "            }",
    "            return scores.entrySet().stream()",
    "                    .sorted((a, b) -> Double.compare(b.getValue(), a.getValue()))",
    "                    .limit(limit)",
    "                    .map(Map.Entry::getKey)",
    "                    .collect(Collectors.toList());",
    "        }",
    "    }",
    "",
    "    // ========================================================================",
    "    // GEO INDEX",
    "    // ========================================================================",
    "",
    "    public static class GeoIndex {",
    "        private final double gridSize;",
    "        private final Map<String, Set<String>> grid;",
    "        private final Map<String, double[]> locations;",
    "",
    "        public GeoIndex(double gridSizeDegrees) {",
    "            this.gridSize = gridSizeDegrees;",
    "            this.grid = new ConcurrentHashMap<>();",
    "            this.locations = new ConcurrentHashMap<>();",
    "        }",
    "",
    "        private String getGridCell(double lat, double lng) {",
    "            return (int)(lat / gridSize) + \",\" + (int)(lng / gridSize);",
    "        }",
    "",
    "        private double haversineDistance(double lat1, double lng1, double lat2, double lng2) {",
    "            double R = 6371;",
    "            double dLat = Math.toRadians(lat2 - lat1);",
    "            double dLng = Math.toRadians(lng2 - lng1);",
    "            double a = Math.sin(dLat / 2) * Math.sin(dLat / 2) +",
    "                       Math.cos(Math.toRadians(lat1)) * Math.cos(Math.toRadians(lat2)) *",
    "                       Math.sin(dLng / 2) * Math.sin(dLng / 2);",
    "            return R * 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1 - a));",
    "        }",
    "",
    "        public void addLocation(String itemId, double lat, double lng) {",
    "            String cell = getGridCell(lat, lng);",
    "            grid.computeIfAbsent(cell, k -> ConcurrentHashMap.newKeySet()).add(itemId);",
    "            locations.put(itemId, new double[]{lat, lng});",
    "        }",
    "",
    "        public List<Map.Entry<String, Double>> searchRadius(double lat, double lng, double radiusKm, int limit) {",
    "            int cellRadius = (int)(radiusKm / (gridSize * 111)) + 1;",
    "            int centerLatCell = (int)(lat / gridSize);",
    "            int centerLngCell = (int)(lng / gridSize);",
    "",
    "            List<Map.Entry<String, Double>> results = new ArrayList<>();",
    "            for (int di = -cellRadius; di <= cellRadius; di++) {",
    "                for (int dj = -cellRadius; dj <= cellRadius; dj++) {",
    "                    String cell = (centerLatCell + di) + \",\" + (centerLngCell + dj);",
    "                    Set<String> items = grid.getOrDefault(cell, Collections.emptySet());",
    "                    for (String itemId : items) {",
    "                        double[] loc = locations.get(itemId);",
    "                        double dist = haversineDistance(lat, lng, loc[0], loc[1]);",
    "                        if (dist <= radiusKm) {",
    "                            results.add(new AbstractMap.SimpleEntry<>(itemId, dist));",
    "                        }",
    "                    }",
    "                }",
    "            }",
    "            results.sort(Comparator.comparingDouble(Map.Entry::getValue));",
    "            return results.subList(0, Math.min(limit, results.size()));",
    "        }",
    "    }",
    "",
    "    // ========================================================================",
    "    // MAIN SERVICE",
    "    // ========================================================================",
    "",
    "    private final Map<String, User> users = new ConcurrentHashMap<>();",
    "    private final Map<String, Post> posts = new ConcurrentHashMap<>();",
    "    private final Map<String, Set<String>> hashtagIndex = new ConcurrentHashMap<>();",
    "    private final Map<String, Set<String>> following = new ConcurrentHashMap<>();",
    "    private final Map<String, List<String>> userPosts = new ConcurrentHashMap<>();",
    "    private final Map<String, Set<String>> userSeenPosts = new ConcurrentHashMap<>();",
    "",
    "    private final InvertedIndex userIndex = new InvertedIndex();",
    "    private final GeoIndex geoIndex = new GeoIndex(0.1);",
    "    private final SlidingWindowCounter trendingCounter = new SlidingWindowCounter(60, 15);",
    "",
    "    public void createUser(User user) {",
    "        users.put(user.userId, user);",
    "        Map<String, String> content = new HashMap<>();",
    "        content.put(\"username\", user.username);",
    "        content.put(\"displayName\", user.displayName);",
    "        content.put(\"bio\", user.bio);",
    "        userIndex.addDocument(user.userId, content);",
    "    }",
    "",
    "    public void followUser(String followerId, String followeeId) {",
    "        following.computeIfAbsent(followerId, k -> ConcurrentHashMap.newKeySet()).add(followeeId);",
    "    }",
    "",
    "    public void createPost(Post post) {",
    "        posts.put(post.postId, post);",
    "        userPosts.computeIfAbsent(post.userId, k -> new ArrayList<>()).add(post.postId);",
    "",
    "        for (String hashtag : post.hashtags) {",
    "            String normalized = hashtag.toLowerCase().replace(\"#\", \"\");",
    "            hashtagIndex.computeIfAbsent(normalized, k -> ConcurrentHashMap.newKeySet()).add(post.postId);",
    "            trendingCounter.increment(normalized);",
    "        }",
    "",
    "        if (post.location != null) {",
    "            geoIndex.addLocation(post.postId, post.location[0], post.location[1]);",
    "        }",
    "    }",
    "",
    "    public List<User> searchUsers(String query, int limit) {",
    "        List<String> resultIds = userIndex.search(query, limit);",
    "        return resultIds.stream()",
    "                .map(users::get)",
    "                .filter(Objects::nonNull)",
    "                .sorted((a, b) -> Integer.compare(b.followerCount, a.followerCount))",
    "                .collect(Collectors.toList());",
    "    }",
    "",
    "    public List<Post> searchHashtag(String hashtag, int limit) {",
    "        String normalized = hashtag.toLowerCase().replace(\"#\", \"\");",
    "        Set<String> postIds = hashtagIndex.getOrDefault(normalized, Collections.emptySet());",
    "        return postIds.stream()",
    "                .map(posts::get)",
    "                .filter(Objects::nonNull)",
    "                .sorted((a, b) -> Integer.compare(b.likeCount + b.commentCount, a.likeCount + a.commentCount))",
    "                .limit(limit)",
    "                .collect(Collectors.toList());",
    "    }",
    "",
    "    public List<Post> searchLocation(double lat, double lng, double radiusKm, int limit) {",
    "        return geoIndex.searchRadius(lat, lng, radiusKm, limit).stream()",
    "                .map(e -> posts.get(e.getKey()))",
    "                .filter(Objects::nonNull)",
    "                .collect(Collectors.toList());",
    "    }",
    "",
    "    public List<TrendingHashtag> getTrendingHashtags(int limit) {",
    "        return hashtagIndex.keySet().stream()",
    "                .map(h -> new TrendingHashtag(\"#\" + h, hashtagIndex.get(h).size(),",
    "                        trendingCounter.getCount(h) / Math.max(1, Math.log1p(hashtagIndex.get(h).size()))))",
    "                .sorted((a, b) -> Double.compare(b.trendScore, a.trendScore))",
    "                .limit(limit)",
    "                .collect(Collectors.toList());",
    "    }",
    "",
    "    public ExploreFeed getExploreFeed(String userId, int pageSize) {",
    "        User user = users.get(userId);",
    "        Set<String> followingSet = following.getOrDefault(userId, Collections.emptySet());",
    "        Set<String> seen = userSeenPosts.getOrDefault(userId, Collections.emptySet());",
    "",
    "        Set<String> candidates = new HashSet<>();",
    "",
    "        // Interest-based candidates",
    "        if (user != null && user.interests != null) {",
    "            for (String interest : user.interests) {",
    "                Set<String> matching = hashtagIndex.getOrDefault(interest.toLowerCase(), Collections.emptySet());",
    "                candidates.addAll(matching.stream().limit(100).collect(Collectors.toSet()));",
    "            }",
    "        }",
    "",
    "        // Trending candidates",
    "        List<Post> trending = posts.values().stream()",
    "                .sorted((a, b) -> Integer.compare(b.likeCount, a.likeCount))",
    "                .limit(100)",
    "                .collect(Collectors.toList());",
    "        candidates.addAll(trending.stream().map(p -> p.postId).collect(Collectors.toSet()));",
    "",
    "        // Filter and rank",
    "        List<Post> rankedPosts = candidates.stream()",
    "                .filter(pid -> !seen.contains(pid))",
    "                .map(posts::get)",
    "                .filter(Objects::nonNull)",
    "                .filter(p -> !followingSet.contains(p.userId))",
    "                .sorted((a, b) -> Integer.compare(b.likeCount + b.commentCount, a.likeCount + a.commentCount))",
    "                .limit(pageSize)",
    "                .collect(Collectors.toList());",
    "",
    "        // Mark as seen",
    "        userSeenPosts.computeIfAbsent(userId, k -> ConcurrentHashMap.newKeySet())",
    "                .addAll(rankedPosts.stream().map(p -> p.postId).collect(Collectors.toSet()));",
    "",
    "        return new ExploreFeed(rankedPosts, String.valueOf(rankedPosts.size()), candidates.size() > pageSize);",
    "    }",
    "",
    "    // ========================================================================",
    "    // MAIN DEMO",
    "    // ========================================================================",
    "",
    "    public static void main(String[] args) {",
    "        System.out.println(\"=\".repeat(70));",
    "        System.out.println(\"INSTAGRAM SEARCH & EXPLORE - PART 3 DEMO (JAVA)\");",
    "        System.out.println(\"=\".repeat(70));",
    "",
    "        InstagramSearchExplore service = new InstagramSearchExplore();",
    "",
    "        // Create users",
    "        System.out.println(\"\\n\ud83d\udcdd Creating users...\");",
    "        service.createUser(new User(\"u1\", \"john_doe\", \"John Doe\", \"Travel photographer\",",
    "                50000, true, Arrays.asList(\"travel\", \"photography\"), null));",
    "        service.createUser(new User(\"u2\", \"jane_smith\", \"Jane Smith\", \"Food blogger\",",
    "                30000, false, Arrays.asList(\"food\", \"cooking\"), null));",
    "        service.createUser(new User(\"u3\", \"john_traveler\", \"John Traveler\", \"World explorer\",",
    "                10000, false, Arrays.asList(\"travel\", \"adventure\"), null));",
    "",
    "        // Create posts",
    "        System.out.println(\"\\n\ud83d\udcf8 Creating posts...\");",
    "        service.createPost(new Post(\"p1\", \"u1\", \"Amazing sunset in Bali!\",",
    "                Arrays.asList(\"travel\", \"sunset\", \"bali\"),",
    "                new double[]{40.7128, -74.0060}, \"New York\", 5000, 200));",
    "        service.createPost(new Post(\"p2\", \"u2\", \"Best pizza in NYC!\",",
    "                Arrays.asList(\"food\", \"pizza\", \"nyc\"),",
    "                new double[]{40.7580, -73.9855}, \"Times Square\", 2000, 100));",
    "        service.createPost(new Post(\"p3\", \"u3\", \"Hiking in the Alps!\",",
    "                Arrays.asList(\"travel\", \"adventure\", \"hiking\"),",
    "                new double[]{46.8182, 8.2275}, \"Swiss Alps\", 1500, 80));",
    "",
    "        // Simulate trending",
    "        for (int i = 0; i < 50; i++) {",
    "            service.trendingCounter.increment(\"travel\");",
    "        }",
    "",
    "        // Test user search",
    "        System.out.println(\"\\n\ud83d\udd0d USER SEARCH: 'john'\");",
    "        for (User user : service.searchUsers(\"john\", 5)) {",
    "            System.out.println(\"  \u2192 \" + user.username + \" (\" + user.displayName + \")\");",
    "        }",
    "",
    "        // Test hashtag search",
    "        System.out.println(\"\\n#\ufe0f\u20e3 HASHTAG SEARCH: #travel\");",
    "        for (Post post : service.searchHashtag(\"travel\", 5)) {",
    "            System.out.println(\"  \u2192 \" + post.postId + \": \" + post.caption);",
    "        }",
    "",
    "        // Test location search",
    "        System.out.println(\"\\n\ud83d\udccd LOCATION SEARCH: Near NYC\");",
    "        for (Post post : service.searchLocation(40.7580, -73.9855, 10, 5)) {",
    "            System.out.println(\"  \u2192 \" + post.postId + \": \" + post.locationName);",
    "        }",
    "",
    "        // Test trending",
    "        System.out.println(\"\\n\ud83d\udcc8 TRENDING HASHTAGS\");",
    "        for (TrendingHashtag h : service.getTrendingHashtags(5)) {",
    "            System.out.printf(\"  \u2192 %s (score: %.2f)%n\", h.hashtag, h.trendScore);",
    "        }",
    "",
    "        // Test explore",
    "        System.out.println(\"\\n\ud83c\udf1f EXPLORE FEED for u3\");",
    "        ExploreFeed feed = service.getExploreFeed(\"u3\", 5);",
    "        for (Post post : feed.posts) {",
    "            System.out.println(\"  \u2192 \" + post.postId + \" by \" + post.userId + \": \" + post.caption);",
    "        }",
    "",
    "        System.out.println(\"\\n\" + \"=\".repeat(70));",
    "        System.out.println(\"\u2705 DEMO COMPLETED SUCCESSFULLY\");",
    "        System.out.println(\"=\".repeat(70));",
    "    }",
    "}"
  ],
  "code_walkthrough": [
    {
      "lines": "1-18 (Python)",
      "explanation": "Import statements and type hints. We use dataclasses for clean data models, defaultdict for auto-initializing collections, and heapq for efficient priority queue operations."
    },
    {
      "lines": "23-75 (Python)",
      "explanation": "Data models: User, Post, SearchResult, ExploreFeed, TrendingHashtag. Each uses @dataclass for clean initialization with default values."
    },
    {
      "lines": "80-118 (Python)",
      "explanation": "CountMinSketch implementation: probabilistic data structure for frequency estimation. Uses multiple hash functions to minimize collision errors. O(1) increment and estimate operations."
    },
    {
      "lines": "124-175 (Python)",
      "explanation": "SlidingWindowCounter: Combines CountMinSketch with time-bucketed windows. Enables trending computation over sliding time windows with automatic bucket expiration."
    },
    {
      "lines": "181-255 (Python)",
      "explanation": "InvertedIndex: Core search infrastructure with tokenization, document indexing, and TF-IDF-like scoring. Includes prefix indexing for autocomplete functionality."
    },
    {
      "lines": "261-328 (Python)",
      "explanation": "GeoIndex: Grid-based spatial indexing for location queries. Uses Haversine formula for accurate distance calculation. O(1) bucket lookup with radius expansion."
    },
    {
      "lines": "334-410 (Python)",
      "explanation": "MLRanker: Simulated ML ranking model. Computes relevance scores based on interest matching, engagement signals, quality, and recency decay. Production would use deep learning."
    },
    {
      "lines": "416-540 (Python)",
      "explanation": "Main InstagramSearchExplore class initialization and user/post management. Integrates all indices and counters. Handles follow relationships and post creation with indexing."
    },
    {
      "lines": "545-620 (Python)",
      "explanation": "search_users implementation: Queries inverted index, applies social graph boosting for the requesting user, and applies verification/follower count boosting for final ranking."
    },
    {
      "lines": "622-680 (Python)",
      "explanation": "search_hashtag implementation: Direct hashtag index lookup with engagement-based sorting and cursor-based pagination for efficient scrolling."
    },
    {
      "lines": "682-720 (Python)",
      "explanation": "search_location implementation: Uses GeoIndex for radius search, then combines distance with engagement for final ranking."
    },
    {
      "lines": "725-760 (Python)",
      "explanation": "get_trending_hashtags implementation: Queries sliding window counters, computes trend scores relative to baseline, and returns top-K trending hashtags."
    },
    {
      "lines": "765-850 (Python)",
      "explanation": "get_explore_feed implementation: Multi-source candidate generation (interest, social, trending, geographic), filtering (seen, blocked), ML ranking, and diversity sampling."
    }
  ],
  "complexity_analysis": {
    "time": {
      "new_methods": {
        "searchUsers": {
          "complexity": "O(log n + k log k)",
          "explanation": "Inverted index lookup O(log n), then sorting top-k results O(k log k)"
        },
        "searchHashtag": {
          "complexity": "O(1) + O(m log m)",
          "explanation": "Hash lookup O(1), sorting m matching posts O(m log m)"
        },
        "searchLocation": {
          "complexity": "O(r\u00b2 + m log m)",
          "explanation": "Grid cells search O(r\u00b2) where r is radius in cells, distance calculation and sorting O(m log m)"
        },
        "getTrendingHashtags": {
          "complexity": "O(h log h)",
          "explanation": "Iterate all h hashtags, sort by trend score"
        },
        "getExploreFeed": {
          "complexity": "O(c log c)",
          "explanation": "Generate c candidates from multiple sources, ML rank and sort"
        }
      },
      "overall_change": "Search operations are now O(log n) instead of O(n) due to inverted indices. Trending computation is O(1) per hashtag due to Count-Min Sketch."
    },
    "space": {
      "additional_space": "O(n\u00b7v + h\u00b7w\u00b7d + g\u00b2\u00b7p)",
      "explanation": "n\u00b7v for inverted index (n documents, v average vocabulary), h\u00b7w\u00b7d for trending sketch (h hashtags, w width, d depth), g\u00b2\u00b7p for geo grid (g cells, p posts per cell)"
    }
  },
  "dry_run": {
    "example_input": "searchUsers('john', 5) with users: john_doe, jane_smith, john_traveler",
    "steps": [
      {
        "step": 1,
        "action": "Tokenize query",
        "state": "tokens = ['john']",
        "explanation": "Normalize and tokenize the search query"
      },
      {
        "step": 2,
        "action": "Inverted index lookup",
        "state": "candidates = {john_doe: 0.8, john_traveler: 0.7}",
        "explanation": "Find documents containing 'john' with TF-IDF scores"
      },
      {
        "step": 3,
        "action": "Prefix match boost",
        "state": "candidates = {john_doe: 0.85, john_traveler: 0.75}",
        "explanation": "Boost scores for prefix matches"
      },
      {
        "step": 4,
        "action": "Apply verification boost",
        "state": "john_doe score *= 1.5",
        "explanation": "Verified users get 1.5x boost"
      },
      {
        "step": 5,
        "action": "Apply follower count boost",
        "state": "scores += log(followers) * 0.1",
        "explanation": "More followers = higher relevance"
      },
      {
        "step": 6,
        "action": "Sort and return",
        "state": "result = [john_doe, john_traveler]",
        "explanation": "Return top-k by final score"
      }
    ],
    "final_output": "[User(john_doe, 50000 followers), User(john_traveler, 10000 followers)]"
  },
  "edge_cases": [
    {
      "case": "Empty search query",
      "handling": "Return empty list immediately",
      "gotcha": "Don't query index with empty tokens"
    },
    {
      "case": "Non-existent hashtag",
      "handling": "Return empty list, don't throw error",
      "gotcha": "Check hashtagIndex.contains before accessing"
    },
    {
      "case": "Location with no nearby posts",
      "handling": "Return empty list gracefully",
      "gotcha": "GeoIndex may return empty results for remote areas"
    },
    {
      "case": "New user with no engagement history",
      "handling": "Fall back to trending posts for explore",
      "gotcha": "Don't fail ML ranking with missing features"
    },
    {
      "case": "User following everyone",
      "handling": "Explore still works by showing non-followed accounts",
      "gotcha": "Filter out all followed users from explore candidates"
    },
    {
      "case": "Hashtag with millions of posts",
      "handling": "Limit results and use engagement-based ranking",
      "gotcha": "Don't load all posts into memory"
    }
  ],
  "test_cases": [
    {
      "name": "Basic user search",
      "input": "searchUsers('john', 10)",
      "expected": "[john_doe, john_traveler]",
      "explanation": "Returns users with 'john' in username/name, ordered by relevance"
    },
    {
      "name": "Hashtag search with pagination",
      "input": "searchHashtag('travel', 5, null)",
      "expected": "([post1, post3], '5')",
      "explanation": "Returns posts with #travel, sorted by engagement, with cursor for next page"
    },
    {
      "name": "Location search NYC",
      "input": "searchLocation(40.7580, -73.9855, 10.0)",
      "expected": "[post1, post2]",
      "explanation": "Returns posts within 10km of Times Square"
    },
    {
      "name": "Trending hashtags",
      "input": "getTrendingHashtags('global', 3)",
      "expected": "[#travel, #photography, #food]",
      "explanation": "Returns hashtags with highest recent activity relative to baseline"
    },
    {
      "name": "Personalized explore feed",
      "input": "getExploreFeed('u3', 5)",
      "expected": "ExploreFeed with travel/adventure posts not from followed users",
      "explanation": "Returns ML-ranked posts matching user interests"
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Using LIKE queries for search",
      "why_wrong": "O(n) full table scan, unacceptable latency at scale",
      "correct_approach": "Use inverted indices (Elasticsearch) for O(log n) lookups",
      "code_example_wrong": "SELECT * FROM users WHERE username LIKE '%john%'",
      "code_example_correct": "elasticsearch.search(index='users', query={'match': {'username': 'john'}})"
    },
    {
      "mistake": "Computing trending by counting all hashtags",
      "why_wrong": "O(n) per query, too slow for real-time trending",
      "correct_approach": "Use Count-Min Sketch with sliding windows for O(1) operations",
      "code_example_wrong": "SELECT hashtag, COUNT(*) FROM posts WHERE created_at > NOW() - INTERVAL 1 HOUR GROUP BY hashtag",
      "code_example_correct": "trending_counter.get_count(hashtag)  # O(1) from pre-computed sketch"
    },
    {
      "mistake": "Returning same explore posts repeatedly",
      "why_wrong": "Poor user experience, users see duplicate content",
      "correct_approach": "Track seen posts per user and filter from candidates",
      "code_example_wrong": "return random.sample(all_posts, page_size)",
      "code_example_correct": "candidates = [p for p in all_posts if p.post_id not in user_seen_posts]"
    },
    {
      "mistake": "Running ML model on all posts",
      "why_wrong": "O(n) inference cost, too slow for real-time",
      "correct_approach": "Two-stage: fast candidate retrieval, then ML ranking on subset",
      "code_example_wrong": "scores = [model.predict(user, post) for post in all_posts]",
      "code_example_correct": "candidates = retrieve_candidates(user, limit=500); scores = model.batch_predict(user, candidates)"
    }
  ],
  "interview_tips": {
    "how_to_present": "Start by clarifying search vs explore use cases. Draw the architecture diagram showing separate search (Elasticsearch) and recommendation (ML) paths. Explain why two-stage ranking is essential for explore. Discuss Count-Min Sketch for trending as a 'clever trick' moment.",
    "what_to_mention": [
      "Elasticsearch for full-text search with inverted indices",
      "Count-Min Sketch for memory-efficient trending computation",
      "Two-stage ML pipeline: candidate generation + ranking",
      "PostGIS/geo_point for location-based queries",
      "Real-time indexing via Kafka consumers",
      "Diversity sampling in explore to avoid monotonous feeds"
    ],
    "time_allocation": "Spend 5 minutes on search architecture, 5 minutes on trending algorithm (Count-Min Sketch), 5 minutes on explore feed ML pipeline, 2-3 minutes on trade-offs and scaling",
    "if_stuck": [
      "Think about what makes search different from feed: search is pull (user knows what they want), explore is push (system recommends)",
      "For trending, think about how to count efficiently: can we avoid counting everything?",
      "For explore, think about Netflix/YouTube: how do they recommend without being slow?",
      "Consider: what's the latency budget? Search needs <100ms, which rules out full scans"
    ]
  },
  "connection_to_next_part": "Part 4 might focus on Direct Messaging or Notifications. The search infrastructure from Part 3 (inverted indices) can be reused for message search. The ML ranking models can be extended for notification prioritization. The real-time indexing pipeline (Kafka) established here will be crucial for message delivery guarantees.",
  "generated_at": "2026-01-14T15:30:40.670814",
  "_meta": {
    "problem_id": "instagram_photo_sharing_design",
    "part_number": 3,
    "model": "claude-opus-4-5-20251101"
  }
}