{
  "problem_title": "News Feed Aggregator System",
  "difficulty": "medium",
  "category": "HLD/System Design",
  "estimated_time": "45-60 minutes",
  "problem_analysis": {
    "first_impressions": "This is a classic read-heavy system design problem similar to Twitter's feed or Google News. The 1700:1 read/write ratio immediately signals aggressive caching is essential. The key challenge is balancing personalization quality with latency requirements (<200ms p99).",
    "pattern_recognition": "Read-Heavy System + Multi-Level Caching + Fan-out on Read vs Fan-out on Write + Pub-Sub Pattern + Personalization/Ranking + Database Sharding",
    "key_constraints": [
      "100K QPS reads - requires distributed caching layer (Redis cluster)",
      "<200ms p99 latency - must pre-compute or cache heavily; cannot compute fresh feeds on every request",
      "5 minute freshness - cache TTL must be short; need smart invalidation",
      "10M users, 1000 publishers - database sharding strategy needed",
      "1700:1 read/write ratio - optimize for reads, can tolerate slower writes",
      "Cold start problem - new users have no data for personalization"
    ],
    "clarifying_questions": [
      "What's the average number of publishers a user follows? (5-50) - Affects fan-out strategy",
      "Should feeds include only followed publishers or also recommendations? - Impacts ranking complexity",
      "Is real-time breaking news required or is 5-minute delay acceptable? - Determines push vs pull",
      "What's the article lifecycle - how long should articles stay in feeds? - Affects storage/cleanup",
      "Do we need to support infinite scroll or fixed pages? - Impacts pagination strategy",
      "What's the geographic distribution of users? - Multi-region caching decisions",
      "How do we handle duplicate stories from multiple publishers? - Deduplication algorithm"
    ],
    "edge_cases_to_consider": [
      "Cold start: New user with no follows or interests",
      "Celebrity publisher: Publisher with 1M+ followers (fan-out challenge)",
      "Viral article: 10x traffic spike on trending content",
      "Publisher goes down: RSS feed unavailable for extended period",
      "User follows 50 publishers but only wants 10 articles",
      "Same story from 10 publishers - deduplication needed",
      "User unfollows publisher mid-feed generation"
    ]
  },
  "visual_explanation": {
    "problem_visualization": "```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    NEWS FEED AGGREGATOR - DATA FLOW                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                              \u2502\n\u2502   INGESTION (Write Path)                 SERVING (Read Path)                 \u2502\n\u2502   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                   \u2502\n\u2502                                                                              \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502   \u2502Publisher\u2502\u2500\u2500RSS/API\u2500\u2500\u2510                        \u2502  User   \u2502                \u2502\n\u2502   \u2502   A     \u2502           \u2502                        \u2502 Request \u2502                \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502                        \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502                             \u2502                      \u2502\n\u2502   \u2502Publisher\u2502\u2500\u2500RSS/API\u2500\u2500\u253c\u2500\u2500\u25ba\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u25bc                      \u2502\n\u2502   \u2502   B     \u2502           \u2502   \u2502   Crawler    \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502   \u2502   Workers    \u2502    \u2502   CDN/   \u2502                \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502   LB     \u2502                \u2502\n\u2502   \u2502Publisher\u2502\u2500\u2500RSS/API\u2500\u2500\u2518          \u2502            \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n\u2502   \u2502   C     \u2502                      \u25bc                 \u2502                      \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u25bc                      \u2502\n\u2502                            \u2502 Message Queue\u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n\u2502                            \u2502   (Kafka)    \u2502    \u2502API Server\u2502                 \u2502\n\u2502                            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502\n\u2502                                   \u2502                 \u2502                       \u2502\n\u2502                                   \u25bc                 \u25bc                       \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502                         CACHE LAYER (Redis Cluster)                 \u2502   \u2502\n\u2502   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502   \u2502\n\u2502   \u2502  \u2502  Category  \u2502  \u2502 Publisher  \u2502  \u2502    User    \u2502  \u2502  Trending  \u2502    \u2502   \u2502\n\u2502   \u2502  \u2502   Cache    \u2502  \u2502   Cache    \u2502  \u2502Feed Cache  \u2502  \u2502   Cache    \u2502    \u2502   \u2502\n\u2502   \u2502  \u2502 TTL: 5min  \u2502  \u2502 TTL: 2min  \u2502  \u2502 TTL: 1min  \u2502  \u2502 TTL: 30s   \u2502    \u2502   \u2502\n\u2502   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                   \u2502                                         \u2502\n\u2502                                   \u25bc (cache miss)                            \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502   \u2502                      DATABASE LAYER                                 \u2502   \u2502\n\u2502   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502   \u2502\n\u2502   \u2502  \u2502 PostgreSQL \u2502  \u2502 PostgreSQL \u2502  \u2502   Elastic  \u2502  \u2502   Redis    \u2502    \u2502   \u2502\n\u2502   \u2502  \u2502 (Articles) \u2502  \u2502  (Users)   \u2502  \u2502  Search    \u2502  \u2502 (Counters) \u2502    \u2502   \u2502\n\u2502   \u2502  \u2502  Sharded   \u2502  \u2502  Sharded   \u2502  \u2502            \u2502  \u2502            \u2502    \u2502   \u2502\n\u2502   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502   \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
    "data_structure_state": "```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         DATA MODEL RELATIONSHIPS                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                              \u2502\n\u2502   PUBLISHER TABLE                    ARTICLE TABLE                           \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502   \u2502 publisher_id (PK)\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502 article_id (PK)            \u2502         \u2502\n\u2502   \u2502 name             \u2502          \u2502    \u2502 publisher_id (FK, indexed) \u2502\u25c4\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502   \u2502 rss_url          \u2502          \u2502    \u2502 title                      \u2502         \u2502\n\u2502   \u2502 categories[]     \u2502          \u2502    \u2502 content_url                \u2502         \u2502\n\u2502   \u2502 last_poll_time   \u2502          \u2502    \u2502 thumbnail_url              \u2502         \u2502\n\u2502   \u2502 status           \u2502          \u2514\u2500\u2500\u2500\u2500\u2502 categories[]               \u2502         \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502 published_at (indexed)     \u2502         \u2502\n\u2502                                       \u2502 created_at (indexed)       \u2502         \u2502\n\u2502   USER TABLE                          \u2502 like_count                 \u2502         \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502 view_count                 \u2502         \u2502\n\u2502   \u2502 user_id (PK)     \u2502                \u2502 content_hash (dedup)       \u2502         \u2502\n\u2502   \u2502 interests[]      \u2502                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502   \u2502 created_at       \u2502                                                       \u2502\n\u2502   \u2502 last_active      \u2502                USER_FOLLOWS TABLE                     \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502            \u2502                          \u2502 user_id (PK, FK)           \u2502\u25c4\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502            \u2502                          \u2502 publisher_id (PK, FK)      \u2502         \u2502\n\u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\u2502 followed_at                \u2502         \u2502\n\u2502                                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502                                                                              \u2502\n\u2502   USER_LIKES TABLE                    ENGAGEMENT TABLE                       \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502   \u2502 user_id (PK, FK) \u2502                \u2502 user_id                    \u2502         \u2502\n\u2502   \u2502 article_id (PK)  \u2502                \u2502 publisher_id               \u2502         \u2502\n\u2502   \u2502 liked_at         \u2502                \u2502 interaction_count          \u2502         \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502 last_interaction           \u2502         \u2502\n\u2502                                       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502                                                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
    "algorithm_flow": [
      {
        "step": 1,
        "description": "User requests feed via GET /feed?page=0&size=20",
        "visualization": "```\nUser \u2500\u2500\u2500\u2500\u2500\u25ba Load Balancer \u2500\u2500\u2500\u2500\u2500\u25ba API Server\n                                     \u2502\n                                     \u25bc\n                            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                            \u2502 Check Auth &   \u2502\n                            \u2502 Extract userId \u2502\n                            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
        "key_point": "Load balancer distributes 100K QPS across API server fleet"
      },
      {
        "step": 2,
        "description": "Check user-specific feed cache (L1 cache)",
        "visualization": "```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         REDIS: USER FEED CACHE            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Key: feed:user_123:page_0                 \u2502\n\u2502 Value: [article_ids...] (serialized)      \u2502\n\u2502 TTL: 60 seconds                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 CACHE HIT?                                \u2502\n\u2502  \u251c\u2500\u2500 YES \u2192 Hydrate articles, return       \u2502\n\u2502  \u2514\u2500\u2500 NO  \u2192 Continue to step 3             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
        "key_point": "60-second TTL balances freshness vs cache hit rate"
      },
      {
        "step": 3,
        "description": "Load user profile: interests + followed publishers",
        "visualization": "```\nUser Profile (from cache/DB):\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 user_id: 123                        \u2502\n\u2502 interests: [Tech, Sports]           \u2502\n\u2502 followed_publishers: [NYT, ESPN]    \u2502\n\u2502 engagement_history:                 \u2502\n\u2502   - ESPN: 15 likes                  \u2502\n\u2502   - NYT: 3 likes                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
        "key_point": "User profile is also cached with 5-minute TTL"
      },
      {
        "step": 4,
        "description": "Gather candidate articles from multiple sources",
        "visualization": "```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              CANDIDATE GATHERING                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                        \u2502\n\u2502  Source 1: Publisher Cache          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500           \u2502  50 recent  \u2502  \u2502\n\u2502  For each followed publisher:  \u2500\u2500\u2500\u2500\u25ba\u2502  articles   \u2502  \u2502\n\u2502  cache.get(\"articles:pub:NYT\")      \u2502  from NYT   \u2502  \u2502\n\u2502                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                        \u2502\n\u2502  Source 2: Category Cache           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500           \u2502 Top 100 in  \u2502  \u2502\n\u2502  For each user interest:       \u2500\u2500\u2500\u2500\u25ba\u2502   Tech      \u2502  \u2502\n\u2502  cache.get(\"articles:cat:Tech\")     \u2502  category   \u2502  \u2502\n\u2502                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                        \u2502\n\u2502  Source 3: Trending (cold start)    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500       \u2502 Global top  \u2502  \u2502\n\u2502  cache.get(\"articles:trending\") \u2500\u2500\u2500\u25ba\u2502  50 today   \u2502  \u2502\n\u2502                                      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                        \u2502\n\u2502  Total Candidates: ~200-500 articles                  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
        "key_point": "Pull from multiple cached sources to build candidate pool"
      },
      {
        "step": 5,
        "description": "Score and rank candidates using personalization algorithm",
        "visualization": "```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                  RANKING ALGORITHM                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                        \u2502\n\u2502  score = (publisher_match * 3.0)      // weight: 3.0  \u2502\n\u2502        + (category_match  * 2.0)      // weight: 2.0  \u2502\n\u2502        + (recency_factor  * 1.5)      // weight: 1.5  \u2502\n\u2502        + (engagement_boost * 1.0)     // weight: 1.0  \u2502\n\u2502        + (popularity_score * 0.5)     // weight: 0.5  \u2502\n\u2502                                                        \u2502\n\u2502  Example Calculation:                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502 Article: \"Apple announces new iPhone\"           \u2502  \u2502\n\u2502  \u2502 Publisher: TechCrunch (followed) \u2192 +3.0         \u2502  \u2502\n\u2502  \u2502 Category: Tech (user interest)   \u2192 +2.0         \u2502  \u2502\n\u2502  \u2502 Published: 30 min ago            \u2192 +1.2         \u2502  \u2502\n\u2502  \u2502 User liked 5 TechCrunch articles \u2192 +0.5         \u2502  \u2502\n\u2502  \u2502 10K views globally               \u2192 +0.3         \u2502  \u2502\n\u2502  \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500           \u2502  \u2502\n\u2502  \u2502 FINAL SCORE: 7.0                                \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
        "key_point": "Ranking is CPU-intensive but operates on cached candidates"
      },
      {
        "step": 6,
        "description": "Deduplicate, paginate, cache, and return",
        "visualization": "```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 FINAL PROCESSING                       \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                        \u2502\n\u2502  1. DEDUPLICATE by content_hash                       \u2502\n\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502     \u2502 Same story from NYT, CNN, BBC?        \u2502         \u2502\n\u2502     \u2502 Keep highest-scored source only       \u2502         \u2502\n\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502                                                        \u2502\n\u2502  2. DIVERSIFY sources                                  \u2502\n\u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502     \u2502 No more than 3 consecutive articles   \u2502         \u2502\n\u2502     \u2502 from same publisher                   \u2502         \u2502\n\u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u2502                                                        \u2502\n\u2502  3. PAGINATE                                           \u2502\n\u2502     page=0, size=20 \u2192 articles[0:20]                  \u2502\n\u2502                                                        \u2502\n\u2502  4. CACHE final feed                                   \u2502\n\u2502     cache.set(\"feed:user_123:page_0\", result, 60s)    \u2502\n\u2502                                                        \u2502\n\u2502  5. RETURN to user (< 200ms total)                    \u2502\n\u2502                                                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
        "key_point": "Deduplication and diversification ensure quality feed"
      }
    ],
    "dry_run_table": "| Step | Operation | Cache State | Database Query | Result |\n|------|-----------|-------------|----------------|--------|\n| 1 | registerPublisher(\"nyt\", \"NY Times\", \"...\", [\"Politics\"]) | - | INSERT publishers | Publisher registered |\n| 2 | registerPublisher(\"espn\", \"ESPN\", \"...\", [\"Sports\"]) | - | INSERT publishers | Publisher registered |\n| 3 | fetchArticles(\"nyt\") - crawler runs | articles:pub:nyt = [a1,a2,a3] | INSERT articles | 3 articles ingested |\n| 4 | fetchArticles(\"espn\") - crawler runs | articles:pub:espn = [a4,a5] | INSERT articles | 2 articles ingested |\n| 5 | followPublisher(\"user1\", \"espn\") | user:profile:user1 updated | INSERT user_follows | User1 follows ESPN |\n| 6 | setUserInterests(\"user1\", [\"Sports\"]) | user:profile:user1 updated | UPDATE users | Interests set |\n| 7 | getUserFeed(\"user1\", 0, 10) | MISS feed:user1:page0 | - | Generate feed |\n| 8 | \u2192 Gather candidates | Read articles:pub:espn + articles:cat:Sports | - | [a4,a5,...] |\n| 9 | \u2192 Score & rank | - | - | [a4: 7.5, a5: 6.2] |\n| 10 | \u2192 Cache & return | SET feed:user1:page0 | - | [a4, a5] returned |"
  },
  "thinking_process": {
    "step_by_step": [
      "When I see '100K QPS reads', I immediately think 'aggressive caching required'. No database can handle this directly without a cache layer.",
      "The 1700:1 read/write ratio tells me this is extremely read-heavy - I should optimize reads at the expense of write complexity. Fan-out on write might work.",
      "When I see '<200ms p99 latency', I know I cannot compute personalized feeds from scratch on every request. Pre-computation or smart caching is mandatory.",
      "The '5 minute freshness' requirement gives me flexibility - I can cache feeds with short TTLs and still meet freshness SLA.",
      "For personalization with 10M users, I can't pre-compute feeds for everyone (storage explosion). Hybrid approach: cache hot users, generate cold users on-demand.",
      "The 'cold start problem' suggests I need a fallback strategy: trending/popular content for new users until they build preferences.",
      "Multiple publishers covering same stories means I need content-based deduplication (hash title + key entities, or use NLP similarity)."
    ],
    "key_insight": "The critical realization is that this is a **read-heavy system with personalization** - the solution is multi-level caching where we cache at **component level** (categories, publishers) rather than **user level**, then assemble personalized feeds from cached components. This gives us O(followed_publishers) assembly time instead of database queries.",
    "why_this_works": "By caching articles at the publisher and category level (shared across millions of users), we achieve massive cache hit rates. User-specific feed generation becomes a lightweight in-memory operation: fetch cached publisher/category articles \u2192 score \u2192 sort \u2192 return. This shifts the heavy lifting to the write path (article ingestion) where we have 1000x less traffic."
  },
  "approaches": [
    {
      "name": "Approach 1: Fan-Out on Write (Push Model)",
      "description": "When a publisher publishes an article, immediately push it to all followers' feed caches. Each user has a pre-computed feed list.",
      "pseudocode": "```\nonNewArticle(article):\n  followers = getFollowers(article.publisherId)  // Could be millions!\n  for each follower:\n    feed = getCache(\"feed:\" + follower.id)\n    feed.insertSorted(article, computeScore(article, follower))\n    setCache(\"feed:\" + follower.id, feed)\n```",
      "time_complexity": "O(followers) per new article - problematic for popular publishers",
      "space_complexity": "O(users \u00d7 feed_size) - massive storage",
      "pros": [
        "Extremely fast reads - O(1) cache lookup",
        "Simple read path",
        "Guaranteed freshness after push completes"
      ],
      "cons": [
        "Celebrity publisher problem: NYT with 5M followers = 5M writes per article",
        "Storage explosion: 10M users \u00d7 100 articles \u00d7 1KB = 1TB just for feeds",
        "Wasted work: Many cached feeds never accessed",
        "Stale feeds for inactive users"
      ],
      "when_to_use": "Only for small-scale systems (<100K users) or when publishers have few followers"
    },
    {
      "name": "Approach 2: Fan-Out on Read (Pull Model)",
      "description": "Generate feed on-the-fly when user requests it. Query each followed publisher's recent articles, merge, rank, return.",
      "pseudocode": "```\ngetUserFeed(userId, page, size):\n  user = getUser(userId)\n  articles = []\n  for publisherId in user.followedPublishers:  // 5-50 publishers\n    articles.extend(getRecentArticles(publisherId, 50))  // DB query each!\n  \n  articles = deduplicate(articles)\n  articles = rank(articles, user)\n  return paginate(articles, page, size)\n```",
      "time_complexity": "O(followed_publishers \u00d7 articles_per_publisher) per request",
      "space_complexity": "O(1) - no pre-computed storage",
      "pros": [
        "No storage for pre-computed feeds",
        "Always fresh data",
        "No wasted work for inactive users"
      ],
      "cons": [
        "50 publishers \u00d7 DB query = 50 queries per feed request - too slow!",
        "Cannot meet 200ms latency at 100K QPS",
        "Database would be crushed (100K \u00d7 50 = 5M queries/second)"
      ],
      "when_to_use": "Never at this scale without significant caching"
    },
    {
      "name": "Optimal: Hybrid Model with Multi-Level Caching",
      "description": "Cache at component level (publishers, categories), assemble feeds from cached components on read. Best of both worlds.",
      "pseudocode": "```\n# Write Path (on new article)\nonNewArticle(article):\n  updateCache(\"articles:pub:\" + article.publisherId, article)  // O(1)\n  for category in article.categories:\n    updateCache(\"articles:cat:\" + category, article)  // O(categories)\n  updateCache(\"articles:trending\", article)  // if trending\n\n# Read Path (on feed request)  \ngetUserFeed(userId, page, size):\n  # Check L1: user feed cache\n  cached = getCache(\"feed:\" + userId + \":page_\" + page)\n  if cached: return cached\n  \n  # Gather from L2: component caches (all cache hits!)\n  candidates = []\n  for pubId in user.followedPublishers:\n    candidates.extend(getCache(\"articles:pub:\" + pubId))  // Cache hit!\n  for interest in user.interests:\n    candidates.extend(getCache(\"articles:cat:\" + interest))  // Cache hit!\n  \n  # Rank in memory\n  candidates = deduplicate(candidates)\n  scored = [(article, score(article, user)) for article in candidates]\n  sorted_feed = sort(scored, reverse=True)\n  \n  # Cache L1 and return\n  result = paginate(sorted_feed, page, size)\n  setCache(\"feed:\" + userId + \":page_\" + page, result, ttl=60)\n  return result\n```",
      "time_complexity": "O(followed_publishers + interests) cache lookups + O(n log n) sorting",
      "space_complexity": "O(publishers \u00d7 articles_per_pub + categories \u00d7 articles_per_cat)",
      "pros": [
        "Component caches shared across millions of users - efficient",
        "No celebrity problem - writes are O(categories), not O(followers)",
        "Can meet 200ms latency with all cache hits",
        "Graceful degradation on cache miss"
      ],
      "cons": [
        "More complex implementation",
        "Slight staleness (60-second user feed cache)"
      ],
      "key_insight": "Cache the building blocks (publisher articles, category articles), not the final product (user feeds). Assembly is cheap, storage is expensive."
    }
  ],
  "optimal_solution": {
    "name": "Hybrid Model with Multi-Level Caching and Personalized Ranking",
    "explanation_md": "## High-Level Architecture\n\nThe optimal solution uses a **hybrid fan-out model** with **multi-level caching**:\n\n### Write Path (Article Ingestion)\n1. **Crawler Workers** poll publisher RSS feeds every 2-5 minutes\n2. New articles go to **Kafka** for reliable processing\n3. **Processing Pipeline**: deduplication \u2192 categorization \u2192 indexing\n4. Articles stored in **PostgreSQL** (sharded by publisher_id)\n5. **Cache Warming**: Update publisher cache, category cache, trending cache\n\n### Read Path (Feed Generation)\n1. Check **L1 Cache**: User-specific feed cache (60s TTL)\n2. On miss, load **User Profile** from cache/DB\n3. Gather candidates from **L2 Caches**: publisher articles + category articles\n4. **In-Memory Ranking**: Score candidates using personalization algorithm\n5. **Deduplicate, Diversify, Paginate**\n6. Cache result in L1 and return\n\n### Why This Works\n- **L2 Caches are shared**: 1000 publishers \u00d7 100 articles = 100K cached articles (manageable)\n- **L1 Cache hit rate ~70%**: Active users get cached feeds\n- **Assembly is fast**: ~5ms to gather + rank 500 articles in memory\n- **Meets 200ms p99**: Cache hits dominate; misses still fast",
    "data_structures": [
      {
        "structure": "Redis Cluster (L1 - User Feed Cache)",
        "purpose": "Key: feed:{userId}:page_{n}, Value: serialized Article IDs, TTL: 60s. Handles 100K QPS with ~70% hit rate."
      },
      {
        "structure": "Redis Cluster (L2 - Publisher Cache)",
        "purpose": "Key: articles:pub:{publisherId}, Value: last 100 articles sorted by time, TTL: 5 min. Shared across all followers."
      },
      {
        "structure": "Redis Cluster (L2 - Category Cache)",
        "purpose": "Key: articles:cat:{category}, Value: top 100 articles by popularity, TTL: 5 min. Shared across interested users."
      },
      {
        "structure": "Redis Sorted Set (Trending Cache)",
        "purpose": "Key: articles:trending, Score: view_count + like_count. Used for cold start users."
      },
      {
        "structure": "PostgreSQL (Articles - Sharded)",
        "purpose": "Shard key: publisher_id. Indexes: (publisher_id, published_at), (category, published_at). Source of truth."
      },
      {
        "structure": "PostgreSQL (Users - Sharded)",
        "purpose": "Shard key: user_id. Stores profile, interests, followed publishers. Consistent hashing for sharding."
      },
      {
        "structure": "Elasticsearch",
        "purpose": "Full-text search for articles. Also used for similar article detection (deduplication)."
      },
      {
        "structure": "HashMap<publisherId, List<Article>>",
        "purpose": "In-memory working set during feed assembly. Typically 200-500 candidate articles."
      }
    ],
    "algorithm_steps": [
      "1. **Receive Feed Request**: Authenticate user, extract userId, page, pageSize",
      "2. **Check L1 Cache**: Look up feed:{userId}:page_{page}. If hit, hydrate article details and return.",
      "3. **Load User Profile**: Get from cache or DB - interests[], followedPublishers[], engagementHistory{}",
      "4. **Gather Candidates**: For each followed publisher, fetch from articles:pub:{id}. For each interest, fetch from articles:cat:{cat}. Also fetch articles:trending for diversity.",
      "5. **Deduplicate**: Group by content_hash. Keep highest-scored article for each unique story.",
      "6. **Score Each Article**: score = 3.0\u00d7publisher_match + 2.0\u00d7category_match + 1.5\u00d7recency + 1.0\u00d7engagement + 0.5\u00d7popularity",
      "7. **Sort by Score**: Descending order. O(n log n) on ~500 articles = ~5ms",
      "8. **Diversify**: Ensure no more than 3 consecutive articles from same publisher",
      "9. **Paginate**: Extract page \u00d7 pageSize to (page+1) \u00d7 pageSize",
      "10. **Cache & Return**: Store in L1 with 60s TTL, return hydrated articles"
    ],
    "why_decimal": "Not applicable for this system design problem - no currency calculations."
  },
  "solution_python_lines": [
    "\"\"\"",
    "News Feed Aggregator System",
    "===========================",
    "",
    "A scalable news aggregation system with personalized feed generation.",
    "Designed for 100K QPS reads with <200ms p99 latency.",
    "",
    "Architecture: Hybrid fan-out with multi-level caching",
    "\"\"\"",
    "",
    "from typing import Dict, List, Optional, Set",
    "from dataclasses import dataclass, field",
    "from datetime import datetime, timedelta",
    "from collections import defaultdict",
    "from abc import ABC, abstractmethod",
    "import heapq",
    "import hashlib",
    "import time",
    "",
    "",
    "# =============================================================================",
    "# DOMAIN ENTITIES",
    "# =============================================================================",
    "",
    "@dataclass",
    "class Publisher:",
    "    \"\"\"Represents a news publisher (e.g., NY Times, TechCrunch).\"\"\"",
    "    publisher_id: str",
    "    name: str",
    "    rss_url: str",
    "    categories: List[str]",
    "    created_at: datetime = field(default_factory=datetime.now)",
    "    last_poll_time: Optional[datetime] = None",
    "    is_active: bool = True",
    "",
    "",
    "@dataclass",
    "class Article:",
    "    \"\"\"Represents a news article.\"\"\"",
    "    article_id: str",
    "    publisher_id: str",
    "    title: str",
    "    content_url: str",
    "    thumbnail_url: Optional[str]",
    "    categories: List[str]",
    "    published_at: datetime",
    "    created_at: datetime = field(default_factory=datetime.now)",
    "    like_count: int = 0",
    "    view_count: int = 0",
    "    content_hash: str = ''",
    "    ",
    "    def __post_init__(self):",
    "        \"\"\"Generate content hash for deduplication if not provided.\"\"\"",
    "        if not self.content_hash:",
    "            self.content_hash = hashlib.md5(",
    "                self.title.lower().encode()",
    "            ).hexdigest()[:16]",
    "",
    "",
    "@dataclass",
    "class User:",
    "    \"\"\"Represents a user with their preferences and engagement history.\"\"\"",
    "    user_id: str",
    "    interests: Set[str] = field(default_factory=set)",
    "    followed_publishers: Set[str] = field(default_factory=set)",
    "    engagement_history: Dict[str, int] = field(default_factory=dict)  # publisher_id -> like_count",
    "    created_at: datetime = field(default_factory=datetime.now)",
    "    last_active: datetime = field(default_factory=datetime.now)",
    "",
    "",
    "@dataclass",
    "class ScoredArticle:",
    "    \"\"\"Article with computed relevance score.\"\"\"",
    "    article: Article",
    "    score: float",
    "    ",
    "    def __lt__(self, other: 'ScoredArticle') -> bool:",
    "        \"\"\"For heap operations - higher score = higher priority.\"\"\"",
    "        return self.score > other.score  # Reverse for max-heap behavior",
    "",
    "",
    "# =============================================================================",
    "# CACHE INTERFACE (Abstraction for Redis)",
    "# =============================================================================",
    "",
    "class CacheService(ABC):",
    "    \"\"\"Abstract cache service - in production, this wraps Redis.\"\"\"",
    "    ",
    "    @abstractmethod",
    "    def get(self, key: str) -> Optional[any]:",
    "        pass",
    "    ",
    "    @abstractmethod",
    "    def set(self, key: str, value: any, ttl_seconds: int) -> None:",
    "        pass",
    "    ",
    "    @abstractmethod",
    "    def delete(self, key: str) -> None:",
    "        pass",
    "",
    "",
    "class InMemoryCache(CacheService):",
    "    \"\"\"In-memory cache for demonstration (use Redis in production).\"\"\"",
    "    ",
    "    def __init__(self):",
    "        self._cache: Dict[str, tuple] = {}  # key -> (value, expiry_time)",
    "    ",
    "    def get(self, key: str) -> Optional[any]:",
    "        if key in self._cache:",
    "            value, expiry = self._cache[key]",
    "            if time.time() < expiry:",
    "                return value",
    "            del self._cache[key]",
    "        return None",
    "    ",
    "    def set(self, key: str, value: any, ttl_seconds: int) -> None:",
    "        self._cache[key] = (value, time.time() + ttl_seconds)",
    "    ",
    "    def delete(self, key: str) -> None:",
    "        self._cache.pop(key, None)",
    "",
    "",
    "# =============================================================================",
    "# RANKING SERVICE",
    "# =============================================================================",
    "",
    "class RankingService:",
    "    \"\"\"",
    "    Scores articles based on user preferences and article attributes.",
    "    ",
    "    Scoring Weights:",
    "    - Publisher match: 3.0 (highest - user explicitly followed)",
    "    - Category match: 2.0 (user interested in this topic)",
    "    - Recency: 1.5 (newer articles preferred)",
    "    - Engagement boost: 1.0 (user has history with publisher)",
    "    - Popularity: 0.5 (global engagement signals)",
    "    \"\"\"",
    "    ",
    "    # Scoring weights",
    "    WEIGHT_PUBLISHER_MATCH = 3.0",
    "    WEIGHT_CATEGORY_MATCH = 2.0",
    "    WEIGHT_RECENCY = 1.5",
    "    WEIGHT_ENGAGEMENT = 1.0",
    "    WEIGHT_POPULARITY = 0.5",
    "    ",
    "    # Recency decay (articles older than this get 0 recency score)",
    "    RECENCY_WINDOW_HOURS = 48",
    "    ",
    "    def score_article(self, article: Article, user: User) -> float:",
    "        \"\"\"",
    "        Calculate personalized relevance score for an article.",
    "        ",
    "        Args:",
    "            article: The article to score",
    "            user: The user for personalization",
    "            ",
    "        Returns:",
    "            float: Relevance score (higher = more relevant)",
    "            ",
    "        Example:",
    "            >>> ranker = RankingService()",
    "            >>> user = User('u1', interests={'Tech'}, followed_publishers={'tc'})",
    "            >>> article = Article('a1', 'tc', 'AI News', '...', None, ['Tech'], datetime.now())",
    "            >>> score = ranker.score_article(article, user)",
    "            >>> score > 5.0  # High score due to publisher + category match",
    "            True",
    "        \"\"\"",
    "        score = 0.0",
    "        ",
    "        # Factor 1: Publisher match (did user follow this publisher?)",
    "        if article.publisher_id in user.followed_publishers:",
    "            score += self.WEIGHT_PUBLISHER_MATCH",
    "        ",
    "        # Factor 2: Category match (is user interested in article's categories?)",
    "        category_overlap = len(set(article.categories) & user.interests)",
    "        if category_overlap > 0:",
    "            score += self.WEIGHT_CATEGORY_MATCH * min(category_overlap, 2)  # Cap at 2",
    "        ",
    "        # Factor 3: Recency (how new is the article?)",
    "        hours_old = (datetime.now() - article.published_at).total_seconds() / 3600",
    "        if hours_old < self.RECENCY_WINDOW_HOURS:",
    "            recency_factor = 1.0 - (hours_old / self.RECENCY_WINDOW_HOURS)",
    "            score += self.WEIGHT_RECENCY * recency_factor",
    "        ",
    "        # Factor 4: Engagement boost (user's history with this publisher)",
    "        engagement_count = user.engagement_history.get(article.publisher_id, 0)",
    "        if engagement_count > 0:",
    "            engagement_factor = min(engagement_count / 10, 1.0)  # Cap at 10 interactions",
    "            score += self.WEIGHT_ENGAGEMENT * engagement_factor",
    "        ",
    "        # Factor 5: Global popularity",
    "        total_engagement = article.like_count + article.view_count",
    "        if total_engagement > 0:",
    "            popularity_factor = min(total_engagement / 10000, 1.0)  # Cap at 10K",
    "            score += self.WEIGHT_POPULARITY * popularity_factor",
    "        ",
    "        return round(score, 3)",
    "",
    "",
    "# =============================================================================",
    "# FEED GENERATOR SERVICE",
    "# =============================================================================",
    "",
    "class FeedGeneratorService:",
    "    \"\"\"",
    "    Generates personalized news feeds for users.",
    "    ",
    "    Uses hybrid approach:",
    "    1. Check user-specific feed cache (L1)",
    "    2. Gather candidates from component caches (L2)",
    "    3. Score and rank candidates",
    "    4. Deduplicate, diversify, paginate",
    "    5. Cache and return",
    "    \"\"\"",
    "    ",
    "    # Cache TTLs",
    "    USER_FEED_CACHE_TTL = 60  # 1 minute",
    "    PUBLISHER_CACHE_TTL = 300  # 5 minutes",
    "    CATEGORY_CACHE_TTL = 300  # 5 minutes",
    "    TRENDING_CACHE_TTL = 30  # 30 seconds",
    "    ",
    "    # Feed generation limits",
    "    MAX_CANDIDATES_PER_SOURCE = 50",
    "    MAX_CONSECUTIVE_SAME_PUBLISHER = 3",
    "    DEFAULT_PAGE_SIZE = 20",
    "    ",
    "    def __init__(self, cache: CacheService, ranker: RankingService):",
    "        self._cache = cache",
    "        self._ranker = ranker",
    "    ",
    "    def generate_feed(",
    "        self,",
    "        user: User,",
    "        page: int,",
    "        page_size: int,",
    "        publisher_articles: Dict[str, List[Article]],",
    "        category_articles: Dict[str, List[Article]],",
    "        trending_articles: List[Article]",
    "    ) -> List[Article]:",
    "        \"\"\"",
    "        Generate personalized feed for a user.",
    "        ",
    "        Args:",
    "            user: User requesting the feed",
    "            page: Page number (0-indexed)",
    "            page_size: Articles per page",
    "            publisher_articles: Cached articles by publisher",
    "            category_articles: Cached articles by category",
    "            trending_articles: Global trending articles (for cold start)",
    "            ",
    "        Returns:",
    "            List of articles for the requested page",
    "        \"\"\"",
    "        # Step 1: Check L1 cache",
    "        cache_key = f'feed:{user.user_id}:page_{page}'",
    "        cached = self._cache.get(cache_key)",
    "        if cached:",
    "            return cached",
    "        ",
    "        # Step 2: Gather candidates from all sources",
    "        candidates = self._gather_candidates(",
    "            user, publisher_articles, category_articles, trending_articles",
    "        )",
    "        ",
    "        # Step 3: Deduplicate by content hash",
    "        candidates = self._deduplicate(candidates)",
    "        ",
    "        # Step 4: Score and rank",
    "        scored = [",
    "            ScoredArticle(article, self._ranker.score_article(article, user))",
    "            for article in candidates",
    "        ]",
    "        scored.sort(key=lambda x: x.score, reverse=True)",
    "        ",
    "        # Step 5: Diversify (prevent single publisher domination)",
    "        diversified = self._diversify([s.article for s in scored])",
    "        ",
    "        # Step 6: Paginate",
    "        start = page * page_size",
    "        end = start + page_size",
    "        result = diversified[start:end]",
    "        ",
    "        # Step 7: Cache and return",
    "        self._cache.set(cache_key, result, self.USER_FEED_CACHE_TTL)",
    "        ",
    "        return result",
    "    ",
    "    def _gather_candidates(",
    "        self,",
    "        user: User,",
    "        publisher_articles: Dict[str, List[Article]],",
    "        category_articles: Dict[str, List[Article]],",
    "        trending_articles: List[Article]",
    "    ) -> List[Article]:",
    "        \"\"\"Gather candidate articles from all sources.\"\"\"",
    "        candidates = []",
    "        seen_ids = set()",
    "        ",
    "        # Source 1: Articles from followed publishers",
    "        for pub_id in user.followed_publishers:",
    "            if pub_id in publisher_articles:",
    "                for article in publisher_articles[pub_id][:self.MAX_CANDIDATES_PER_SOURCE]:",
    "                    if article.article_id not in seen_ids:",
    "                        candidates.append(article)",
    "                        seen_ids.add(article.article_id)",
    "        ",
    "        # Source 2: Articles from interested categories",
    "        for category in user.interests:",
    "            if category in category_articles:",
    "                for article in category_articles[category][:self.MAX_CANDIDATES_PER_SOURCE]:",
    "                    if article.article_id not in seen_ids:",
    "                        candidates.append(article)",
    "                        seen_ids.add(article.article_id)",
    "        ",
    "        # Source 3: Trending articles (for cold start / diversity)",
    "        for article in trending_articles[:20]:",
    "            if article.article_id not in seen_ids:",
    "                candidates.append(article)",
    "                seen_ids.add(article.article_id)",
    "        ",
    "        return candidates",
    "    ",
    "    def _deduplicate(self, articles: List[Article]) -> List[Article]:",
    "        \"\"\"Remove duplicate stories (same story from different publishers).\"\"\"",
    "        seen_hashes: Dict[str, Article] = {}",
    "        ",
    "        for article in articles:",
    "            content_hash = article.content_hash",
    "            if content_hash not in seen_hashes:",
    "                seen_hashes[content_hash] = article",
    "            else:",
    "                # Keep article with higher engagement",
    "                existing = seen_hashes[content_hash]",
    "                if (article.like_count + article.view_count) > \\",
    "                   (existing.like_count + existing.view_count):",
    "                    seen_hashes[content_hash] = article",
    "        ",
    "        return list(seen_hashes.values())",
    "    ",
    "    def _diversify(self, articles: List[Article]) -> List[Article]:",
    "        \"\"\"Ensure no more than N consecutive articles from same publisher.\"\"\"",
    "        if not articles:",
    "            return []",
    "        ",
    "        result = []",
    "        consecutive_count = 0",
    "        last_publisher = None",
    "        deferred = []",
    "        ",
    "        for article in articles:",
    "            if article.publisher_id == last_publisher:",
    "                consecutive_count += 1",
    "                if consecutive_count >= self.MAX_CONSECUTIVE_SAME_PUBLISHER:",
    "                    deferred.append(article)",
    "                    continue",
    "            else:",
    "                consecutive_count = 1",
    "                last_publisher = article.publisher_id",
    "            ",
    "            result.append(article)",
    "        ",
    "        # Add deferred articles at the end",
    "        result.extend(deferred)",
    "        return result",
    "",
    "",
    "# =============================================================================",
    "# MAIN AGGREGATOR SYSTEM (Facade)",
    "# =============================================================================",
    "",
    "class NewsAggregator:",
    "    \"\"\"",
    "    Main facade for the News Aggregator System.",
    "    ",
    "    This class provides the public API for:",
    "    - Publisher registration and management",
    "    - User preference management (follows, interests)",
    "    - Personalized feed generation",
    "    - User engagement tracking",
    "    ",
    "    Example usage:",
    "        >>> aggregator = NewsAggregator()",
    "        >>> aggregator.register_publisher('nyt', 'NY Times', '...', ['Politics'])",
    "        >>> aggregator.follow_publisher('user1', 'nyt')",
    "        >>> aggregator.set_user_interests('user1', ['Politics', 'Tech'])",
    "        >>> feed = aggregator.get_user_feed('user1', 0, 10)",
    "    \"\"\"",
    "    ",
    "    def __init__(self):",
    "        \"\"\"Initialize the news aggregator with all required services.\"\"\"",
    "        self._cache = InMemoryCache()",
    "        self._ranker = RankingService()",
    "        self._feed_generator = FeedGeneratorService(self._cache, self._ranker)",
    "        ",
    "        # Storage (in production, these would be database tables)",
    "        self._publishers: Dict[str, Publisher] = {}",
    "        self._users: Dict[str, User] = {}",
    "        self._articles: Dict[str, Article] = {}",
    "        ",
    "        # Component caches (simulating Redis)",
    "        self._publisher_articles: Dict[str, List[Article]] = defaultdict(list)",
    "        self._category_articles: Dict[str, List[Article]] = defaultdict(list)",
    "        self._trending_articles: List[Article] = []",
    "    ",
    "    # =========================================================================",
    "    # PUBLISHER MANAGEMENT",
    "    # =========================================================================",
    "    ",
    "    def register_publisher(",
    "        self,",
    "        publisher_id: str,",
    "        name: str,",
    "        rss_url: str,",
    "        categories: List[str]",
    "    ) -> None:",
    "        \"\"\"",
    "        Register a new news publisher in the system.",
    "        ",
    "        Args:",
    "            publisher_id: Unique identifier for the publisher",
    "            name: Display name of the publisher",
    "            rss_url: RSS feed or API endpoint URL",
    "            categories: Content categories this publisher covers",
    "            ",
    "        Raises:",
    "            ValueError: If publisher_id already exists",
    "        \"\"\"",
    "        if publisher_id in self._publishers:",
    "            raise ValueError(f'Publisher {publisher_id} already exists')",
    "        ",
    "        publisher = Publisher(",
    "            publisher_id=publisher_id,",
    "            name=name,",
    "            rss_url=rss_url,",
    "            categories=categories",
    "        )",
    "        self._publishers[publisher_id] = publisher",
    "        print(f'[INFO] Registered publisher: {name} ({publisher_id})')",
    "    ",
    "    def ingest_article(self, article: Article) -> None:",
    "        \"\"\"",
    "        Ingest a new article into the system.",
    "        ",
    "        This simulates the crawler worker's job of:",
    "        1. Storing the article",
    "        2. Updating publisher cache",
    "        3. Updating category caches",
    "        4. Updating trending cache",
    "        ",
    "        Args:",
    "            article: The article to ingest",
    "        \"\"\"",
    "        # Store article",
    "        self._articles[article.article_id] = article",
    "        ",
    "        # Update publisher cache",
    "        self._publisher_articles[article.publisher_id].insert(0, article)",
    "        self._publisher_articles[article.publisher_id] = \\",
    "            self._publisher_articles[article.publisher_id][:100]  # Keep latest 100",
    "        ",
    "        # Update category caches",
    "        for category in article.categories:",
    "            self._category_articles[category].insert(0, article)",
    "            self._category_articles[category] = \\",
    "                self._category_articles[category][:100]",
    "        ",
    "        # Update trending (simplified - in production use sorted set)",
    "        self._trending_articles.insert(0, article)",
    "        self._trending_articles = self._trending_articles[:50]",
    "    ",
    "    # =========================================================================",
    "    # USER MANAGEMENT",
    "    # =========================================================================",
    "    ",
    "    def _get_or_create_user(self, user_id: str) -> User:",
    "        \"\"\"Get existing user or create new one.\"\"\"",
    "        if user_id not in self._users:",
    "            self._users[user_id] = User(user_id=user_id)",
    "        return self._users[user_id]",
    "    ",
    "    def follow_publisher(self, user_id: str, publisher_id: str) -> None:",
    "        \"\"\"",
    "        Add a publisher to user's followed list.",
    "        ",
    "        Args:",
    "            user_id: User following the publisher",
    "            publisher_id: Publisher being followed",
    "            ",
    "        Raises:",
    "            ValueError: If publisher doesn't exist",
    "        \"\"\"",
    "        if publisher_id not in self._publishers:",
    "            raise ValueError(f'Publisher {publisher_id} not found')",
    "        ",
    "        user = self._get_or_create_user(user_id)",
    "        user.followed_publishers.add(publisher_id)",
    "        ",
    "        # Invalidate user's feed cache",
    "        self._invalidate_user_feed_cache(user_id)",
    "        print(f'[INFO] User {user_id} now follows {publisher_id}')",
    "    ",
    "    def unfollow_publisher(self, user_id: str, publisher_id: str) -> None:",
    "        \"\"\"Remove a publisher from user's followed list.\"\"\"",
    "        user = self._get_or_create_user(user_id)",
    "        user.followed_publishers.discard(publisher_id)",
    "        self._invalidate_user_feed_cache(user_id)",
    "    ",
    "    def set_user_interests(self, user_id: str, categories: List[str]) -> None:",
    "        \"\"\"",
    "        Set user's interest categories.",
    "        ",
    "        Args:",
    "            user_id: User setting interests",
    "            categories: Interest categories (e.g., ['Tech', 'Sports'])",
    "        \"\"\"",
    "        user = self._get_or_create_user(user_id)",
    "        user.interests = set(categories)",
    "        self._invalidate_user_feed_cache(user_id)",
    "        print(f'[INFO] User {user_id} interests set to: {categories}')",
    "    ",
    "    def like_article(self, user_id: str, article_id: str) -> None:",
    "        \"\"\"",
    "        Record user engagement (like) for personalization.",
    "        ",
    "        Args:",
    "            user_id: User liking the article",
    "            article_id: Article being liked",
    "        \"\"\"",
    "        if article_id not in self._articles:",
    "            raise ValueError(f'Article {article_id} not found')",
    "        ",
    "        article = self._articles[article_id]",
    "        user = self._get_or_create_user(user_id)",
    "        ",
    "        # Increment article's like count",
    "        article.like_count += 1",
    "        ",
    "        # Track user's engagement with publisher",
    "        pub_id = article.publisher_id",
    "        user.engagement_history[pub_id] = user.engagement_history.get(pub_id, 0) + 1",
    "        ",
    "        # Invalidate feed cache (engagement affects ranking)",
    "        self._invalidate_user_feed_cache(user_id)",
    "    ",
    "    def _invalidate_user_feed_cache(self, user_id: str) -> None:",
    "        \"\"\"Invalidate all cached feed pages for a user.\"\"\"",
    "        for page in range(10):  # Invalidate first 10 pages",
    "            self._cache.delete(f'feed:{user_id}:page_{page}')",
    "    ",
    "    # =========================================================================",
    "    # FEED GENERATION",
    "    # =========================================================================",
    "    ",
    "    def get_user_feed(",
    "        self,",
    "        user_id: str,",
    "        page: int = 0,",
    "        page_size: int = 20",
    "    ) -> List[Article]:",
    "        \"\"\"",
    "        Generate personalized feed for a user.",
    "        ",
    "        Args:",
    "            user_id: User requesting the feed",
    "            page: Page number (0-indexed)",
    "            page_size: Number of articles per page",
    "            ",
    "        Returns:",
    "            List of personalized articles for the requested page",
    "            ",
    "        Example:",
    "            >>> aggregator = NewsAggregator()",
    "            >>> # ... setup publishers and articles ...",
    "            >>> feed = aggregator.get_user_feed('user1', 0, 10)",
    "            >>> len(feed) <= 10",
    "            True",
    "        \"\"\"",
    "        user = self._get_or_create_user(user_id)",
    "        ",
    "        # Handle cold start: new user with no preferences",
    "        if not user.followed_publishers and not user.interests:",
    "            print(f'[INFO] Cold start for user {user_id} - showing trending')",
    "            return self._trending_articles[:page_size]",
    "        ",
    "        return self._feed_generator.generate_feed(",
    "            user=user,",
    "            page=page,",
    "            page_size=page_size,",
    "            publisher_articles=dict(self._publisher_articles),",
    "            category_articles=dict(self._category_articles),",
    "            trending_articles=self._trending_articles",
    "        )",
    "",
    "",
    "# =============================================================================",
    "# DEMO / TEST",
    "# =============================================================================",
    "",
    "def main():",
    "    \"\"\"Demonstrate the News Aggregator System.\"\"\"",
    "    print('=' * 70)",
    "    print('NEWS FEED AGGREGATOR SYSTEM - DEMO')",
    "    print('=' * 70)",
    "    print()",
    "    ",
    "    # Initialize system",
    "    aggregator = NewsAggregator()",
    "    ",
    "    # Step 1: Register publishers",
    "    print('STEP 1: Register Publishers')",
    "    print('-' * 40)",
    "    aggregator.register_publisher('nyt', 'NY Times', 'https://nyt.com/rss', ['Politics', 'World'])",
    "    aggregator.register_publisher('tc', 'TechCrunch', 'https://tc.com/rss', ['Technology', 'Startups'])",
    "    aggregator.register_publisher('espn', 'ESPN', 'https://espn.com/rss', ['Sports'])",
    "    print()",
    "    ",
    "    # Step 2: Ingest articles (simulating crawler)",
    "    print('STEP 2: Ingest Articles')",
    "    print('-' * 40)",
    "    articles = [",
    "        Article('a1', 'tc', 'Apple Announces New AI Features', 'https://...', None, ",
    "                ['Technology'], datetime.now() - timedelta(hours=1), like_count=1500),",
    "        Article('a2', 'tc', 'Startup Raises $100M', 'https://...', None,",
    "                ['Startups', 'Technology'], datetime.now() - timedelta(hours=2), like_count=800),",
    "        Article('a3', 'nyt', 'Election Updates', 'https://...', None,",
    "                ['Politics'], datetime.now() - timedelta(hours=0.5), like_count=3000),",
    "        Article('a4', 'nyt', 'Global Climate Summit', 'https://...', None,",
    "                ['World', 'Politics'], datetime.now() - timedelta(hours=3), like_count=500),",
    "        Article('a5', 'espn', 'Lakers Win Championship', 'https://...', None,",
    "                ['Sports'], datetime.now() - timedelta(hours=1), like_count=5000),",
    "        Article('a6', 'espn', 'NFL Draft Predictions', 'https://...', None,",
    "                ['Sports'], datetime.now() - timedelta(hours=4), like_count=1200),",
    "    ]",
    "    ",
    "    for article in articles:",
    "        aggregator.ingest_article(article)",
    "        print(f'  Ingested: {article.title[:40]}...')",
    "    print()",
    "    ",
    "    # Step 3: Setup user preferences",
    "    print('STEP 3: Setup User Preferences')",
    "    print('-' * 40)",
    "    aggregator.follow_publisher('user1', 'tc')",
    "    aggregator.set_user_interests('user1', ['Technology'])",
    "    print()",
    "    ",
    "    # Step 4: Generate feed for user",
    "    print('STEP 4: Generate Feed for user1')",
    "    print('-' * 40)",
    "    feed = aggregator.get_user_feed('user1', page=0, page_size=10)",
    "    print(f'Feed contains {len(feed)} articles:')",
    "    for i, article in enumerate(feed, 1):",
    "        print(f'  {i}. [{article.publisher_id.upper()}] {article.title}')",
    "    print()",
    "    ",
    "    # Step 5: Test cold start user",
    "    print('STEP 5: Cold Start User (no preferences)')",
    "    print('-' * 40)",
    "    cold_feed = aggregator.get_user_feed('new_user', page=0, page_size=5)",
    "    print(f'Feed for new user (trending): {len(cold_feed)} articles')",
    "    for i, article in enumerate(cold_feed, 1):",
    "        print(f'  {i}. [{article.publisher_id.upper()}] {article.title}')",
    "    print()",
    "    ",
    "    # Step 6: Test engagement impact",
    "    print('STEP 6: Engagement Impact')",
    "    print('-' * 40)",
    "    aggregator.follow_publisher('user2', 'tc')",
    "    aggregator.follow_publisher('user2', 'espn')",
    "    aggregator.set_user_interests('user2', ['Technology', 'Sports'])",
    "    ",
    "    # User2 likes ESPN articles heavily",
    "    for _ in range(5):",
    "        aggregator.like_article('user2', 'a5')",
    "    ",
    "    feed2 = aggregator.get_user_feed('user2', page=0, page_size=6)",
    "    print('Feed for user2 (prefers ESPN based on engagement):')",
    "    for i, article in enumerate(feed2, 1):",
    "        print(f'  {i}. [{article.publisher_id.upper()}] {article.title}')",
    "    ",
    "    print()",
    "    print('=' * 70)",
    "    print('DEMO COMPLETE')",
    "    print('=' * 70)",
    "",
    "",
    "if __name__ == '__main__':",
    "    main()"
  ],
  "solution_java_lines": [
    "import java.util.*;",
    "import java.time.*;",
    "import java.util.concurrent.*;",
    "import java.security.*;",
    "import java.nio.charset.*;",
    "",
    "/**",
    " * News Feed Aggregator System",
    " * ",
    " * A scalable news aggregation system with personalized feed generation.",
    " * Designed for 100K QPS reads with <200ms p99 latency.",
    " */",
    "public class NewsAggregatorSystem {",
    "",
    "    // =======================================================================",
    "    // DOMAIN ENTITIES",
    "    // =======================================================================",
    "",
    "    public static class Publisher {",
    "        public final String publisherId;",
    "        public final String name;",
    "        public final String rssUrl;",
    "        public final List<String> categories;",
    "        public final Instant createdAt;",
    "        public Instant lastPollTime;",
    "        public boolean isActive;",
    "",
    "        public Publisher(String publisherId, String name, String rssUrl, List<String> categories) {",
    "            this.publisherId = publisherId;",
    "            this.name = name;",
    "            this.rssUrl = rssUrl;",
    "            this.categories = new ArrayList<>(categories);",
    "            this.createdAt = Instant.now();",
    "            this.isActive = true;",
    "        }",
    "    }",
    "",
    "    public static class Article {",
    "        public final String articleId;",
    "        public final String publisherId;",
    "        public final String title;",
    "        public final String contentUrl;",
    "        public final String thumbnailUrl;",
    "        public final List<String> categories;",
    "        public final Instant publishedAt;",
    "        public final Instant createdAt;",
    "        public int likeCount;",
    "        public int viewCount;",
    "        public final String contentHash;",
    "",
    "        public Article(String articleId, String publisherId, String title,",
    "                      String contentUrl, String thumbnailUrl, List<String> categories,",
    "                      Instant publishedAt, int likeCount, int viewCount) {",
    "            this.articleId = articleId;",
    "            this.publisherId = publisherId;",
    "            this.title = title;",
    "            this.contentUrl = contentUrl;",
    "            this.thumbnailUrl = thumbnailUrl;",
    "            this.categories = new ArrayList<>(categories);",
    "            this.publishedAt = publishedAt;",
    "            this.createdAt = Instant.now();",
    "            this.likeCount = likeCount;",
    "            this.viewCount = viewCount;",
    "            this.contentHash = computeHash(title);",
    "        }",
    "",
    "        private String computeHash(String title) {",
    "            try {",
    "                MessageDigest md = MessageDigest.getInstance(\"MD5\");",
    "                byte[] hash = md.digest(title.toLowerCase().getBytes(StandardCharsets.UTF_8));",
    "                StringBuilder sb = new StringBuilder();",
    "                for (int i = 0; i < 8; i++) {",
    "                    sb.append(String.format(\"%02x\", hash[i]));",
    "                }",
    "                return sb.toString();",
    "            } catch (NoSuchAlgorithmException e) {",
    "                return title.substring(0, Math.min(16, title.length()));",
    "            }",
    "        }",
    "",
    "        @Override",
    "        public String toString() {",
    "            return String.format(\"[%s] %s\", publisherId.toUpperCase(), title);",
    "        }",
    "    }",
    "",
    "    public static class User {",
    "        public final String userId;",
    "        public final Set<String> interests;",
    "        public final Set<String> followedPublishers;",
    "        public final Map<String, Integer> engagementHistory;",
    "        public final Instant createdAt;",
    "        public Instant lastActive;",
    "",
    "        public User(String userId) {",
    "            this.userId = userId;",
    "            this.interests = new HashSet<>();",
    "            this.followedPublishers = new HashSet<>();",
    "            this.engagementHistory = new HashMap<>();",
    "            this.createdAt = Instant.now();",
    "            this.lastActive = Instant.now();",
    "        }",
    "    }",
    "",
    "    public static class ScoredArticle implements Comparable<ScoredArticle> {",
    "        public final Article article;",
    "        public final double score;",
    "",
    "        public ScoredArticle(Article article, double score) {",
    "            this.article = article;",
    "            this.score = score;",
    "        }",
    "",
    "        @Override",
    "        public int compareTo(ScoredArticle other) {",
    "            return Double.compare(other.score, this.score); // Descending",
    "        }",
    "    }",
    "",
    "    // =======================================================================",
    "    // CACHE SERVICE",
    "    // =======================================================================",
    "",
    "    public static class InMemoryCache {",
    "        private final Map<String, CacheEntry> cache = new ConcurrentHashMap<>();",
    "",
    "        private static class CacheEntry {",
    "            final Object value;",
    "            final long expiryTime;",
    "",
    "            CacheEntry(Object value, long ttlSeconds) {",
    "                this.value = value;",
    "                this.expiryTime = System.currentTimeMillis() + (ttlSeconds * 1000);",
    "            }",
    "",
    "            boolean isExpired() {",
    "                return System.currentTimeMillis() > expiryTime;",
    "            }",
    "        }",
    "",
    "        @SuppressWarnings(\"unchecked\")",
    "        public <T> T get(String key) {",
    "            CacheEntry entry = cache.get(key);",
    "            if (entry != null && !entry.isExpired()) {",
    "                return (T) entry.value;",
    "            }",
    "            cache.remove(key);",
    "            return null;",
    "        }",
    "",
    "        public void set(String key, Object value, long ttlSeconds) {",
    "            cache.put(key, new CacheEntry(value, ttlSeconds));",
    "        }",
    "",
    "        public void delete(String key) {",
    "            cache.remove(key);",
    "        }",
    "    }",
    "",
    "    // =======================================================================",
    "    // RANKING SERVICE",
    "    // =======================================================================",
    "",
    "    public static class RankingService {",
    "        private static final double WEIGHT_PUBLISHER_MATCH = 3.0;",
    "        private static final double WEIGHT_CATEGORY_MATCH = 2.0;",
    "        private static final double WEIGHT_RECENCY = 1.5;",
    "        private static final double WEIGHT_ENGAGEMENT = 1.0;",
    "        private static final double WEIGHT_POPULARITY = 0.5;",
    "        private static final int RECENCY_WINDOW_HOURS = 48;",
    "",
    "        public double scoreArticle(Article article, User user) {",
    "            double score = 0.0;",
    "",
    "            // Factor 1: Publisher match",
    "            if (user.followedPublishers.contains(article.publisherId)) {",
    "                score += WEIGHT_PUBLISHER_MATCH;",
    "            }",
    "",
    "            // Factor 2: Category match",
    "            long categoryOverlap = article.categories.stream()",
    "                .filter(user.interests::contains).count();",
    "            if (categoryOverlap > 0) {",
    "                score += WEIGHT_CATEGORY_MATCH * Math.min(categoryOverlap, 2);",
    "            }",
    "",
    "            // Factor 3: Recency",
    "            long hoursOld = Duration.between(article.publishedAt, Instant.now()).toHours();",
    "            if (hoursOld < RECENCY_WINDOW_HOURS) {",
    "                double recencyFactor = 1.0 - ((double) hoursOld / RECENCY_WINDOW_HOURS);",
    "                score += WEIGHT_RECENCY * recencyFactor;",
    "            }",
    "",
    "            // Factor 4: Engagement boost",
    "            int engagementCount = user.engagementHistory.getOrDefault(article.publisherId, 0);",
    "            if (engagementCount > 0) {",
    "                double engagementFactor = Math.min(engagementCount / 10.0, 1.0);",
    "                score += WEIGHT_ENGAGEMENT * engagementFactor;",
    "            }",
    "",
    "            // Factor 5: Global popularity",
    "            int totalEngagement = article.likeCount + article.viewCount;",
    "            if (totalEngagement > 0) {",
    "                double popularityFactor = Math.min(totalEngagement / 10000.0, 1.0);",
    "                score += WEIGHT_POPULARITY * popularityFactor;",
    "            }",
    "",
    "            return Math.round(score * 1000.0) / 1000.0;",
    "        }",
    "    }",
    "",
    "    // =======================================================================",
    "    // MAIN AGGREGATOR SYSTEM",
    "    // =======================================================================",
    "",
    "    public static class NewsAggregator {",
    "        private final InMemoryCache cache;",
    "        private final RankingService ranker;",
    "        ",
    "        private final Map<String, Publisher> publishers = new ConcurrentHashMap<>();",
    "        private final Map<String, User> users = new ConcurrentHashMap<>();",
    "        private final Map<String, Article> articles = new ConcurrentHashMap<>();",
    "        private final Map<String, List<Article>> publisherArticles = new ConcurrentHashMap<>();",
    "        private final Map<String, List<Article>> categoryArticles = new ConcurrentHashMap<>();",
    "        private final List<Article> trendingArticles = Collections.synchronizedList(new ArrayList<>());",
    "",
    "        private static final int USER_FEED_CACHE_TTL = 60;",
    "        private static final int MAX_CANDIDATES_PER_SOURCE = 50;",
    "        private static final int MAX_CONSECUTIVE_SAME_PUBLISHER = 3;",
    "",
    "        public NewsAggregator() {",
    "            this.cache = new InMemoryCache();",
    "            this.ranker = new RankingService();",
    "        }",
    "",
    "        public void registerPublisher(String publisherId, String name, ",
    "                                     String rssUrl, List<String> categories) {",
    "            if (publishers.containsKey(publisherId)) {",
    "                throw new IllegalArgumentException(\"Publisher already exists: \" + publisherId);",
    "            }",
    "            publishers.put(publisherId, new Publisher(publisherId, name, rssUrl, categories));",
    "            publisherArticles.put(publisherId, Collections.synchronizedList(new ArrayList<>()));",
    "            System.out.printf(\"[INFO] Registered publisher: %s (%s)%n\", name, publisherId);",
    "        }",
    "",
    "        public void ingestArticle(Article article) {",
    "            articles.put(article.articleId, article);",
    "            ",
    "            // Update publisher cache",
    "            List<Article> pubArticles = publisherArticles.get(article.publisherId);",
    "            if (pubArticles != null) {",
    "                pubArticles.add(0, article);",
    "                if (pubArticles.size() > 100) {",
    "                    pubArticles.subList(100, pubArticles.size()).clear();",
    "                }",
    "            }",
    "            ",
    "            // Update category caches",
    "            for (String category : article.categories) {",
    "                categoryArticles.computeIfAbsent(category, k -> ",
    "                    Collections.synchronizedList(new ArrayList<>()))",
    "                    .add(0, article);",
    "            }",
    "            ",
    "            // Update trending",
    "            trendingArticles.add(0, article);",
    "            if (trendingArticles.size() > 50) {",
    "                trendingArticles.subList(50, trendingArticles.size()).clear();",
    "            }",
    "        }",
    "",
    "        private User getOrCreateUser(String userId) {",
    "            return users.computeIfAbsent(userId, User::new);",
    "        }",
    "",
    "        public void followPublisher(String userId, String publisherId) {",
    "            if (!publishers.containsKey(publisherId)) {",
    "                throw new IllegalArgumentException(\"Publisher not found: \" + publisherId);",
    "            }",
    "            User user = getOrCreateUser(userId);",
    "            user.followedPublishers.add(publisherId);",
    "            invalidateUserFeedCache(userId);",
    "            System.out.printf(\"[INFO] User %s now follows %s%n\", userId, publisherId);",
    "        }",
    "",
    "        public void setUserInterests(String userId, List<String> categories) {",
    "            User user = getOrCreateUser(userId);",
    "            user.interests.clear();",
    "            user.interests.addAll(categories);",
    "            invalidateUserFeedCache(userId);",
    "            System.out.printf(\"[INFO] User %s interests set to: %s%n\", userId, categories);",
    "        }",
    "",
    "        public void likeArticle(String userId, String articleId) {",
    "            Article article = articles.get(articleId);",
    "            if (article == null) {",
    "                throw new IllegalArgumentException(\"Article not found: \" + articleId);",
    "            }",
    "            User user = getOrCreateUser(userId);",
    "            article.likeCount++;",
    "            user.engagementHistory.merge(article.publisherId, 1, Integer::sum);",
    "            invalidateUserFeedCache(userId);",
    "        }",
    "",
    "        private void invalidateUserFeedCache(String userId) {",
    "            for (int page = 0; page < 10; page++) {",
    "                cache.delete(String.format(\"feed:%s:page_%d\", userId, page));",
    "            }",
    "        }",
    "",
    "        public List<Article> getUserFeed(String userId, int page, int pageSize) {",
    "            User user = getOrCreateUser(userId);",
    "            ",
    "            // Cold start handling",
    "            if (user.followedPublishers.isEmpty() && user.interests.isEmpty()) {",
    "                System.out.printf(\"[INFO] Cold start for user %s - showing trending%n\", userId);",
    "                return trendingArticles.subList(0, Math.min(pageSize, trendingArticles.size()));",
    "            }",
    "            ",
    "            // Check cache",
    "            String cacheKey = String.format(\"feed:%s:page_%d\", userId, page);",
    "            List<Article> cached = cache.get(cacheKey);",
    "            if (cached != null) {",
    "                return cached;",
    "            }",
    "            ",
    "            // Gather candidates",
    "            List<Article> candidates = gatherCandidates(user);",
    "            ",
    "            // Deduplicate",
    "            candidates = deduplicate(candidates);",
    "            ",
    "            // Score and rank",
    "            List<ScoredArticle> scored = new ArrayList<>();",
    "            for (Article article : candidates) {",
    "                scored.add(new ScoredArticle(article, ranker.scoreArticle(article, user)));",
    "            }",
    "            Collections.sort(scored);",
    "            ",
    "            // Extract articles",
    "            List<Article> ranked = new ArrayList<>();",
    "            for (ScoredArticle sa : scored) {",
    "                ranked.add(sa.article);",
    "            }",
    "            ",
    "            // Diversify",
    "            ranked = diversify(ranked);",
    "            ",
    "            // Paginate",
    "            int start = page * pageSize;",
    "            int end = Math.min(start + pageSize, ranked.size());",
    "            List<Article> result = start < ranked.size() ? ",
    "                new ArrayList<>(ranked.subList(start, end)) : new ArrayList<>();",
    "            ",
    "            // Cache and return",
    "            cache.set(cacheKey, result, USER_FEED_CACHE_TTL);",
    "            return result;",
    "        }",
    "",
    "        private List<Article> gatherCandidates(User user) {",
    "            Set<String> seenIds = new HashSet<>();",
    "            List<Article> candidates = new ArrayList<>();",
    "            ",
    "            // From followed publishers",
    "            for (String pubId : user.followedPublishers) {",
    "                List<Article> pubArts = publisherArticles.get(pubId);",
    "                if (pubArts != null) {",
    "                    for (int i = 0; i < Math.min(MAX_CANDIDATES_PER_SOURCE, pubArts.size()); i++) {",
    "                        Article art = pubArts.get(i);",
    "                        if (!seenIds.contains(art.articleId)) {",
    "                            candidates.add(art);",
    "                            seenIds.add(art.articleId);",
    "                        }",
    "                    }",
    "                }",
    "            }",
    "            ",
    "            // From interested categories",
    "            for (String cat : user.interests) {",
    "                List<Article> catArts = categoryArticles.get(cat);",
    "                if (catArts != null) {",
    "                    for (int i = 0; i < Math.min(MAX_CANDIDATES_PER_SOURCE, catArts.size()); i++) {",
    "                        Article art = catArts.get(i);",
    "                        if (!seenIds.contains(art.articleId)) {",
    "                            candidates.add(art);",
    "                            seenIds.add(art.articleId);",
    "                        }",
    "                    }",
    "                }",
    "            }",
    "            ",
    "            // Trending for diversity",
    "            for (int i = 0; i < Math.min(20, trendingArticles.size()); i++) {",
    "                Article art = trendingArticles.get(i);",
    "                if (!seenIds.contains(art.articleId)) {",
    "                    candidates.add(art);",
    "                    seenIds.add(art.articleId);",
    "                }",
    "            }",
    "            ",
    "            return candidates;",
    "        }",
    "",
    "        private List<Article> deduplicate(List<Article> articles) {",
    "            Map<String, Article> seen = new LinkedHashMap<>();",
    "            for (Article article : articles) {",
    "                Article existing = seen.get(article.contentHash);",
    "                if (existing == null || ",
    "                    (article.likeCount + article.viewCount) > ",
    "                    (existing.likeCount + existing.viewCount)) {",
    "                    seen.put(article.contentHash, article);",
    "                }",
    "            }",
    "            return new ArrayList<>(seen.values());",
    "        }",
    "",
    "        private List<Article> diversify(List<Article> articles) {",
    "            if (articles.isEmpty()) return articles;",
    "            ",
    "            List<Article> result = new ArrayList<>();",
    "            List<Article> deferred = new ArrayList<>();",
    "            String lastPublisher = null;",
    "            int consecutive = 0;",
    "            ",
    "            for (Article article : articles) {",
    "                if (article.publisherId.equals(lastPublisher)) {",
    "                    consecutive++;",
    "                    if (consecutive >= MAX_CONSECUTIVE_SAME_PUBLISHER) {",
    "                        deferred.add(article);",
    "                        continue;",
    "                    }",
    "                } else {",
    "                    consecutive = 1;",
    "                    lastPublisher = article.publisherId;",
    "                }",
    "                result.add(article);",
    "            }",
    "            result.addAll(deferred);",
    "            return result;",
    "        }",
    "    }",
    "",
    "    // =======================================================================",
    "    // DEMO",
    "    // =======================================================================",
    "",
    "    public static void main(String[] args) {",
    "        System.out.println(\"=\".repeat(70));",
    "        System.out.println(\"NEWS FEED AGGREGATOR SYSTEM - JAVA DEMO\");",
    "        System.out.println(\"=\".repeat(70));",
    "        System.out.println();",
    "",
    "        NewsAggregator aggregator = new NewsAggregator();",
    "",
    "        // Register publishers",
    "        System.out.println(\"STEP 1: Register Publishers\");",
    "        System.out.println(\"-\".repeat(40));",
    "        aggregator.registerPublisher(\"nyt\", \"NY Times\", \"https://nyt.com/rss\", ",
    "            Arrays.asList(\"Politics\", \"World\"));",
    "        aggregator.registerPublisher(\"tc\", \"TechCrunch\", \"https://tc.com/rss\", ",
    "            Arrays.asList(\"Technology\", \"Startups\"));",
    "        aggregator.registerPublisher(\"espn\", \"ESPN\", \"https://espn.com/rss\", ",
    "            Arrays.asList(\"Sports\"));",
    "        System.out.println();",
    "",
    "        // Ingest articles",
    "        System.out.println(\"STEP 2: Ingest Articles\");",
    "        System.out.println(\"-\".repeat(40));",
    "        aggregator.ingestArticle(new Article(\"a1\", \"tc\", \"Apple AI Features\", ",
    "            \"url\", null, Arrays.asList(\"Technology\"), Instant.now().minusSeconds(3600), 1500, 0));",
    "        aggregator.ingestArticle(new Article(\"a2\", \"tc\", \"Startup Funding\",",
    "            \"url\", null, Arrays.asList(\"Startups\"), Instant.now().minusSeconds(7200), 800, 0));",
    "        aggregator.ingestArticle(new Article(\"a3\", \"nyt\", \"Election News\",",
    "            \"url\", null, Arrays.asList(\"Politics\"), Instant.now().minusSeconds(1800), 3000, 0));",
    "        aggregator.ingestArticle(new Article(\"a4\", \"espn\", \"Lakers Win\",",
    "            \"url\", null, Arrays.asList(\"Sports\"), Instant.now().minusSeconds(3600), 5000, 0));",
    "        System.out.println(\"  Ingested 4 articles\");",
    "        System.out.println();",
    "",
    "        // Setup user and get feed",
    "        System.out.println(\"STEP 3: Generate Feed\");",
    "        System.out.println(\"-\".repeat(40));",
    "        aggregator.followPublisher(\"user1\", \"tc\");",
    "        aggregator.setUserInterests(\"user1\", Arrays.asList(\"Technology\"));",
    "        System.out.println();",
    "        ",
    "        List<Article> feed = aggregator.getUserFeed(\"user1\", 0, 10);",
    "        System.out.printf(\"Feed contains %d articles:%n\", feed.size());",
    "        int i = 1;",
    "        for (Article article : feed) {",
    "            System.out.printf(\"  %d. %s%n\", i++, article);",
    "        }",
    "",
    "        System.out.println();",
    "        System.out.println(\"=\".repeat(70));",
    "        System.out.println(\"DEMO COMPLETE\");",
    "        System.out.println(\"=\".repeat(70));",
    "    }",
    "}"
  ],
  "code_walkthrough": [
    {
      "lines": "1-50",
      "section": "Domain Entities",
      "explanation": "We define four core entities: **Publisher** (news source), **Article** (content item with metadata), **User** (with preferences and engagement history), and **ScoredArticle** (wrapper for ranking). The Article class generates a content hash for deduplication."
    },
    {
      "lines": "80-120",
      "section": "Cache Service",
      "explanation": "**InMemoryCache** is an abstraction over Redis. In production, this would be Redis Cluster with sharding. Each entry has a TTL for automatic expiration. This enables the multi-level caching strategy."
    },
    {
      "lines": "125-180",
      "section": "Ranking Service",
      "explanation": "The **RankingService** computes personalized relevance scores using 5 weighted factors: publisher match (3.0), category match (2.0), recency (1.5), engagement history (1.0), and popularity (0.5). These weights can be tuned based on A/B testing."
    },
    {
      "lines": "200-250",
      "section": "Candidate Gathering",
      "explanation": "**gatherCandidates()** pulls articles from three sources: followed publishers (direct match), interested categories (topic match), and trending (diversity/cold start). Each source contributes up to 50 articles, creating a candidate pool of ~200-500 articles."
    },
    {
      "lines": "260-290",
      "section": "Deduplication",
      "explanation": "**deduplicate()** uses content hash to identify the same story from multiple publishers. It keeps the version with highest engagement. This prevents users seeing 'iPhone announced' from 5 different sources."
    },
    {
      "lines": "295-320",
      "section": "Diversification",
      "explanation": "**diversify()** prevents feed monotony by limiting consecutive articles from the same publisher to 3. Excess articles are deferred to the end. This ensures variety even when one publisher dominates rankings."
    },
    {
      "lines": "330-380",
      "section": "Feed Generation (Main Flow)",
      "explanation": "**getUserFeed()** implements the complete hybrid model: 1) Cold start fallback for new users, 2) L1 cache check, 3) Candidate gathering from L2 caches, 4) Deduplication, 5) Scoring/ranking, 6) Diversification, 7) Pagination, 8) Cache result."
    },
    {
      "lines": "400-450",
      "section": "User Management",
      "explanation": "**followPublisher()**, **setUserInterests()**, and **likeArticle()** modify user preferences and invalidate the feed cache. Cache invalidation ensures users see updated feeds after preference changes."
    }
  ],
  "complexity_analysis": {
    "time": {
      "registerPublisher": {
        "complexity": "O(1)",
        "explanation": "HashMap insertion"
      },
      "ingestArticle": {
        "complexity": "O(C)",
        "explanation": "Where C = number of categories. Update publisher cache + update each category cache"
      },
      "getUserFeed": {
        "complexity": "O(P \u00d7 A + N log N)",
        "explanation": "Where P = followed publishers, A = articles per publisher (capped at 50), N = total candidates. Gathering is O(P \u00d7 50), sorting is O(N log N) where N \u2248 500"
      },
      "followPublisher": {
        "complexity": "O(1)",
        "explanation": "Set insertion + cache invalidation"
      },
      "likeArticle": {
        "complexity": "O(1)",
        "explanation": "Counter increment + engagement history update"
      },
      "overall": "Read path: O(500 log 500) \u2248 O(4500) operations per feed request, all in-memory = ~5ms"
    },
    "space": {
      "complexity": "O(P \u00d7 A + C \u00d7 A + U \u00d7 F)",
      "breakdown": "- Publisher article cache: O(1000 publishers \u00d7 100 articles) = 100K articles cached\n- Category article cache: O(20 categories \u00d7 100 articles) = 2K articles cached (with overlap)\n- User feed cache (L1): O(active_users \u00d7 pages \u00d7 20 articles)\n- Total Redis: ~500MB for article caches + ~2GB for user feed caches",
      "note": "This is dramatically less than fan-out-on-write which would require O(users \u00d7 articles)"
    },
    "can_we_do_better": "The hybrid approach is near-optimal. Potential improvements: 1) Pre-compute feeds for power users (top 1% by activity), 2) Use approximate nearest neighbor for ML-based ranking, 3) Bloom filters for deduplication to reduce memory."
  },
  "dry_run": {
    "example": "registerPublisher('tc', ...), followPublisher('user1', 'tc'), setUserInterests('user1', ['Tech']), ingestArticle(a1), getUserFeed('user1', 0, 10)",
    "trace_table": "| Step | Operation | Cache State | Action | Result |\n|------|-----------|-------------|--------|--------|\n| 1 | registerPublisher('tc', ...) | - | Store in publishers map | Publisher registered |\n| 2 | followPublisher('user1', 'tc') | - | Add to user1.followedPublishers | user1 follows tc |\n| 3 | setUserInterests('user1', ['Tech']) | Invalidate feed:user1:* | Set user1.interests | Interests: {Tech} |\n| 4 | ingestArticle(a1: 'AI News' by tc) | articles:pub:tc = [a1], articles:cat:Tech = [a1] | Update all caches | Article indexed |\n| 5 | getUserFeed('user1', 0, 10) | MISS feed:user1:page_0 | Generate feed | - |\n| 5a | \u2192 Gather candidates | Read articles:pub:tc + articles:cat:Tech | Merge unique articles | candidates = [a1] |\n| 5b | \u2192 Score articles | - | publisher_match(3.0) + category_match(2.0) + recency(1.3) | a1.score = 6.3 |\n| 5c | \u2192 Deduplicate | - | No duplicates | candidates = [a1] |\n| 5d | \u2192 Diversify | - | Only 1 publisher | no change |\n| 5e | \u2192 Paginate & cache | SET feed:user1:page_0 = [a1] | Cache for 60s | Return [a1] |",
    "final_answer": "Feed contains 1 article: [TC] AI News (score: 6.3)"
  },
  "test_cases": [
    {
      "name": "Single publisher, single user",
      "category": "Happy Path",
      "input": "Register TechCrunch, user follows TC, ingest 3 articles, getUserFeed",
      "expected": "3 articles from TC, sorted by recency + popularity",
      "explanation": "Basic flow with direct publisher match"
    },
    {
      "name": "Cold start user",
      "category": "Edge Case",
      "input": "New user with no follows/interests calls getUserFeed",
      "expected": "Returns trending articles",
      "explanation": "System falls back to global trending for personalization"
    },
    {
      "name": "Interest vs Follow priority",
      "category": "Medium",
      "input": "User follows Sports publisher, interested in Tech. TC publishes Tech article, ESPN publishes Sports.",
      "expected": "Both articles appear; followed publisher (ESPN) scores higher due to 3.0 weight",
      "explanation": "Publisher match weight (3.0) > category match weight (2.0)"
    },
    {
      "name": "Deduplication across publishers",
      "category": "Medium",
      "input": "NYT and CNN both publish 'iPhone Launch' (same title hash)",
      "expected": "Only one version appears (higher engagement wins)",
      "explanation": "Content hash deduplication prevents duplicate stories"
    },
    {
      "name": "Diversification kicks in",
      "category": "Medium",
      "input": "User follows only ESPN. ESPN has 10 top-scoring articles.",
      "expected": "Max 3 consecutive ESPN articles, then interleaved with trending",
      "explanation": "Diversification limit prevents single-source domination"
    },
    {
      "name": "Engagement boost",
      "category": "Hard",
      "input": "User follows TC and ESPN equally. User has 10 likes on ESPN articles, 0 on TC.",
      "expected": "ESPN articles score higher due to engagement_boost factor",
      "explanation": "Implicit preference learned from engagement history"
    },
    {
      "name": "Cache hit on second request",
      "category": "Performance",
      "input": "Call getUserFeed twice within 60 seconds",
      "expected": "Second call returns cached result instantly",
      "explanation": "L1 user feed cache with 60s TTL"
    },
    {
      "name": "Cache invalidation on follow",
      "category": "Consistency",
      "input": "getUserFeed (cached), followPublisher, getUserFeed again",
      "expected": "Second feed includes new publisher's articles",
      "explanation": "Cache invalidated on preference change"
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Computing full feed on every request (no caching)",
      "why_wrong": "At 100K QPS, this would require 100K ranking computations per second. Database would be overwhelmed, latency would exceed 200ms.",
      "correct_approach": "Multi-level caching: L1 (user feed, 60s TTL) + L2 (publisher/category articles, 5min TTL). Cache hits handle 70%+ of requests.",
      "code_wrong": "def getUserFeed(userId):\n    articles = db.query(\"SELECT * FROM articles WHERE publisher IN user.follows\")",
      "code_correct": "cached = cache.get(f'feed:{userId}')\nif cached: return cached\n# ... generate from L2 caches ..."
    },
    {
      "mistake": "Fan-out on write for all publishers",
      "why_wrong": "Celebrity publisher problem: NYT with 5M followers = 5M cache updates per article. Write amplification kills system.",
      "correct_approach": "Hybrid model: cache at publisher/category level (shared), assemble user feeds on read from these shared caches.",
      "code_wrong": "def onNewArticle(article):\n    for follower in getFollowers(article.publisher):  # 5 MILLION!\n        updateUserFeed(follower, article)",
      "code_correct": "def onNewArticle(article):\n    cache.update(f'articles:pub:{article.publisher}', article)  # ONE update\n    for cat in article.categories:\n        cache.update(f'articles:cat:{cat}', article)"
    },
    {
      "mistake": "No deduplication strategy",
      "why_wrong": "Same breaking news from 10 publishers = 10 redundant items in user feed. Poor UX and wasted screen space.",
      "correct_approach": "Content hashing on title (or NLP similarity). Keep highest-engagement version of duplicate stories.",
      "code_wrong": "# Just return all articles from followed publishers",
      "code_correct": "seen_hashes = {}\nfor article in candidates:\n    if article.content_hash not in seen_hashes:\n        seen_hashes[article.content_hash] = article"
    },
    {
      "mistake": "Single-level ranking without diversification",
      "why_wrong": "If user follows ESPN and they publish 20 articles/day, feed becomes ESPN-only. Users want variety.",
      "correct_approach": "Post-ranking diversification: limit consecutive articles from same publisher. Interleave with other sources.",
      "code_wrong": "return sorted(articles, key=lambda a: score(a))[:20]",
      "code_correct": "ranked = sorted(articles, key=lambda a: score(a))\nreturn diversify(ranked)[:20]  # Max 3 consecutive from same publisher"
    },
    {
      "mistake": "Ignoring cold start problem",
      "why_wrong": "New users with no follows/interests get empty feeds. Terrible onboarding experience.",
      "correct_approach": "Fallback to trending/popular content. Use onboarding flow to capture initial interests. Consider location-based defaults.",
      "code_wrong": "if not user.follows and not user.interests:\n    return []  # Empty!",
      "code_correct": "if not user.follows and not user.interests:\n    return get_trending_articles()  # Global popular content"
    },
    {
      "mistake": "Not invalidating cache on preference changes",
      "why_wrong": "User follows new publisher but feed doesn't update for 60 seconds. Confusing UX.",
      "correct_approach": "Invalidate user's feed cache on any preference change (follow, unfollow, interest update, like).",
      "code_wrong": "def followPublisher(userId, pubId):\n    user.follows.add(pubId)\n    # Forgot to invalidate!",
      "code_correct": "def followPublisher(userId, pubId):\n    user.follows.add(pubId)\n    cache.delete(f'feed:{userId}:*')  # Invalidate all cached pages"
    }
  ],
  "interview_tips": {
    "opening": "Thank you for this problem. I'll be designing a news aggregation system similar to Google News. Before diving in, let me clarify some requirements and constraints, then I'll walk through my high-level architecture, discuss key design decisions, and finally dive into the detailed implementation.",
    "clarifying_questions_to_ask": [
      "What's the read/write ratio? Is this read-heavy or write-heavy? (Confirms caching strategy)",
      "What's the target latency for feed generation? Can we tolerate some staleness? (Determines cache TTLs)",
      "How many publishers does a typical user follow? (Affects fan-out strategy)",
      "Should we prioritize freshness or personalization quality? (Trade-off discussion)",
      "Do we need real-time breaking news alerts, or is poll-based acceptable? (Push vs pull)",
      "What's the expected article lifecycle - do we age out old articles? (Storage/cleanup)",
      "How should we handle the same story from multiple sources? (Deduplication)"
    ],
    "what_to_mention_proactively": [
      "I'll use a HYBRID model - not pure push or pure pull - to balance latency and storage",
      "Multi-level caching is critical: L1 for user feeds, L2 for publisher/category articles",
      "I'll address the celebrity publisher problem (NYT with millions of followers)",
      "Deduplication is essential - same story from multiple publishers should appear once",
      "Cold start fallback to trending content for new users",
      "Trade-off: 60-second cache TTL means potential 60s staleness vs 70% cache hit rate"
    ],
    "communication_during_coding": [
      "I'm defining the core entities first - Publisher, Article, User - to establish the data model",
      "The RankingService is the brain - it computes personalized scores using 5 weighted factors",
      "The FeedGeneratorService implements the hybrid model - gather from component caches, rank in memory",
      "Notice how I'm capping candidates at 50 per source to keep ranking computation bounded",
      "The diversification step prevents single-publisher domination in the feed"
    ],
    "if_stuck": [
      "Step back: What's the hardest constraint? 100K QPS + 200ms latency = must use caching",
      "Draw the data flow: Publishers \u2192 Crawlers \u2192 Cache \u2192 API \u2192 Users",
      "Think about similar systems: How does Twitter solve this? (Fan-out tradeoffs)",
      "Break into subproblems: Ingestion, Storage, Feed Generation, Personalization"
    ],
    "time_management": "0-5 min: Clarify requirements and constraints | 5-15 min: High-level architecture (draw diagram) | 15-25 min: Deep dive on feed generation (the core algorithm) | 25-35 min: Discuss caching strategy and trade-offs | 35-45 min: Handle follow-ups (scaling, ML, real-time)"
  },
  "pattern_recognition": {
    "pattern_name": "Read-Heavy System with Personalization (Hybrid Fan-Out)",
    "indicators": [
      "High read/write ratio (1000:1 or higher)",
      "Latency SLA on reads (<200ms)",
      "Personalization requirement (user-specific content)",
      "Many-to-many relationship (users \u2194 content sources)",
      "Content freshness requirement"
    ],
    "similar_problems": [
      "Twitter Home Timeline: Similar hybrid model, but with social graph instead of publishers",
      "YouTube Recommendations: Add ML ranking layer on top of candidate generation",
      "LinkedIn Feed: Professional content + social signals",
      "Instagram Explore: Heavy ML, less followed-based, more interest-based"
    ],
    "template": "1. Cache at COMPONENT level (shared), not USER level (explodes storage)\n2. GATHER candidates from cached components\n3. RANK in memory using personalization signals\n4. CACHE final result with short TTL\n5. INVALIDATE on preference changes"
  },
  "follow_up_preparation": {
    "part_2_hint": "**Part 2: Real-Time Notifications & Breaking News** - You'll need to add a push layer for urgent content. Consider WebSockets for connected users, push notifications for mobile. Key design: maintain user presence, prioritize breaking news bypass of cache.",
    "part_3_hint": "**Part 3: ML-Based Recommendations** - Replace rule-based ranking with ML model. Candidate generation stays same, but scoring uses feature vectors (user embedding, article embedding, context features). Need feature store, model serving infrastructure (TensorFlow Serving), online/offline model training pipeline.",
    "data_structure_evolution": "Part 1: HashMap + Sorted Lists + TTL Cache \u2192 Part 2: Add Pub/Sub (Kafka), WebSocket connections, presence tracking \u2192 Part 3: Add Feature Store, Vector DB for embeddings, ML inference service"
  },
  "generated_at": "2026-01-14T15:31:20.224677",
  "_meta": {
    "problem_id": "news_feed_aggregator",
    "part_number": null,
    "model": "claude-opus-4-5-20251101"
  }
}