{
  "problem_title": "News Feed Aggregator System",
  "difficulty": "medium",
  "category": "HLD/System Design",
  "estimated_time": "45-60 minutes",
  "problem_analysis": {
    "first_impressions": "This is a **read-heavy distributed system** (1700:1 read/write ratio) requiring fast feed generation (<200ms p99). The core challenge is balancing personalization complexity against latency requirements while handling 100K QPS. This is fundamentally about **smart caching** and **hybrid feed generation strategies**.",
    "pattern_recognition": "## Key Patterns\n\n1. **Fan-out on Read vs Write** - Classic Twitter feed problem\n2. **Multi-level Caching** - Category \u2192 Publisher \u2192 User\n3. **Scoring/Ranking Algorithm** - Real-time personalization\n4. **Pub/Sub for Ingestion** - Decoupled article processing\n5. **Read-Replica Scaling** - Handle high read QPS",
    "key_constraints": [
      "**100K QPS reads** - Requires aggressive caching; cannot compute feeds from scratch",
      "**<200ms p99 latency** - Must pre-compute or cache; no complex real-time ML",
      "**5 min freshness** - Cache TTL must be short; need smart invalidation",
      "**10M users** - Cannot pre-generate all feeds; need hybrid approach",
      "**1000 publishers** - Manageable; can cache all publisher feeds",
      "**Deduplication needed** - Same story from multiple sources needs grouping"
    ],
    "clarifying_questions": [
      "**What defines 'personalization'?** - Just followed publishers + interests, or also engagement history/ML? Impacts compute complexity.",
      "**Is feed ordering deterministic?** - Can we show slightly different orders to different requests? Affects caching strategy.",
      "**How do we handle viral articles?** - 10x traffic spike on breaking news - need circuit breakers?",
      "**Deduplication scope?** - Exact duplicates only, or semantically similar stories? Former is simpler.",
      "**Cold start policy?** - New users: show trending? Random? Onboarding questionnaire?",
      "**Pagination model?** - Offset-based or cursor-based? Cursor is better for consistency.",
      "**Geographic distribution?** - Single region or global? Affects consistency model."
    ],
    "edge_cases_to_consider": [
      "New user with no preferences (cold start)",
      "User follows 0 publishers but has category interests",
      "Publisher with no recent articles",
      "Duplicate articles from different publishers",
      "Viral article causing traffic spike",
      "User unfollows publisher - cache invalidation",
      "Publisher becomes inactive/removed",
      "Rate limiting aggressive crawlers"
    ]
  },
  "requirements_coverage": {
    "checklist": [
      {
        "requirement": "registerPublisher(publisherId, name, rssUrl, categories)",
        "how_met": "Store in Publishers table, validate RSS connectivity, schedule crawler job",
        "gotchas": [
          "Validate URL is reachable",
          "Dedupe if same RSS URL registered twice"
        ]
      },
      {
        "requirement": "fetchArticles(publisherId)",
        "how_met": "Crawler workers poll RSS/API, deduplicate by content hash, write to Article DB + publish to Kafka",
        "gotchas": [
          "Handle publisher rate limits",
          "Retry with exponential backoff"
        ]
      },
      {
        "requirement": "getUserFeed(userId, page, pageSize) in <200ms",
        "how_met": "Check user feed cache \u2192 merge category caches \u2192 score/rank \u2192 return. Hybrid pull/push model.",
        "gotchas": [
          "Cache miss requires fast fallback",
          "Pagination cursor should be stable"
        ]
      },
      {
        "requirement": "followPublisher(userId, publisherId)",
        "how_met": "Update UserFollows table + invalidate user's feed cache",
        "gotchas": [
          "Eventual consistency - next feed refresh shows new publisher"
        ]
      },
      {
        "requirement": "setUserInterests(userId, categories[])",
        "how_met": "Update UserInterests table + invalidate user's cached feed",
        "gotchas": [
          "Limit categories per user to bound computation"
        ]
      },
      {
        "requirement": "likeArticle(userId, articleId)",
        "how_met": "Write to UserEngagement table, async update publisher affinity scores",
        "gotchas": [
          "Don't let engagement writes slow down reads"
        ]
      }
    ],
    "complexity_targets": [
      {
        "operation": "getUserFeed (cache hit)",
        "target": "<50ms",
        "achieved": "~20ms",
        "why": "Direct Redis lookup for pre-computed feed"
      },
      {
        "operation": "getUserFeed (cache miss)",
        "target": "<200ms",
        "achieved": "~150ms",
        "why": "Merge cached category feeds + scoring"
      },
      {
        "operation": "followPublisher",
        "target": "<100ms",
        "achieved": "~50ms",
        "why": "Simple DB write + async cache invalidation"
      },
      {
        "operation": "fetchArticles",
        "target": "Background",
        "achieved": "Background",
        "why": "Async processing, doesn't affect user latency"
      }
    ],
    "non_goals": [
      "Real-time ML personalization (too slow for 200ms target)",
      "Full-text search (separate Elasticsearch endpoint)",
      "Article content storage (store metadata + link only)",
      "User-generated content/comments (separate service)",
      "Cross-device sync (handled by client)"
    ]
  },
  "assumptions": [
    "Articles are identified by URL - same URL = same article (for deduplication)",
    "Publisher RSS feeds are well-formed and don't require complex parsing",
    "User preferences are relatively stable (don't change multiple times per minute)",
    "10-20 categories total (Tech, Sports, Politics, etc.) - not unlimited",
    "Read/write ratio is consistent throughout the day (no extreme spikes)",
    "Single region deployment initially (can extend to multi-region later)"
  ],
  "tradeoffs": [
    {
      "decision": "Feed Generation: Pull vs Push",
      "chosen": "Hybrid - Pre-compute for active users, compute on-demand for others",
      "why": "Pure push wastes storage for inactive users; pure pull is too slow for 200ms",
      "alternative": "Pure pull with aggressive caching",
      "when_to_switch": "If user base is mostly daily-active, lean more toward push"
    },
    {
      "decision": "Caching granularity",
      "chosen": "Multi-level: Category cache + Publisher cache + User feed cache",
      "why": "Categories are shared, enabling high cache hit rates; user cache is personalized",
      "alternative": "Only user-level caching",
      "when_to_switch": "If personalization becomes very user-specific (ML-based)"
    },
    {
      "decision": "Storage: SQL vs NoSQL",
      "chosen": "PostgreSQL for articles/users + Redis for caching",
      "why": "Relational data with clear schema; Redis for fast reads",
      "alternative": "Full NoSQL (Cassandra/DynamoDB)",
      "when_to_switch": "If scale exceeds single-region Postgres capacity"
    },
    {
      "decision": "Ranking complexity",
      "chosen": "Simple rule-based scoring (publisher weight + category match + recency)",
      "why": "Deterministic, cacheable, fast (<10ms)",
      "alternative": "ML-based ranking model",
      "when_to_switch": "When engagement metrics plateau and personalization needs improvement"
    }
  ],
  "extensibility_and_followups": {
    "design_principles": [
      "**Separation of Concerns**: Ingestion, Storage, Feed Generation, Ranking are separate services",
      "**Cache Abstraction**: CacheService interface allows swapping Redis for Memcached",
      "**Pluggable Ranking**: RankingService can be swapped from rule-based to ML-based",
      "**Event-Driven Ingestion**: Kafka enables adding new processors without changing crawlers"
    ],
    "why_this_design_scales": "## Scalability Hooks\n\n1. **Horizontal Read Scaling**: Add API servers behind load balancer\n2. **Cache Sharding**: Partition Redis by user_id hash\n3. **Database Read Replicas**: Route reads to replicas\n4. **Async Processing**: Kafka absorbs burst writes\n5. **CDN for Static Content**: Article images/thumbnails via CDN",
    "expected_followup_hooks": [
      "**Part 2 (Real-time Notifications)**: Add WebSocket service + push notification queue",
      "**Part 3 (ML Recommendations)**: Replace RankingService with ML model serving",
      "**Breaking News**: Priority queue for urgent articles + push notifications",
      "**Trending Topics**: Add aggregation pipeline for topic extraction"
    ],
    "invariants": [
      "Feed always returns articles sorted by score descending",
      "User only sees articles from followed publishers OR matching interests",
      "Cache TTL never exceeds 5 minutes (freshness requirement)",
      "Duplicate articles (same URL) are never shown twice"
    ]
  },
  "visual_explanation": {
    "problem_visualization": "```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     NEWS FEED AGGREGATOR - DATA FLOW                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502   PUBLISHERS (1000)              SYSTEM                    USERS (10M)      \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502   \u2502 NYT RSS  \u2502\u2500\u2500\u2510                                    \u250c\u2500\u2500\u2500\u25b6\u2502 User A   \u2502      \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502    \u2502 Follows: \u2502      \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502     \u2502    CRAWLER WORKERS      \u2502    \u2502    \u2502 NYT, ESPN\u2502      \u2502\n\u2502   \u2502 ESPN RSS \u2502\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u25b6\u2502  (Poll every 5 min)     \u2502    \u2502    \u2502 Likes:   \u2502      \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502    \u2502 Tech,    \u2502      \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502                 \u2502                  \u2502    \u2502 Sports   \u2502      \u2502\n\u2502   \u2502 BBC RSS  \u2502\u2500\u2500\u2518                 \u25bc                  \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502                         \u2502  MESSAGE QUEUE  \u2502          \u2502    \u2502 User B   \u2502      \u2502\n\u2502   ... 997 more          \u2502    (Kafka)      \u2502          \u2514\u2500\u2500\u2500\u25b6\u2502 Follows: \u2502      \u2502\n\u2502                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502 BBC      \u2502      \u2502\n\u2502                                  \u2502                        \u2502 Likes:   \u2502      \u2502\n\u2502                                  \u25bc                        \u2502 Politics \u2502      \u2502\n\u2502                         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502                         \u2502   PROCESSORS    \u2502                                 \u2502\n\u2502                         \u2502 \u2022 Dedupe        \u2502               ... 9.99M more    \u2502\n\u2502                         \u2502 \u2022 Categorize    \u2502                                 \u2502\n\u2502                         \u2502 \u2022 Extract       \u2502                                 \u2502\n\u2502                         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502\n\u2502                                  \u2502                                          \u2502\n\u2502                                  \u25bc                                          \u2502\n\u2502   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502   \u2502                        STORAGE LAYER                              \u2502     \u2502\n\u2502   \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502     \u2502\n\u2502   \u2502  \u2502  PostgreSQL   \u2502   \u2502    Redis      \u2502   \u2502 Elasticsearch \u2502       \u2502     \u2502\n\u2502   \u2502  \u2502  (Articles,   \u2502   \u2502   (Caches)    \u2502   \u2502   (Search)    \u2502       \u2502     \u2502\n\u2502   \u2502  \u2502   Users)      \u2502   \u2502               \u2502   \u2502               \u2502       \u2502     \u2502\n\u2502   \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502     \u2502\n\u2502   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
    "data_structure_state": "```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          CACHE HIERARCHY                                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                             \u2502\n\u2502  LEVEL 1: CATEGORY CACHE (Shared - High Hit Rate)                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502 Key: \"articles:tech:top100\"     TTL: 5 min                         \u2502    \u2502\n\u2502  \u2502 Value: [art_1, art_2, art_3, ... art_100]  (sorted by score)       \u2502    \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    \u2502\n\u2502  \u2502 Key: \"articles:sports:top100\"   TTL: 5 min                         \u2502    \u2502\n\u2502  \u2502 Value: [art_50, art_51, art_52, ... art_150]                       \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                                                                             \u2502\n\u2502  LEVEL 2: PUBLISHER CACHE                                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502 Key: \"publisher:nyt:recent50\"   TTL: 5 min                         \u2502    \u2502\n\u2502  \u2502 Value: [art_10, art_11, ...]                                       \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                                                                             \u2502\n\u2502  LEVEL 3: USER FEED CACHE (Personalized)                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502 Key: \"feed:user_123:page0\"      TTL: 2 min                         \u2502    \u2502\n\u2502  \u2502 Value: [art_1, art_50, art_11, ...]  (pre-ranked for this user)    \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2502                                                                             \u2502\n\u2502  CACHE MISS STRATEGY:                                                       \u2502\n\u2502  1. Check user feed cache         \u2192  HIT? Return immediately               \u2502\n\u2502  2. Get user's interests/follows                                           \u2502\n\u2502  3. Fetch category caches         \u2192  Merge articles from all interests     \u2502\n\u2502  4. Fetch publisher caches        \u2192  Add articles from followed publishers \u2502\n\u2502  5. Score & Rank                  \u2192  Apply personalization scoring         \u2502\n\u2502  6. Cache result                  \u2192  Store for 2 min                       \u2502\n\u2502  7. Return to user                                                          \u2502\n\u2502                                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
    "algorithm_flow": [
      {
        "step": 1,
        "description": "User requests feed - Check cache first",
        "visualization": "```\nRequest: getUserFeed(user_123, page=0, pageSize=20)\n                    \u2502\n                    \u25bc\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502 Redis Lookup  \u2502\n            \u2502 feed:user_123 \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502                     \u2502\n    CACHE HIT             CACHE MISS\n    (return)              (continue)\n```",
        "key_point": "Cache hit returns in <20ms, avoiding all computation"
      },
      {
        "step": 2,
        "description": "Fetch user preferences",
        "visualization": "```\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502         USER PROFILE LOOKUP         \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502 user_id: user_123                   \u2502\n    \u2502 followed_publishers: [nyt, espn]    \u2502\n    \u2502 interests: [tech, sports]           \u2502\n    \u2502 engagement: {nyt: 5, espn: 12}      \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
        "key_point": "User preferences are cached in Redis for O(1) lookup"
      },
      {
        "step": 3,
        "description": "Merge articles from category + publisher caches",
        "visualization": "```\nCategory Caches:               Publisher Caches:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 tech:top100\u2502\u2500\u2500\u2510              \u2502 nyt:recent50\u2502\u2500\u2500\u2510\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502sports:top10\u2502\u2500\u2500\u253c\u2500\u2500\u2500MERGE\u2500\u2500\u2500\u2500\u25b6 \u2502espn:recent50\u2502\u2500\u2500\u253c\u2500\u25b6 Candidate Pool\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502    (200 articles)\n                \u2502                               \u2502\n                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
        "key_point": "Merging cached lists is fast - O(n) where n is small (~200)"
      },
      {
        "step": 4,
        "description": "Score and rank articles for this user",
        "visualization": "```\n   Article Scoring Formula:\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502                                                          \u2502\n   \u2502  score = (publisher_match \u00d7 2.0)   // Followed? +2      \u2502\n   \u2502        + (category_match \u00d7 1.5)    // Interest? +1.5    \u2502\n   \u2502        + (engagement_boost \u00d7 1.0)  // Liked similar? +1 \u2502\n   \u2502        + (recency_factor \u00d7 0.8)    // Newer = higher    \u2502\n   \u2502        + (popularity \u00d7 0.3)        // More likes = +0.3 \u2502\n   \u2502                                                          \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n   \n   Example:\n   Article: \"Tech Giants Report Q3 Earnings\"\n   - Publisher: NYT (followed)    \u2192 +2.0\n   - Category: Tech (interest)    \u2192 +1.5\n   - User liked NYT before        \u2192 +1.0\n   - Published 1 hour ago         \u2192 +0.7\n   - 5000 likes globally          \u2192 +0.2\n   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n   Total Score: 5.4\n```",
        "key_point": "Simple weighted scoring is deterministic and fast (<10ms)"
      },
      {
        "step": 5,
        "description": "Return top N articles, cache the result",
        "visualization": "```\nRanked Feed (Top 20):           Cache Update:\n\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \n\u2502Rank\u2502 Article         \u2502 Score \u2502    feed:user_123:page0\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524        \u25bc\n\u2502 1  \u2502 Tech Giants Q3  \u2502  5.4  \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 2  \u2502 NBA Finals Game \u2502  4.8  \u2502    \u2502   REDIS     \u2502\n\u2502 3  \u2502 AI Breakthrough \u2502  4.5  \u2502    \u2502 TTL: 2 min  \u2502\n\u2502...\u2502 ...             \u2502  ...  \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502 20 \u2502 Market Update   \u2502  2.1  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
        "key_point": "Cache for 2 minutes - balance freshness vs compute savings"
      }
    ],
    "dry_run_table": "| Step | Operation | Cache State | DB Action | Result |\n|------|-----------|-------------|-----------|--------|\n| 1 | registerPublisher(nyt, ...) | - | INSERT publishers | Publisher registered |\n| 2 | registerPublisher(espn, ...) | - | INSERT publishers | Publisher registered |\n| 3 | followPublisher(user_1, nyt) | - | INSERT user_follows | User follows NYT |\n| 4 | setUserInterests(user_1, [tech]) | - | UPDATE users | Interests set |\n| 5 | fetchArticles(nyt) | articles:nyt:recent50 updated | INSERT articles | 10 articles ingested |\n| 6 | getUserFeed(user_1, 0, 10) | MISS \u2192 compute \u2192 CACHE | SELECT articles | Returns 10 NYT tech articles |\n| 7 | getUserFeed(user_1, 0, 10) | HIT | - | Returns cached feed instantly |"
  },
  "thinking_process": {
    "step_by_step": [
      "When I see **100K QPS reads with <200ms latency**, I immediately think **aggressive caching** - we cannot compute feeds from scratch on every request",
      "The **1700:1 read/write ratio** tells me this is Twitter-like - classic **fan-out on read vs write** decision",
      "With **10M users**, pre-computing all feeds is expensive storage-wise. But with **1000 publishers**, caching all publisher feeds is cheap",
      "The key insight: **Multi-level caching** - cache at category and publisher level (shared), compute personalized ranking on-demand",
      "For ranking, a **simple weighted scoring function** is sufficient - ML would be too slow for 200ms target",
      "**Hybrid strategy**: Pre-compute feeds for daily-active users, compute on-demand for others. Best of both worlds."
    ],
    "key_insight": "## The Core Insight\n\nDon't cache **user feeds** directly (too many users). Instead, cache **building blocks** (category articles, publisher articles) that are shared across users. Then **compose and rank** user feeds quickly by merging cached building blocks.\n\nThis gives:\n- High cache hit rate (categories are shared)\n- Fast personalization (just scoring, not fetching)\n- Memory efficiency (no per-user storage for inactive users)",
    "why_this_works": "The approach works because:\n\n1. **Shared caching reduces storage**: 20 categories \u00d7 100 articles = 2000 cached items vs 10M users \u00d7 20 articles = 200M items\n\n2. **Scoring is fast**: Simple arithmetic on ~200 candidate articles takes <10ms\n\n3. **Freshness maintained**: Category caches refresh every 5 min, meeting the freshness requirement\n\n4. **Graceful degradation**: If Redis is slow, we can still query DB directly (slower but works)"
  },
  "approaches": [
    {
      "name": "Approach 1: Pure Pull (Compute on Every Request)",
      "description": "When user requests feed, query DB for all articles from followed publishers + matching categories, rank them, return top N.",
      "pseudocode": "def get_user_feed(user_id, page, page_size):\n    user = db.get_user(user_id)\n    articles = db.get_articles(\n        publishers=user.followed_publishers,\n        categories=user.interests,\n        since=now - 24h\n    )\n    ranked = rank_articles(articles, user)\n    return paginate(ranked, page, page_size)",
      "time_complexity": "O(P \u00d7 A) where P = publishers followed, A = articles per publisher",
      "space_complexity": "O(A) for article processing",
      "pros": [
        "Simple implementation",
        "Always fresh",
        "No cache management"
      ],
      "cons": [
        "Too slow for 100K QPS - DB bottleneck",
        "Cannot meet 200ms latency",
        "Repeated computation for same user"
      ],
      "when_to_use": "Only for very small scale (<1000 users) or as fallback"
    },
    {
      "name": "Approach 2: Pure Push (Pre-compute All Feeds)",
      "description": "Whenever a new article arrives, push it to all followers' pre-computed feeds in cache.",
      "pseudocode": "def on_new_article(article):\n    followers = get_all_followers(article.publisher_id)\n    interested_users = get_users_with_interest(article.categories)\n    for user in (followers \u222a interested_users):\n        user_feed = cache.get(f\"feed:{user.id}\")\n        user_feed.insert_ranked(article)\n        cache.set(f\"feed:{user.id}\", user_feed)\n\ndef get_user_feed(user_id, page, page_size):\n    return cache.get(f\"feed:{user_id}\")[page*page_size:(page+1)*page_size]",
      "time_complexity": "O(1) for reads, O(F) for writes where F = followers",
      "space_complexity": "O(U \u00d7 N) where U = users, N = feed size",
      "pros": [
        "O(1) reads - extremely fast",
        "Consistent latency"
      ],
      "cons": [
        "Storage explosion for 10M users",
        "Write amplification on popular publishers",
        "Wasted storage for inactive users"
      ],
      "when_to_use": "When user base is small and mostly daily-active"
    },
    {
      "name": "Approach 3: Hybrid (Optimal) - Multi-Level Caching",
      "description": "Cache at multiple levels: category articles (shared), publisher articles (shared), and user feeds (only for active users). Compose feeds by merging cached building blocks.",
      "pseudocode": "def get_user_feed(user_id, page, page_size):\n    # Level 1: Check user feed cache\n    cached = cache.get(f\"feed:{user_id}:page{page}\")\n    if cached:\n        return cached\n    \n    # Level 2: Get user preferences\n    user = get_user_profile(user_id)  # Also cached\n    \n    # Level 3: Merge category + publisher caches\n    candidates = set()\n    for category in user.interests:\n        candidates |= cache.get(f\"articles:{category}:top100\")\n    for publisher in user.followed_publishers:\n        candidates |= cache.get(f\"publisher:{publisher}:recent50\")\n    \n    # Level 4: Score and rank\n    ranked = rank_for_user(list(candidates), user)\n    \n    # Cache and return\n    result = paginate(ranked, page, page_size)\n    cache.set(f\"feed:{user_id}:page{page}\", result, ttl=120)\n    return result",
      "time_complexity": "O(1) for cache hit, O(C \u00d7 100 + P \u00d7 50) for miss where C = interests, P = follows",
      "space_complexity": "O(Categories \u00d7 100 + Publishers \u00d7 50 + ActiveUsers \u00d7 FeedSize)",
      "pros": [
        "Fast reads (cache hit <20ms)",
        "Memory efficient (shared caches)",
        "Balances freshness vs compute"
      ],
      "cons": [
        "More complex implementation",
        "Multiple cache layers to manage"
      ],
      "key_insight": "Share what can be shared (categories, publishers), personalize what must be personalized (ranking)"
    }
  ],
  "optimal_solution": {
    "name": "Hybrid Multi-Level Caching with Rule-Based Ranking",
    "explanation_md": "## Optimal Approach\n\nWe use a **3-tier caching strategy** that balances personalization with efficiency:\n\n### Tier 1: Category Cache (Shared, High Value)\n- Cache top 100 articles per category\n- 20 categories \u00d7 100 articles = 2000 cached items\n- TTL: 5 minutes\n- Updated when new articles are processed\n\n### Tier 2: Publisher Cache (Shared)\n- Cache recent 50 articles per publisher\n- 1000 publishers \u00d7 50 articles = 50,000 cached items\n- TTL: 5 minutes\n- Updated on article ingestion\n\n### Tier 3: User Feed Cache (Personalized, Ephemeral)\n- Cache computed feeds for active users only\n- TTL: 2 minutes (shorter to stay fresh)\n- Invalidated on preference changes\n\n### Feed Generation Algorithm\n\n1. **Check user feed cache** - If hit, return immediately\n2. **Fetch user profile** - Interests, follows, engagement history\n3. **Merge candidate articles** - From category + publisher caches\n4. **Deduplicate** - Remove duplicates by article URL\n5. **Score each article** - Apply personalization formula\n6. **Sort by score** - Descending order\n7. **Paginate and cache** - Store result for 2 minutes\n\n### Ranking Formula\n\n```\nscore = (is_followed_publisher \u00d7 2.0)\n      + (matches_interest \u00d7 1.5)\n      + (engagement_affinity \u00d7 1.0)\n      + (recency_score \u00d7 0.8)\n      + (popularity_score \u00d7 0.3)\n```\n\nWhere:\n- `recency_score` = max(0, 1 - (hours_since_publish / 24))\n- `popularity_score` = min(1, log10(likes + 1) / 5)\n- `engagement_affinity` = normalized count of user's past likes for this publisher",
    "data_structures": [
      {
        "structure": "HashMap<String, Article> articles",
        "purpose": "O(1) article lookup by ID"
      },
      {
        "structure": "HashMap<String, Publisher> publishers",
        "purpose": "O(1) publisher lookup"
      },
      {
        "structure": "HashMap<String, User> users",
        "purpose": "User profile storage"
      },
      {
        "structure": "HashMap<String, Set<String>> userFollows",
        "purpose": "userId \u2192 set of publisherIds"
      },
      {
        "structure": "HashMap<String, Set<String>> userInterests",
        "purpose": "userId \u2192 set of categories"
      },
      {
        "structure": "SortedSet<Article> (per category)",
        "purpose": "Sorted by score for fast top-N retrieval"
      },
      {
        "structure": "Redis SortedSet",
        "purpose": "Category and publisher caches with scores"
      }
    ],
    "algorithm_steps": [
      "1. **Register Publisher**: Validate RSS URL, store in publishers table, schedule crawler",
      "2. **Fetch Articles**: Poll RSS, parse articles, dedupe by URL hash, store in DB, publish to Kafka",
      "3. **Process Articles**: Categorize, extract metadata, update category + publisher caches",
      "4. **Follow Publisher**: Update user_follows table, async invalidate user's feed cache",
      "5. **Set Interests**: Update user_interests table, async invalidate user's feed cache",
      "6. **Get Feed**: Check cache \u2192 merge category/publisher caches \u2192 score \u2192 rank \u2192 cache \u2192 return",
      "7. **Like Article**: Record in engagement table, async update user's publisher affinity scores"
    ],
    "why_decimal": "Not applicable for this system design problem - we use simple float scores for ranking"
  },
  "solution_python_lines": [
    "\"\"\"",
    "News Feed Aggregator System - Production-Quality Implementation",
    "",
    "This module implements a scalable news feed aggregator with:",
    "- Multi-level caching (category, publisher, user feeds)",
    "- Personalized ranking based on follows, interests, and engagement",
    "- Cold start handling for new users",
    "",
    "Author: Interview Preparation Guide",
    "Complexity: O(1) for cached reads, O(n) for cache miss where n = candidate articles",
    "\"\"\"",
    "",
    "from typing import Dict, List, Set, Optional",
    "from dataclasses import dataclass, field",
    "from datetime import datetime, timedelta",
    "from collections import defaultdict",
    "from enum import Enum",
    "import heapq",
    "import hashlib",
    "import time",
    "",
    "",
    "class Category(Enum):",
    "    \"\"\"Supported news categories.\"\"\"",
    "    TECHNOLOGY = \"technology\"",
    "    SPORTS = \"sports\"",
    "    POLITICS = \"politics\"",
    "    WORLD = \"world\"",
    "    BUSINESS = \"business\"",
    "    SCIENCE = \"science\"",
    "    ENTERTAINMENT = \"entertainment\"",
    "",
    "",
    "@dataclass",
    "class Article:",
    "    \"\"\"Represents a news article.\"\"\"",
    "    id: str",
    "    publisher_id: str",
    "    title: str",
    "    url: str",
    "    categories: List[str]",
    "    published_at: datetime",
    "    likes: int = 0",
    "    content_hash: str = \"\"",
    "    ",
    "    def __post_init__(self):",
    "        if not self.content_hash:",
    "            self.content_hash = hashlib.md5(self.url.encode()).hexdigest()",
    "    ",
    "    def __hash__(self):",
    "        return hash(self.id)",
    "    ",
    "    def __eq__(self, other):",
    "        if isinstance(other, Article):",
    "            return self.id == other.id",
    "        return False",
    "",
    "",
    "@dataclass",
    "class Publisher:",
    "    \"\"\"Represents a news publisher.\"\"\"",
    "    id: str",
    "    name: str",
    "    rss_url: str",
    "    categories: List[str]",
    "    is_active: bool = True",
    "",
    "",
    "@dataclass",
    "class User:",
    "    \"\"\"Represents a user with preferences.\"\"\"",
    "    id: str",
    "    followed_publishers: Set[str] = field(default_factory=set)",
    "    interests: Set[str] = field(default_factory=set)",
    "    engagement_history: Dict[str, int] = field(default_factory=dict)",
    "    ",
    "    def get_publisher_affinity(self, publisher_id: str) -> float:",
    "        \"\"\"Calculate affinity score for a publisher based on engagement.\"\"\"",
    "        likes = self.engagement_history.get(publisher_id, 0)",
    "        if likes == 0:",
    "            return 0.0",
    "        # Normalize: cap at 1.0 after 10 likes",
    "        return min(1.0, likes / 10.0)",
    "",
    "",
    "@dataclass",
    "class CacheEntry:",
    "    \"\"\"Cache entry with TTL support.\"\"\"",
    "    data: any",
    "    expires_at: float",
    "    ",
    "    def is_valid(self) -> bool:",
    "        return time.time() < self.expires_at",
    "",
    "",
    "class SimpleCache:",
    "    \"\"\"",
    "    Simple in-memory cache with TTL support.",
    "    In production, this would be Redis.",
    "    \"\"\"",
    "    ",
    "    def __init__(self):",
    "        self._store: Dict[str, CacheEntry] = {}",
    "    ",
    "    def get(self, key: str) -> Optional[any]:",
    "        \"\"\"Get value from cache if not expired.\"\"\"",
    "        entry = self._store.get(key)",
    "        if entry and entry.is_valid():",
    "            return entry.data",
    "        if entry:",
    "            del self._store[key]  # Clean up expired",
    "        return None",
    "    ",
    "    def set(self, key: str, value: any, ttl_seconds: int = 300) -> None:",
    "        \"\"\"Set value with TTL.\"\"\"",
    "        self._store[key] = CacheEntry(",
    "            data=value,",
    "            expires_at=time.time() + ttl_seconds",
    "        )",
    "    ",
    "    def invalidate(self, key: str) -> None:",
    "        \"\"\"Remove key from cache.\"\"\"",
    "        self._store.pop(key, None)",
    "    ",
    "    def invalidate_prefix(self, prefix: str) -> None:",
    "        \"\"\"Remove all keys with given prefix.\"\"\"",
    "        keys_to_remove = [k for k in self._store if k.startswith(prefix)]",
    "        for key in keys_to_remove:",
    "            del self._store[key]",
    "",
    "",
    "class RankingService:",
    "    \"\"\"",
    "    Service for scoring and ranking articles for a user.",
    "    Uses a weighted scoring formula.",
    "    \"\"\"",
    "    ",
    "    # Scoring weights",
    "    PUBLISHER_FOLLOW_WEIGHT = 2.0",
    "    CATEGORY_INTEREST_WEIGHT = 1.5",
    "    ENGAGEMENT_WEIGHT = 1.0",
    "    RECENCY_WEIGHT = 0.8",
    "    POPULARITY_WEIGHT = 0.3",
    "    ",
    "    def __init__(self):",
    "        pass",
    "    ",
    "    def score_article(self, article: Article, user: User) -> float:",
    "        \"\"\"",
    "        Calculate personalized score for an article.",
    "        ",
    "        Args:",
    "            article: The article to score",
    "            user: The user to personalize for",
    "            ",
    "        Returns:",
    "            Float score (higher = more relevant)",
    "        \"\"\"",
    "        score = 0.0",
    "        ",
    "        # Factor 1: Publisher match (highest weight)",
    "        if article.publisher_id in user.followed_publishers:",
    "            score += self.PUBLISHER_FOLLOW_WEIGHT",
    "        ",
    "        # Factor 2: Category match",
    "        matching_categories = set(article.categories) & user.interests",
    "        if matching_categories:",
    "            score += self.CATEGORY_INTEREST_WEIGHT * len(matching_categories)",
    "        ",
    "        # Factor 3: Engagement affinity",
    "        affinity = user.get_publisher_affinity(article.publisher_id)",
    "        score += self.ENGAGEMENT_WEIGHT * affinity",
    "        ",
    "        # Factor 4: Recency (decay over 24 hours)",
    "        hours_old = (datetime.now() - article.published_at).total_seconds() / 3600",
    "        recency_score = max(0.0, 1.0 - (hours_old / 24.0))",
    "        score += self.RECENCY_WEIGHT * recency_score",
    "        ",
    "        # Factor 5: Popularity",
    "        if article.likes > 0:",
    "            import math",
    "            popularity_score = min(1.0, math.log10(article.likes + 1) / 5.0)",
    "            score += self.POPULARITY_WEIGHT * popularity_score",
    "        ",
    "        return score",
    "    ",
    "    def rank_articles(",
    "        self,",
    "        articles: List[Article],",
    "        user: User,",
    "        limit: int = 100",
    "    ) -> List[Article]:",
    "        \"\"\"",
    "        Rank articles for a user and return top N.",
    "        ",
    "        Uses a heap for efficient top-N selection.",
    "        \"\"\"",
    "        scored = [(-self.score_article(a, user), a) for a in articles]",
    "        # Use heap for efficient top-N",
    "        heapq.heapify(scored)",
    "        ",
    "        result = []",
    "        for _ in range(min(limit, len(scored))):",
    "            if scored:",
    "                _, article = heapq.heappop(scored)",
    "                result.append(article)",
    "        ",
    "        return result",
    "",
    "",
    "class FeedGeneratorService:",
    "    \"\"\"",
    "    Service for generating personalized user feeds.",
    "    Implements the hybrid caching strategy.",
    "    \"\"\"",
    "    ",
    "    CATEGORY_CACHE_TTL = 300  # 5 minutes",
    "    PUBLISHER_CACHE_TTL = 300  # 5 minutes",
    "    USER_FEED_CACHE_TTL = 120  # 2 minutes",
    "    CATEGORY_TOP_N = 100",
    "    PUBLISHER_RECENT_N = 50",
    "    ",
    "    def __init__(",
    "        self,",
    "        cache: SimpleCache,",
    "        ranking_service: RankingService,",
    "        articles_store: Dict[str, Article],",
    "        users_store: Dict[str, User]",
    "    ):",
    "        self.cache = cache",
    "        self.ranking_service = ranking_service",
    "        self.articles = articles_store",
    "        self.users = users_store",
    "        self._category_articles: Dict[str, List[str]] = defaultdict(list)",
    "        self._publisher_articles: Dict[str, List[str]] = defaultdict(list)",
    "    ",
    "    def add_article_to_indexes(self, article: Article) -> None:",
    "        \"\"\"Add article to category and publisher indexes.\"\"\"",
    "        # Add to category indexes",
    "        for category in article.categories:",
    "            self._category_articles[category].append(article.id)",
    "            # Keep sorted by recency (in production, use sorted set)",
    "            self._invalidate_category_cache(category)",
    "        ",
    "        # Add to publisher index",
    "        self._publisher_articles[article.publisher_id].append(article.id)",
    "        self._invalidate_publisher_cache(article.publisher_id)",
    "    ",
    "    def _invalidate_category_cache(self, category: str) -> None:",
    "        self.cache.invalidate(f\"articles:{category}:top{self.CATEGORY_TOP_N}\")",
    "    ",
    "    def _invalidate_publisher_cache(self, publisher_id: str) -> None:",
    "        self.cache.invalidate(f\"publisher:{publisher_id}:recent{self.PUBLISHER_RECENT_N}\")",
    "    ",
    "    def invalidate_user_feed(self, user_id: str) -> None:",
    "        \"\"\"Invalidate all cached pages for a user.\"\"\"",
    "        self.cache.invalidate_prefix(f\"feed:{user_id}:\")",
    "    ",
    "    def get_category_articles(self, category: str) -> List[Article]:",
    "        \"\"\"Get top articles for a category (cached).\"\"\"",
    "        cache_key = f\"articles:{category}:top{self.CATEGORY_TOP_N}\"",
    "        ",
    "        cached = self.cache.get(cache_key)",
    "        if cached:",
    "            return cached",
    "        ",
    "        # Compute and cache",
    "        article_ids = self._category_articles.get(category, [])",
    "        articles = [self.articles[aid] for aid in article_ids if aid in self.articles]",
    "        # Sort by recency",
    "        articles.sort(key=lambda a: a.published_at, reverse=True)",
    "        articles = articles[:self.CATEGORY_TOP_N]",
    "        ",
    "        self.cache.set(cache_key, articles, self.CATEGORY_CACHE_TTL)",
    "        return articles",
    "    ",
    "    def get_publisher_articles(self, publisher_id: str) -> List[Article]:",
    "        \"\"\"Get recent articles for a publisher (cached).\"\"\"",
    "        cache_key = f\"publisher:{publisher_id}:recent{self.PUBLISHER_RECENT_N}\"",
    "        ",
    "        cached = self.cache.get(cache_key)",
    "        if cached:",
    "            return cached",
    "        ",
    "        # Compute and cache",
    "        article_ids = self._publisher_articles.get(publisher_id, [])",
    "        articles = [self.articles[aid] for aid in article_ids if aid in self.articles]",
    "        articles.sort(key=lambda a: a.published_at, reverse=True)",
    "        articles = articles[:self.PUBLISHER_RECENT_N]",
    "        ",
    "        self.cache.set(cache_key, articles, self.PUBLISHER_CACHE_TTL)",
    "        return articles",
    "    ",
    "    def get_trending_articles(self, limit: int = 50) -> List[Article]:",
    "        \"\"\"Get globally trending articles (for cold start).\"\"\"",
    "        cache_key = \"articles:trending\"",
    "        ",
    "        cached = self.cache.get(cache_key)",
    "        if cached:",
    "            return cached[:limit]",
    "        ",
    "        # Get recent articles sorted by popularity",
    "        cutoff = datetime.now() - timedelta(hours=24)",
    "        recent = [a for a in self.articles.values() if a.published_at > cutoff]",
    "        recent.sort(key=lambda a: a.likes, reverse=True)",
    "        trending = recent[:100]",
    "        ",
    "        self.cache.set(cache_key, trending, 60)  # 1 minute TTL for trending",
    "        return trending[:limit]",
    "    ",
    "    def generate_feed(",
    "        self,",
    "        user_id: str,",
    "        page: int = 0,",
    "        page_size: int = 20",
    "    ) -> List[Article]:",
    "        \"\"\"",
    "        Generate personalized feed for a user.",
    "        ",
    "        Args:",
    "            user_id: The user requesting the feed",
    "            page: Page number (0-indexed)",
    "            page_size: Number of articles per page",
    "            ",
    "        Returns:",
    "            List of articles personalized for the user",
    "        \"\"\"",
    "        # Check user feed cache first",
    "        cache_key = f\"feed:{user_id}:page{page}\"",
    "        cached = self.cache.get(cache_key)",
    "        if cached:",
    "            return cached",
    "        ",
    "        # Get user profile",
    "        user = self.users.get(user_id)",
    "        if not user:",
    "            user = User(id=user_id)  # Create default user",
    "            self.users[user_id] = user",
    "        ",
    "        # Cold start: no preferences",
    "        if not user.followed_publishers and not user.interests:",
    "            trending = self.get_trending_articles(page_size * (page + 1))",
    "            result = trending[page * page_size:(page + 1) * page_size]",
    "            self.cache.set(cache_key, result, self.USER_FEED_CACHE_TTL)",
    "            return result",
    "        ",
    "        # Merge candidates from category and publisher caches",
    "        candidates: Set[Article] = set()",
    "        ",
    "        # Add articles from user's interest categories",
    "        for category in user.interests:",
    "            category_articles = self.get_category_articles(category)",
    "            candidates.update(category_articles)",
    "        ",
    "        # Add articles from followed publishers",
    "        for publisher_id in user.followed_publishers:",
    "            publisher_articles = self.get_publisher_articles(publisher_id)",
    "            candidates.update(publisher_articles)",
    "        ",
    "        if not candidates:",
    "            # Fallback to trending",
    "            trending = self.get_trending_articles(page_size * (page + 1))",
    "            result = trending[page * page_size:(page + 1) * page_size]",
    "            self.cache.set(cache_key, result, self.USER_FEED_CACHE_TTL)",
    "            return result",
    "        ",
    "        # Score and rank",
    "        ranked = self.ranking_service.rank_articles(",
    "            list(candidates),",
    "            user,",
    "            limit=page_size * (page + 2)  # Fetch a bit more for pagination",
    "        )",
    "        ",
    "        # Paginate",
    "        start = page * page_size",
    "        end = start + page_size",
    "        result = ranked[start:end]",
    "        ",
    "        # Cache the result",
    "        self.cache.set(cache_key, result, self.USER_FEED_CACHE_TTL)",
    "        ",
    "        return result",
    "",
    "",
    "class NewsAggregator:",
    "    \"\"\"",
    "    Main News Aggregator System.",
    "    ",
    "    Provides the public API for:",
    "    - Registering publishers",
    "    - Fetching articles",
    "    - User preferences (follows, interests)",
    "    - Feed generation",
    "    - Engagement tracking",
    "    \"\"\"",
    "    ",
    "    def __init__(self):",
    "        \"\"\"Initialize the news aggregator system.\"\"\"",
    "        # Storage",
    "        self._publishers: Dict[str, Publisher] = {}",
    "        self._articles: Dict[str, Article] = {}",
    "        self._users: Dict[str, User] = {}",
    "        self._article_url_index: Dict[str, str] = {}  # url -> article_id (dedup)",
    "        ",
    "        # Services",
    "        self._cache = SimpleCache()",
    "        self._ranking_service = RankingService()",
    "        self._feed_generator = FeedGeneratorService(",
    "            cache=self._cache,",
    "            ranking_service=self._ranking_service,",
    "            articles_store=self._articles,",
    "            users_store=self._users",
    "        )",
    "        ",
    "        # Metrics (in production, use proper metrics library)",
    "        self._article_count = 0",
    "    ",
    "    def register_publisher(",
    "        self,",
    "        publisher_id: str,",
    "        name: str,",
    "        rss_url: str,",
    "        categories: List[str]",
    "    ) -> None:",
    "        \"\"\"",
    "        Register a new news publisher.",
    "        ",
    "        Args:",
    "            publisher_id: Unique identifier for the publisher",
    "            name: Display name",
    "            rss_url: RSS feed or API endpoint URL",
    "            categories: Content categories this publisher covers",
    "            ",
    "        Raises:",
    "            ValueError: If publisher_id already exists or URL is invalid",
    "        \"\"\"",
    "        if publisher_id in self._publishers:",
    "            raise ValueError(f\"Publisher {publisher_id} already exists\")",
    "        ",
    "        if not rss_url or not rss_url.startswith(('http://', 'https://')):",
    "            raise ValueError(f\"Invalid RSS URL: {rss_url}\")",
    "        ",
    "        publisher = Publisher(",
    "            id=publisher_id,",
    "            name=name,",
    "            rss_url=rss_url,",
    "            categories=categories",
    "        )",
    "        ",
    "        self._publishers[publisher_id] = publisher",
    "    ",
    "    def fetch_articles(self, publisher_id: str) -> List[Article]:",
    "        \"\"\"",
    "        Fetch and store articles from a publisher.",
    "        ",
    "        In production, this would be called by crawler workers.",
    "        Here we simulate article ingestion.",
    "        ",
    "        Args:",
    "            publisher_id: Publisher to fetch from",
    "            ",
    "        Returns:",
    "            List of newly ingested articles",
    "        \"\"\"",
    "        if publisher_id not in self._publishers:",
    "            raise ValueError(f\"Unknown publisher: {publisher_id}\")",
    "        ",
    "        publisher = self._publishers[publisher_id]",
    "        ",
    "        # In production: Parse RSS, extract articles",
    "        # Here we simulate with test data",
    "        new_articles = []",
    "        ",
    "        # Simulated article ingestion would happen here",
    "        # For demo, we'll add articles via ingest_article method",
    "        ",
    "        return new_articles",
    "    ",
    "    def ingest_article(",
    "        self,",
    "        publisher_id: str,",
    "        title: str,",
    "        url: str,",
    "        categories: Optional[List[str]] = None,",
    "        published_at: Optional[datetime] = None,",
    "        likes: int = 0",
    "    ) -> Optional[Article]:",
    "        \"\"\"",
    "        Ingest a single article (with deduplication).",
    "        ",
    "        Args:",
    "            publisher_id: Source publisher",
    "            title: Article title",
    "            url: Article URL (used for deduplication)",
    "            categories: Article categories (defaults to publisher's categories)",
    "            published_at: Publication time (defaults to now)",
    "            likes: Initial like count",
    "            ",
    "        Returns:",
    "            The article if new, None if duplicate",
    "        \"\"\"",
    "        # Check for duplicate by URL",
    "        if url in self._article_url_index:",
    "            return None  # Duplicate",
    "        ",
    "        if publisher_id not in self._publishers:",
    "            raise ValueError(f\"Unknown publisher: {publisher_id}\")",
    "        ",
    "        publisher = self._publishers[publisher_id]",
    "        ",
    "        # Generate article ID",
    "        self._article_count += 1",
    "        article_id = f\"article_{publisher_id}_{self._article_count}\"",
    "        ",
    "        article = Article(",
    "            id=article_id,",
    "            publisher_id=publisher_id,",
    "            title=title,",
    "            url=url,",
    "            categories=categories or publisher.categories,",
    "            published_at=published_at or datetime.now(),",
    "            likes=likes",
    "        )",
    "        ",
    "        # Store article",
    "        self._articles[article_id] = article",
    "        self._article_url_index[url] = article_id",
    "        ",
    "        # Update indexes for feed generation",
    "        self._feed_generator.add_article_to_indexes(article)",
    "        ",
    "        return article",
    "    ",
    "    def follow_publisher(self, user_id: str, publisher_id: str) -> None:",
    "        \"\"\"",
    "        Add publisher to user's followed list.",
    "        ",
    "        Args:",
    "            user_id: User following the publisher",
    "            publisher_id: Publisher to follow",
    "            ",
    "        Raises:",
    "            ValueError: If publisher doesn't exist",
    "        \"\"\"",
    "        if publisher_id not in self._publishers:",
    "            raise ValueError(f\"Unknown publisher: {publisher_id}\")",
    "        ",
    "        user = self._get_or_create_user(user_id)",
    "        user.followed_publishers.add(publisher_id)",
    "        ",
    "        # Invalidate cached feed",
    "        self._feed_generator.invalidate_user_feed(user_id)",
    "    ",
    "    def unfollow_publisher(self, user_id: str, publisher_id: str) -> None:",
    "        \"\"\"Remove publisher from user's followed list.\"\"\"",
    "        user = self._get_or_create_user(user_id)",
    "        user.followed_publishers.discard(publisher_id)",
    "        self._feed_generator.invalidate_user_feed(user_id)",
    "    ",
    "    def set_user_interests(self, user_id: str, categories: List[str]) -> None:",
    "        \"\"\"",
    "        Set user's interest categories.",
    "        ",
    "        Args:",
    "            user_id: User setting interests",
    "            categories: List of category names",
    "        \"\"\"",
    "        user = self._get_or_create_user(user_id)",
    "        user.interests = set(categories)",
    "        ",
    "        # Invalidate cached feed",
    "        self._feed_generator.invalidate_user_feed(user_id)",
    "    ",
    "    def like_article(self, user_id: str, article_id: str) -> None:",
    "        \"\"\"",
    "        Record user engagement (like) for an article.",
    "        ",
    "        Updates user's publisher affinity for personalization.",
    "        \"\"\"",
    "        if article_id not in self._articles:",
    "            raise ValueError(f\"Unknown article: {article_id}\")",
    "        ",
    "        article = self._articles[article_id]",
    "        article.likes += 1",
    "        ",
    "        user = self._get_or_create_user(user_id)",
    "        publisher_id = article.publisher_id",
    "        user.engagement_history[publisher_id] = (",
    "            user.engagement_history.get(publisher_id, 0) + 1",
    "        )",
    "        ",
    "        # Invalidate user feed to reflect new affinity",
    "        self._feed_generator.invalidate_user_feed(user_id)",
    "    ",
    "    def get_user_feed(",
    "        self,",
    "        user_id: str,",
    "        page: int = 0,",
    "        page_size: int = 20",
    "    ) -> List[Article]:",
    "        \"\"\"",
    "        Get personalized feed for a user.",
    "        ",
    "        Args:",
    "            user_id: User requesting the feed",
    "            page: Page number (0-indexed)",
    "            page_size: Number of articles per page",
    "            ",
    "        Returns:",
    "            List of articles personalized for the user",
    "        \"\"\"",
    "        return self._feed_generator.generate_feed(user_id, page, page_size)",
    "    ",
    "    def _get_or_create_user(self, user_id: str) -> User:",
    "        \"\"\"Get existing user or create new one.\"\"\"",
    "        if user_id not in self._users:",
    "            self._users[user_id] = User(id=user_id)",
    "        return self._users[user_id]",
    "    ",
    "    def get_stats(self) -> Dict:",
    "        \"\"\"Get system statistics.\"\"\"",
    "        return {",
    "            'publishers': len(self._publishers),",
    "            'articles': len(self._articles),",
    "            'users': len(self._users)",
    "        }",
    "",
    "",
    "def main():",
    "    \"\"\"Demo the News Aggregator System.\"\"\"",
    "    print(\"=\" * 60)",
    "    print(\"NEWS FEED AGGREGATOR SYSTEM - DEMO\")",
    "    print(\"=\" * 60)",
    "    print()",
    "    ",
    "    # Initialize system",
    "    aggregator = NewsAggregator()",
    "    print(\"\u2713 System initialized\")",
    "    print()",
    "    ",
    "    # Register publishers",
    "    print(\"--- Registering Publishers ---\")",
    "    aggregator.register_publisher(",
    "        \"pub_nyt\", \"NY Times\", ",
    "        \"https://nyt.com/rss\", ",
    "        [\"politics\", \"world\"]",
    "    )",
    "    aggregator.register_publisher(",
    "        \"pub_tc\", \"TechCrunch\",",
    "        \"https://techcrunch.com/rss\",",
    "        [\"technology\", \"startups\"]",
    "    )",
    "    aggregator.register_publisher(",
    "        \"pub_espn\", \"ESPN\",",
    "        \"https://espn.com/rss\",",
    "        [\"sports\"]",
    "    )",
    "    print(f\"\u2713 Registered {aggregator.get_stats()['publishers']} publishers\")",
    "    print()",
    "    ",
    "    # Ingest articles",
    "    print(\"--- Ingesting Articles ---\")",
    "    now = datetime.now()",
    "    ",
    "    # NYT articles",
    "    aggregator.ingest_article(",
    "        \"pub_nyt\", \"Breaking: Major Policy Change\",",
    "        \"https://nyt.com/article1\", [\"politics\"],",
    "        now - timedelta(hours=1), likes=5000",
    "    )",
    "    aggregator.ingest_article(",
    "        \"pub_nyt\", \"Global Summit Results\",",
    "        \"https://nyt.com/article2\", [\"world\"],",
    "        now - timedelta(hours=2), likes=3000",
    "    )",
    "    ",
    "    # TechCrunch articles",
    "    aggregator.ingest_article(",
    "        \"pub_tc\", \"AI Startup Raises $100M\",",
    "        \"https://tc.com/article1\", [\"technology\", \"startups\"],",
    "        now - timedelta(minutes=30), likes=2000",
    "    )",
    "    aggregator.ingest_article(",
    "        \"pub_tc\", \"New iPhone Features Leaked\",",
    "        \"https://tc.com/article2\", [\"technology\"],",
    "        now - timedelta(hours=3), likes=8000",
    "    )",
    "    aggregator.ingest_article(",
    "        \"pub_tc\", \"Cloud Computing Trends 2024\",",
    "        \"https://tc.com/article3\", [\"technology\"],",
    "        now - timedelta(hours=5), likes=1500",
    "    )",
    "    ",
    "    # ESPN articles",
    "    aggregator.ingest_article(",
    "        \"pub_espn\", \"NBA Finals Preview\",",
    "        \"https://espn.com/article1\", [\"sports\"],",
    "        now - timedelta(hours=1), likes=10000",
    "    )",
    "    aggregator.ingest_article(",
    "        \"pub_espn\", \"World Cup Qualifiers\",",
    "        \"https://espn.com/article2\", [\"sports\"],",
    "        now - timedelta(hours=4), likes=7000",
    "    )",
    "    ",
    "    print(f\"\u2713 Ingested {aggregator.get_stats()['articles']} articles\")",
    "    print()",
    "    ",
    "    # Test duplicate detection",
    "    print(\"--- Testing Deduplication ---\")",
    "    dup = aggregator.ingest_article(",
    "        \"pub_nyt\", \"Breaking: Major Policy Change (duplicate)\",",
    "        \"https://nyt.com/article1\",  # Same URL!",
    "        [\"politics\"], now",
    "    )",
    "    print(f\"\u2713 Duplicate detection: {'Blocked' if dup is None else 'FAILED'}\")",
    "    print()",
    "    ",
    "    # Setup User 1: Tech enthusiast",
    "    print(\"--- Setting Up User 1 (Tech Enthusiast) ---\")",
    "    aggregator.follow_publisher(\"user_1\", \"pub_tc\")",
    "    aggregator.set_user_interests(\"user_1\", [\"technology\", \"startups\"])",
    "    print(\"\u2713 User 1 follows: TechCrunch\")",
    "    print(\"\u2713 User 1 interests: technology, startups\")",
    "    print()",
    "    ",
    "    # Get feed for User 1",
    "    print(\"--- User 1's Feed ---\")",
    "    feed = aggregator.get_user_feed(\"user_1\", page=0, page_size=5)",
    "    for i, article in enumerate(feed, 1):",
    "        print(f\"  {i}. [{article.publisher_id}] {article.title}\")",
    "    print()",
    "    ",
    "    # Setup User 2: Sports + Politics",
    "    print(\"--- Setting Up User 2 (Sports & Politics) ---\")",
    "    aggregator.follow_publisher(\"user_2\", \"pub_espn\")",
    "    aggregator.follow_publisher(\"user_2\", \"pub_nyt\")",
    "    aggregator.set_user_interests(\"user_2\", [\"sports\", \"politics\"])",
    "    print(\"\u2713 User 2 follows: ESPN, NY Times\")",
    "    print(\"\u2713 User 2 interests: sports, politics\")",
    "    print()",
    "    ",
    "    # Get feed for User 2",
    "    print(\"--- User 2's Feed ---\")",
    "    feed = aggregator.get_user_feed(\"user_2\", page=0, page_size=5)",
    "    for i, article in enumerate(feed, 1):",
    "        print(f\"  {i}. [{article.publisher_id}] {article.title}\")",
    "    print()",
    "    ",
    "    # Test cold start (new user)",
    "    print(\"--- Cold Start: New User (No Preferences) ---\")",
    "    feed = aggregator.get_user_feed(\"new_user\", page=0, page_size=5)",
    "    print(\"Feed shows trending articles:\")",
    "    for i, article in enumerate(feed, 1):",
    "        print(f\"  {i}. [{article.publisher_id}] {article.title} ({article.likes} likes)\")",
    "    print()",
    "    ",
    "    # Test engagement-based personalization",
    "    print(\"--- Testing Engagement-Based Personalization ---\")",
    "    print(\"User 3 has interests in technology but no follows.\")",
    "    aggregator.set_user_interests(\"user_3\", [\"technology\"])",
    "    ",
    "    # Find an NYT article (to test if engagement boosts it)",
    "    # This is a bit contrived since NYT doesn't have tech articles in our demo",
    "    print(\"Before likes:\")",
    "    feed = aggregator.get_user_feed(\"user_3\", page=0, page_size=3)",
    "    for i, article in enumerate(feed, 1):",
    "        print(f\"  {i}. [{article.publisher_id}] {article.title}\")",
    "    print()",
    "    ",
    "    print(\"=\" * 60)",
    "    print(\"DEMO COMPLETE\")",
    "    print(\"=\" * 60)",
    "",
    "",
    "if __name__ == \"__main__\":",
    "    main()"
  ],
  "solution_java_lines": [
    "import java.time.Instant;",
    "import java.time.Duration;",
    "import java.util.*;",
    "import java.util.concurrent.ConcurrentHashMap;",
    "import java.util.stream.Collectors;",
    "",
    "/**",
    " * News Feed Aggregator System - Production-Quality Implementation",
    " * ",
    " * Features:",
    " * - Multi-level caching (category, publisher, user feeds)",
    " * - Personalized ranking based on follows, interests, engagement",
    " * - Cold start handling for new users",
    " * - Deduplication by URL",
    " */",
    "public class NewsFeedAggregator {",
    "    ",
    "    // ==================== DATA MODELS ====================",
    "    ",
    "    public static class Article {",
    "        public final String id;",
    "        public final String publisherId;",
    "        public final String title;",
    "        public final String url;",
    "        public final List<String> categories;",
    "        public final Instant publishedAt;",
    "        public int likes;",
    "        public final String contentHash;",
    "        ",
    "        public Article(String id, String publisherId, String title, String url,",
    "                      List<String> categories, Instant publishedAt, int likes) {",
    "            this.id = id;",
    "            this.publisherId = publisherId;",
    "            this.title = title;",
    "            this.url = url;",
    "            this.categories = categories;",
    "            this.publishedAt = publishedAt;",
    "            this.likes = likes;",
    "            this.contentHash = Integer.toHexString(url.hashCode());",
    "        }",
    "        ",
    "        @Override",
    "        public boolean equals(Object o) {",
    "            if (this == o) return true;",
    "            if (o == null || getClass() != o.getClass()) return false;",
    "            return id.equals(((Article) o).id);",
    "        }",
    "        ",
    "        @Override",
    "        public int hashCode() {",
    "            return id.hashCode();",
    "        }",
    "    }",
    "    ",
    "    public static class Publisher {",
    "        public final String id;",
    "        public final String name;",
    "        public final String rssUrl;",
    "        public final List<String> categories;",
    "        public boolean isActive = true;",
    "        ",
    "        public Publisher(String id, String name, String rssUrl, List<String> categories) {",
    "            this.id = id;",
    "            this.name = name;",
    "            this.rssUrl = rssUrl;",
    "            this.categories = categories;",
    "        }",
    "    }",
    "    ",
    "    public static class User {",
    "        public final String id;",
    "        public final Set<String> followedPublishers = new HashSet<>();",
    "        public final Set<String> interests = new HashSet<>();",
    "        public final Map<String, Integer> engagementHistory = new HashMap<>();",
    "        ",
    "        public User(String id) {",
    "            this.id = id;",
    "        }",
    "        ",
    "        public double getPublisherAffinity(String publisherId) {",
    "            int likes = engagementHistory.getOrDefault(publisherId, 0);",
    "            if (likes == 0) return 0.0;",
    "            return Math.min(1.0, likes / 10.0);",
    "        }",
    "    }",
    "    ",
    "    // ==================== CACHE ====================",
    "    ",
    "    public static class CacheEntry<T> {",
    "        public final T data;",
    "        public final long expiresAt;",
    "        ",
    "        public CacheEntry(T data, long ttlMillis) {",
    "            this.data = data;",
    "            this.expiresAt = System.currentTimeMillis() + ttlMillis;",
    "        }",
    "        ",
    "        public boolean isValid() {",
    "            return System.currentTimeMillis() < expiresAt;",
    "        }",
    "    }",
    "    ",
    "    public static class SimpleCache {",
    "        private final Map<String, CacheEntry<?>> store = new ConcurrentHashMap<>();",
    "        ",
    "        @SuppressWarnings(\"unchecked\")",
    "        public <T> T get(String key) {",
    "            CacheEntry<?> entry = store.get(key);",
    "            if (entry != null && entry.isValid()) {",
    "                return (T) entry.data;",
    "            }",
    "            if (entry != null) {",
    "                store.remove(key);",
    "            }",
    "            return null;",
    "        }",
    "        ",
    "        public <T> void set(String key, T value, long ttlMillis) {",
    "            store.put(key, new CacheEntry<>(value, ttlMillis));",
    "        }",
    "        ",
    "        public void invalidate(String key) {",
    "            store.remove(key);",
    "        }",
    "        ",
    "        public void invalidatePrefix(String prefix) {",
    "            store.keySet().removeIf(k -> k.startsWith(prefix));",
    "        }",
    "    }",
    "    ",
    "    // ==================== RANKING SERVICE ====================",
    "    ",
    "    public static class RankingService {",
    "        private static final double PUBLISHER_FOLLOW_WEIGHT = 2.0;",
    "        private static final double CATEGORY_INTEREST_WEIGHT = 1.5;",
    "        private static final double ENGAGEMENT_WEIGHT = 1.0;",
    "        private static final double RECENCY_WEIGHT = 0.8;",
    "        private static final double POPULARITY_WEIGHT = 0.3;",
    "        ",
    "        public double scoreArticle(Article article, User user) {",
    "            double score = 0.0;",
    "            ",
    "            // Factor 1: Publisher match",
    "            if (user.followedPublishers.contains(article.publisherId)) {",
    "                score += PUBLISHER_FOLLOW_WEIGHT;",
    "            }",
    "            ",
    "            // Factor 2: Category match",
    "            long matchingCategories = article.categories.stream()",
    "                .filter(user.interests::contains).count();",
    "            score += CATEGORY_INTEREST_WEIGHT * matchingCategories;",
    "            ",
    "            // Factor 3: Engagement affinity",
    "            score += ENGAGEMENT_WEIGHT * user.getPublisherAffinity(article.publisherId);",
    "            ",
    "            // Factor 4: Recency",
    "            long hoursOld = Duration.between(article.publishedAt, Instant.now()).toHours();",
    "            double recencyScore = Math.max(0.0, 1.0 - (hoursOld / 24.0));",
    "            score += RECENCY_WEIGHT * recencyScore;",
    "            ",
    "            // Factor 5: Popularity",
    "            if (article.likes > 0) {",
    "                double popularityScore = Math.min(1.0, Math.log10(article.likes + 1) / 5.0);",
    "                score += POPULARITY_WEIGHT * popularityScore;",
    "            }",
    "            ",
    "            return score;",
    "        }",
    "        ",
    "        public List<Article> rankArticles(List<Article> articles, User user, int limit) {",
    "            return articles.stream()",
    "                .sorted((a, b) -> Double.compare(scoreArticle(b, user), scoreArticle(a, user)))",
    "                .limit(limit)",
    "                .collect(Collectors.toList());",
    "        }",
    "    }",
    "    ",
    "    // ==================== MAIN AGGREGATOR ====================",
    "    ",
    "    private final Map<String, Publisher> publishers = new ConcurrentHashMap<>();",
    "    private final Map<String, Article> articles = new ConcurrentHashMap<>();",
    "    private final Map<String, User> users = new ConcurrentHashMap<>();",
    "    private final Map<String, String> articleUrlIndex = new ConcurrentHashMap<>();",
    "    private final Map<String, List<String>> categoryArticles = new ConcurrentHashMap<>();",
    "    private final Map<String, List<String>> publisherArticles = new ConcurrentHashMap<>();",
    "    ",
    "    private final SimpleCache cache = new SimpleCache();",
    "    private final RankingService rankingService = new RankingService();",
    "    ",
    "    private int articleCounter = 0;",
    "    ",
    "    private static final long CATEGORY_CACHE_TTL = 300_000; // 5 min",
    "    private static final long USER_FEED_CACHE_TTL = 120_000; // 2 min",
    "    private static final int CATEGORY_TOP_N = 100;",
    "    private static final int PUBLISHER_RECENT_N = 50;",
    "    ",
    "    public void registerPublisher(String publisherId, String name,",
    "                                  String rssUrl, List<String> categories) {",
    "        if (publishers.containsKey(publisherId)) {",
    "            throw new IllegalArgumentException(\"Publisher already exists: \" + publisherId);",
    "        }",
    "        if (rssUrl == null || (!rssUrl.startsWith(\"http://\") && !rssUrl.startsWith(\"https://\"))) {",
    "            throw new IllegalArgumentException(\"Invalid RSS URL: \" + rssUrl);",
    "        }",
    "        publishers.put(publisherId, new Publisher(publisherId, name, rssUrl, categories));",
    "    }",
    "    ",
    "    public Article ingestArticle(String publisherId, String title, String url,",
    "                                 List<String> categories, Instant publishedAt, int likes) {",
    "        // Deduplication",
    "        if (articleUrlIndex.containsKey(url)) {",
    "            return null;",
    "        }",
    "        ",
    "        if (!publishers.containsKey(publisherId)) {",
    "            throw new IllegalArgumentException(\"Unknown publisher: \" + publisherId);",
    "        }",
    "        ",
    "        Publisher publisher = publishers.get(publisherId);",
    "        String articleId = \"article_\" + publisherId + \"_\" + (++articleCounter);",
    "        ",
    "        List<String> articleCategories = categories != null ? categories : publisher.categories;",
    "        Article article = new Article(articleId, publisherId, title, url,",
    "            articleCategories, publishedAt != null ? publishedAt : Instant.now(), likes);",
    "        ",
    "        articles.put(articleId, article);",
    "        articleUrlIndex.put(url, articleId);",
    "        ",
    "        // Update indexes",
    "        for (String category : articleCategories) {",
    "            categoryArticles.computeIfAbsent(category, k -> new ArrayList<>()).add(articleId);",
    "            cache.invalidate(\"articles:\" + category + \":top\" + CATEGORY_TOP_N);",
    "        }",
    "        publisherArticles.computeIfAbsent(publisherId, k -> new ArrayList<>()).add(articleId);",
    "        cache.invalidate(\"publisher:\" + publisherId + \":recent\" + PUBLISHER_RECENT_N);",
    "        ",
    "        return article;",
    "    }",
    "    ",
    "    public void followPublisher(String userId, String publisherId) {",
    "        if (!publishers.containsKey(publisherId)) {",
    "            throw new IllegalArgumentException(\"Unknown publisher: \" + publisherId);",
    "        }",
    "        User user = getOrCreateUser(userId);",
    "        user.followedPublishers.add(publisherId);",
    "        cache.invalidatePrefix(\"feed:\" + userId + \":\");",
    "    }",
    "    ",
    "    public void setUserInterests(String userId, List<String> categories) {",
    "        User user = getOrCreateUser(userId);",
    "        user.interests.clear();",
    "        user.interests.addAll(categories);",
    "        cache.invalidatePrefix(\"feed:\" + userId + \":\");",
    "    }",
    "    ",
    "    public void likeArticle(String userId, String articleId) {",
    "        Article article = articles.get(articleId);",
    "        if (article == null) {",
    "            throw new IllegalArgumentException(\"Unknown article: \" + articleId);",
    "        }",
    "        article.likes++;",
    "        ",
    "        User user = getOrCreateUser(userId);",
    "        user.engagementHistory.merge(article.publisherId, 1, Integer::sum);",
    "        cache.invalidatePrefix(\"feed:\" + userId + \":\");",
    "    }",
    "    ",
    "    public List<Article> getUserFeed(String userId, int page, int pageSize) {",
    "        String cacheKey = \"feed:\" + userId + \":page\" + page;",
    "        List<Article> cached = cache.get(cacheKey);",
    "        if (cached != null) {",
    "            return cached;",
    "        }",
    "        ",
    "        User user = getOrCreateUser(userId);",
    "        ",
    "        // Cold start",
    "        if (user.followedPublishers.isEmpty() && user.interests.isEmpty()) {",
    "            List<Article> trending = getTrendingArticles(pageSize * (page + 1));",
    "            int start = page * pageSize;",
    "            int end = Math.min(start + pageSize, trending.size());",
    "            List<Article> result = trending.subList(start, end);",
    "            cache.set(cacheKey, new ArrayList<>(result), USER_FEED_CACHE_TTL);",
    "            return result;",
    "        }",
    "        ",
    "        // Merge candidates",
    "        Set<Article> candidates = new HashSet<>();",
    "        for (String category : user.interests) {",
    "            candidates.addAll(getCategoryArticles(category));",
    "        }",
    "        for (String pubId : user.followedPublishers) {",
    "            candidates.addAll(getPublisherArticles(pubId));",
    "        }",
    "        ",
    "        if (candidates.isEmpty()) {",
    "            List<Article> trending = getTrendingArticles(pageSize * (page + 1));",
    "            int start = page * pageSize;",
    "            int end = Math.min(start + pageSize, trending.size());",
    "            List<Article> result = trending.subList(start, end);",
    "            cache.set(cacheKey, new ArrayList<>(result), USER_FEED_CACHE_TTL);",
    "            return result;",
    "        }",
    "        ",
    "        // Rank",
    "        List<Article> ranked = rankingService.rankArticles(",
    "            new ArrayList<>(candidates), user, pageSize * (page + 2));",
    "        ",
    "        // Paginate",
    "        int start = page * pageSize;",
    "        int end = Math.min(start + pageSize, ranked.size());",
    "        List<Article> result = ranked.subList(start, end);",
    "        ",
    "        cache.set(cacheKey, new ArrayList<>(result), USER_FEED_CACHE_TTL);",
    "        return result;",
    "    }",
    "    ",
    "    private List<Article> getCategoryArticles(String category) {",
    "        String cacheKey = \"articles:\" + category + \":top\" + CATEGORY_TOP_N;",
    "        List<Article> cached = cache.get(cacheKey);",
    "        if (cached != null) return cached;",
    "        ",
    "        List<String> ids = categoryArticles.getOrDefault(category, Collections.emptyList());",
    "        List<Article> result = ids.stream()",
    "            .map(articles::get)",
    "            .filter(Objects::nonNull)",
    "            .sorted((a, b) -> b.publishedAt.compareTo(a.publishedAt))",
    "            .limit(CATEGORY_TOP_N)",
    "            .collect(Collectors.toList());",
    "        ",
    "        cache.set(cacheKey, result, CATEGORY_CACHE_TTL);",
    "        return result;",
    "    }",
    "    ",
    "    private List<Article> getPublisherArticles(String publisherId) {",
    "        String cacheKey = \"publisher:\" + publisherId + \":recent\" + PUBLISHER_RECENT_N;",
    "        List<Article> cached = cache.get(cacheKey);",
    "        if (cached != null) return cached;",
    "        ",
    "        List<String> ids = publisherArticles.getOrDefault(publisherId, Collections.emptyList());",
    "        List<Article> result = ids.stream()",
    "            .map(articles::get)",
    "            .filter(Objects::nonNull)",
    "            .sorted((a, b) -> b.publishedAt.compareTo(a.publishedAt))",
    "            .limit(PUBLISHER_RECENT_N)",
    "            .collect(Collectors.toList());",
    "        ",
    "        cache.set(cacheKey, result, CATEGORY_CACHE_TTL);",
    "        return result;",
    "    }",
    "    ",
    "    private List<Article> getTrendingArticles(int limit) {",
    "        Instant cutoff = Instant.now().minus(Duration.ofHours(24));",
    "        return articles.values().stream()",
    "            .filter(a -> a.publishedAt.isAfter(cutoff))",
    "            .sorted((a, b) -> Integer.compare(b.likes, a.likes))",
    "            .limit(limit)",
    "            .collect(Collectors.toList());",
    "    }",
    "    ",
    "    private User getOrCreateUser(String userId) {",
    "        return users.computeIfAbsent(userId, User::new);",
    "    }",
    "    ",
    "    // ==================== MAIN ====================",
    "    ",
    "    public static void main(String[] args) {",
    "        System.out.println(\"============================================================\");",
    "        System.out.println(\"NEWS FEED AGGREGATOR SYSTEM - DEMO\");",
    "        System.out.println(\"============================================================\");",
    "        System.out.println();",
    "        ",
    "        NewsFeedAggregator aggregator = new NewsFeedAggregator();",
    "        System.out.println(\"\u2713 System initialized\");",
    "        System.out.println();",
    "        ",
    "        // Register publishers",
    "        System.out.println(\"--- Registering Publishers ---\");",
    "        aggregator.registerPublisher(\"pub_nyt\", \"NY Times\",",
    "            \"https://nyt.com/rss\", Arrays.asList(\"politics\", \"world\"));",
    "        aggregator.registerPublisher(\"pub_tc\", \"TechCrunch\",",
    "            \"https://techcrunch.com/rss\", Arrays.asList(\"technology\", \"startups\"));",
    "        aggregator.registerPublisher(\"pub_espn\", \"ESPN\",",
    "            \"https://espn.com/rss\", Arrays.asList(\"sports\"));",
    "        System.out.println(\"\u2713 Registered 3 publishers\");",
    "        System.out.println();",
    "        ",
    "        // Ingest articles",
    "        System.out.println(\"--- Ingesting Articles ---\");",
    "        Instant now = Instant.now();",
    "        ",
    "        aggregator.ingestArticle(\"pub_nyt\", \"Breaking: Major Policy Change\",",
    "            \"https://nyt.com/article1\", Arrays.asList(\"politics\"),",
    "            now.minus(Duration.ofHours(1)), 5000);",
    "        aggregator.ingestArticle(\"pub_tc\", \"AI Startup Raises $100M\",",
    "            \"https://tc.com/article1\", Arrays.asList(\"technology\", \"startups\"),",
    "            now.minus(Duration.ofMinutes(30)), 2000);",
    "        aggregator.ingestArticle(\"pub_espn\", \"NBA Finals Preview\",",
    "            \"https://espn.com/article1\", Arrays.asList(\"sports\"),",
    "            now.minus(Duration.ofHours(1)), 10000);",
    "        System.out.println(\"\u2713 Ingested articles\");",
    "        System.out.println();",
    "        ",
    "        // Test deduplication",
    "        System.out.println(\"--- Testing Deduplication ---\");",
    "        Article dup = aggregator.ingestArticle(\"pub_nyt\", \"Duplicate\",",
    "            \"https://nyt.com/article1\", null, now, 0);",
    "        System.out.println(\"\u2713 Duplicate detection: \" + (dup == null ? \"Blocked\" : \"FAILED\"));",
    "        System.out.println();",
    "        ",
    "        // Setup user",
    "        System.out.println(\"--- Setting Up User 1 (Tech Enthusiast) ---\");",
    "        aggregator.followPublisher(\"user_1\", \"pub_tc\");",
    "        aggregator.setUserInterests(\"user_1\", Arrays.asList(\"technology\"));",
    "        System.out.println(\"\u2713 User 1 follows: TechCrunch\");",
    "        System.out.println(\"\u2713 User 1 interests: technology\");",
    "        System.out.println();",
    "        ",
    "        // Get feed",
    "        System.out.println(\"--- User 1's Feed ---\");",
    "        List<Article> feed = aggregator.getUserFeed(\"user_1\", 0, 5);",
    "        for (int i = 0; i < feed.size(); i++) {",
    "            Article a = feed.get(i);",
    "            System.out.println(\"  \" + (i+1) + \". [\" + a.publisherId + \"] \" + a.title);",
    "        }",
    "        System.out.println();",
    "        ",
    "        // Cold start",
    "        System.out.println(\"--- Cold Start: New User ---\");",
    "        List<Article> newUserFeed = aggregator.getUserFeed(\"new_user\", 0, 5);",
    "        System.out.println(\"Feed shows trending articles:\");",
    "        for (int i = 0; i < newUserFeed.size(); i++) {",
    "            Article a = newUserFeed.get(i);",
    "            System.out.println(\"  \" + (i+1) + \". [\" + a.publisherId + \"] \" + a.title);",
    "        }",
    "        System.out.println();",
    "        ",
    "        System.out.println(\"============================================================\");",
    "        System.out.println(\"DEMO COMPLETE\");",
    "        System.out.println(\"============================================================\");",
    "    }",
    "}"
  ],
  "code_walkthrough": [
    {
      "lines": "1-30",
      "section": "Data Models",
      "explanation": "We define three core entities: **Article** (the content), **Publisher** (the source), and **User** (the consumer). Article uses URL hash for deduplication. User tracks followed publishers, interests, and engagement history for personalization."
    },
    {
      "lines": "80-130",
      "section": "Cache Implementation",
      "explanation": "**SimpleCache** provides TTL-based caching. In production, this would be Redis. Key features: automatic expiration cleanup, prefix-based invalidation for user feeds. Cache entries track `expires_at` timestamp."
    },
    {
      "lines": "135-185",
      "section": "Ranking Service",
      "explanation": "The **scoring formula** is the heart of personalization: `score = 2.0*publisher_match + 1.5*category_match + 1.0*engagement + 0.8*recency + 0.3*popularity`. Higher weights for explicit signals (follows), lower for implicit (popularity)."
    },
    {
      "lines": "190-270",
      "section": "Feed Generation",
      "explanation": "**Hybrid approach**: First check user feed cache (fast path). On miss, merge articles from category caches and publisher caches, then score/rank. Cold start users get trending articles. Results are cached for 2 minutes."
    },
    {
      "lines": "280-350",
      "section": "Public API",
      "explanation": "Clean API methods: `registerPublisher`, `followPublisher`, `setUserInterests`, `likeArticle`, `getUserFeed`. Each mutation invalidates relevant caches. Deduplication by URL prevents duplicate articles."
    },
    {
      "lines": "360-420",
      "section": "Demo Main",
      "explanation": "Comprehensive demo showing: publisher registration, article ingestion, deduplication, user preference setup, personalized feed generation, and cold start handling."
    }
  ],
  "debugging_strategy": {
    "how_to_test_incrementally": "1. Test publisher registration alone\n2. Test article ingestion + dedup\n3. Test category/publisher caching\n4. Test cold start feed\n5. Test personalized feed\n6. Test cache invalidation on preference change",
    "what_to_print_or_assert": [
      "print(f\"Cache hit rate: {hits}/{total}\")",
      "assert len(feed) <= page_size",
      "assert all(a.publisher_id in user.followed_publishers or set(a.categories) & user.interests for a in feed)"
    ],
    "common_failure_modes": [
      "Cache not invalidated on preference change \u2192 stale feeds",
      "Duplicate articles appearing \u2192 dedup index not updated",
      "Cold start returning empty \u2192 trending articles not seeded",
      "Ranking scores all zero \u2192 user profile not found"
    ],
    "how_to_fix_fast": "1. Add logging at cache lookup/miss points\n2. Verify user profile is correctly loaded\n3. Check category/publisher indexes are populated\n4. Ensure cache invalidation is called on mutations"
  },
  "complexity_analysis": {
    "time": {
      "getUserFeed_cache_hit": {
        "complexity": "O(1)",
        "explanation": "Direct Redis/cache lookup"
      },
      "getUserFeed_cache_miss": {
        "complexity": "O(C\u00d7100 + P\u00d750 + n log k)",
        "explanation": "C=interests, P=follows, n=candidates, k=page_size. Merge cached lists + heap-based top-k selection"
      },
      "followPublisher": {
        "complexity": "O(1)",
        "explanation": "Set insertion + async cache invalidation"
      },
      "ingestArticle": {
        "complexity": "O(m)",
        "explanation": "m = number of categories (update each index)"
      },
      "overall": "Cache hits: O(1). Cache misses: O(100-500 articles) which is <10ms in practice"
    },
    "space": {
      "complexity": "O(A + U + C\u00d7100 + P\u00d750)",
      "breakdown": "- A = all articles in DB\n- U = all users\n- C\u00d7100 = category cache (20 categories \u00d7 100 articles)\n- P\u00d750 = publisher cache (1000 publishers \u00d7 50 articles)\n- User feed cache: O(active_users \u00d7 feed_size)",
      "note": "Dominant factor is article storage. Caches are bounded by design."
    },
    "can_we_do_better": "The hybrid approach is optimal for this use case. Pure push would require O(U) storage for feeds. Pure pull would have O(articles) latency per request. Hybrid balances both."
  },
  "dry_run": {
    "example": "registerPublisher(nyt), registerPublisher(tc), followPublisher(user_1, tc), setUserInterests(user_1, [tech]), ingestArticle(tc, 'AI News'), getUserFeed(user_1, 0, 10)",
    "trace_table": "| Step | Operation | Cache State | User State | Result |\n|------|-----------|-------------|------------|--------|\n| 1 | registerPublisher(nyt) | - | - | NYT added |\n| 2 | registerPublisher(tc) | - | - | TC added |\n| 3 | followPublisher(user_1, tc) | feed:user_1:* invalidated | follows: {tc} | User follows TC |\n| 4 | setUserInterests(user_1, [tech]) | feed:user_1:* invalidated | interests: {tech} | Interests set |\n| 5 | ingestArticle(tc, 'AI News') | articles:tech:top100 invalidated, publisher:tc:recent50 invalidated | - | Article stored |\n| 6 | getUserFeed(user_1, 0, 10) | MISS \u2192 compute \u2192 CACHE feed:user_1:page0 | - | Returns [AI News] |",
    "final_answer": "Feed contains 'AI News' article because user follows TechCrunch AND has tech interest"
  },
  "test_cases": [
    {
      "name": "Basic: Single publisher, single user",
      "category": "Happy Path",
      "input": "registerPublisher(nyt), followPublisher(u1, nyt), ingestArticle(nyt, 'News'), getUserFeed(u1)",
      "expected": "['News']",
      "explanation": "User follows NYT, so NYT articles appear in feed"
    },
    {
      "name": "Cold Start: New user with no preferences",
      "category": "Edge Case",
      "input": "ingestArticle(nyt, 'Article1', likes=1000), getUserFeed(new_user)",
      "expected": "['Article1']",
      "explanation": "New user with no preferences sees trending articles sorted by popularity"
    },
    {
      "name": "Deduplication: Same URL from multiple ingests",
      "category": "Dedup",
      "input": "ingestArticle(nyt, 'News', url='http://a'), ingestArticle(tc, 'News Copy', url='http://a')",
      "expected": "Second ingest returns None",
      "explanation": "Same URL = duplicate article, should be blocked"
    },
    {
      "name": "Interest vs Follow priority",
      "category": "Personalization",
      "input": "followPublisher(u1, espn), setUserInterests(u1, [tech]), ingestArticle(espn, 'Sports'), ingestArticle(tc, 'Tech')",
      "expected": "Feed shows both, with balanced ranking",
      "explanation": "User follows ESPN (sports) but interested in tech. Feed should include both."
    },
    {
      "name": "Cache invalidation on preference change",
      "category": "Cache",
      "input": "getUserFeed(u1) \u2192 cached, followPublisher(u1, newPub), getUserFeed(u1)",
      "expected": "Second call sees new publisher's articles",
      "explanation": "Following new publisher invalidates cached feed, triggering recomputation"
    },
    {
      "name": "Engagement boosts ranking",
      "category": "Personalization",
      "input": "setUserInterests(u1, [tech]), likeArticle(u1, tc_article) x5, getUserFeed(u1)",
      "expected": "TechCrunch articles ranked higher due to engagement affinity",
      "explanation": "Repeated likes for TC articles increase affinity score, boosting future TC articles"
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Computing full feed on every request without caching",
      "why_wrong": "With 100K QPS, DB would be overwhelmed. Cannot meet 200ms latency.",
      "correct_approach": "Use multi-level caching: category, publisher, and user feed caches",
      "code_wrong": "def get_feed(user_id):\n    articles = db.query('SELECT * FROM articles WHERE ...')  # Every time!",
      "code_correct": "def get_feed(user_id):\n    cached = cache.get(f'feed:{user_id}')\n    if cached: return cached  # Fast path\n    # Only compute on cache miss"
    },
    {
      "mistake": "Pre-computing feeds for ALL users (pure push)",
      "why_wrong": "10M users \u00d7 20 articles = 200M cached items. Storage explosion + wasted for inactive users.",
      "correct_approach": "Only cache feeds for active users. Use shared category/publisher caches for others.",
      "code_wrong": "def on_new_article(article):\n    for user in ALL_USERS:  # 10M users!\n        update_feed(user, article)",
      "code_correct": "# Cache category/publisher feeds (shared)\n# Compute user feeds on-demand, cache only for active users"
    },
    {
      "mistake": "Not invalidating cache on preference changes",
      "why_wrong": "User unfollows a publisher but still sees their articles in cached feed",
      "correct_approach": "Invalidate user's feed cache whenever preferences change",
      "code_wrong": "def unfollow(user_id, pub_id):\n    user.follows.remove(pub_id)  # Cache not invalidated!",
      "code_correct": "def unfollow(user_id, pub_id):\n    user.follows.remove(pub_id)\n    cache.invalidate_prefix(f'feed:{user_id}:')"
    },
    {
      "mistake": "Using complex ML ranking in real-time",
      "why_wrong": "ML inference for 200 articles would take >100ms, exceeding 200ms budget",
      "correct_approach": "Use simple weighted scoring for real-time. Batch ML for periodic model updates.",
      "code_wrong": "def rank(articles, user):\n    return ml_model.predict_scores(articles, user)  # Too slow!",
      "code_correct": "def rank(articles, user):\n    return sorted(articles, key=lambda a: simple_score(a, user))  # <10ms"
    }
  ],
  "interview_tips": {
    "opening": "Thank you for this problem. Before I dive into the design, I'd like to clarify a few requirements and share my understanding...",
    "clarifying_questions_to_ask": [
      "What defines personalization - just follows/interests, or also engagement history?",
      "Is the feed deterministic, or can we show slightly different orders to improve cache efficiency?",
      "How do we handle breaking news that needs immediate visibility?",
      "What's the geographic distribution - single region or multi-region?",
      "How do we handle publisher rate limiting or unavailability?"
    ],
    "what_to_mention_proactively": [
      "This is a read-heavy system (1700:1), so caching is critical",
      "I'll use a hybrid pull/push model to balance freshness and efficiency",
      "The ranking formula should be simple enough for real-time but effective",
      "Cold start handling is important for new user experience"
    ],
    "communication_during_coding": [
      "I'm designing a multi-level cache here - categories shared across users, feeds personalized",
      "This ranking score prioritizes explicit signals (follows) over implicit (popularity)",
      "Cache invalidation is triggered on any preference change to ensure consistency"
    ],
    "if_stuck": [
      "Step back: What's the bottleneck? High read QPS \u2192 need caching",
      "Ask: What can be shared vs personalized? Categories are shared, ranking is personal",
      "Draw the data flow: Publisher \u2192 Kafka \u2192 Storage \u2192 Cache \u2192 User"
    ],
    "time_management": "0-8min: Requirements & Clarification | 8-20min: High-level Architecture | 20-35min: Deep Dive (caching, ranking) | 35-45min: Data Model & API | 45-60min: Scaling & Edge Cases"
  },
  "pattern_recognition": {
    "pattern_name": "Fan-out on Read with Hybrid Caching",
    "indicators": [
      "High read QPS",
      "Personalized content",
      "Limited storage budget",
      "Freshness requirements"
    ],
    "similar_problems": [
      "Twitter Feed Design - Similar hybrid approach",
      "Instagram Explore - Category-based recommendations",
      "YouTube Recommendations - Multi-signal ranking",
      "LinkedIn Feed - Professional network personalization"
    ],
    "template": "1. Identify shared vs personalized components\n2. Cache shared components aggressively\n3. Compute personalized layer on-demand (or cache for active users)\n4. Use simple, fast ranking for real-time"
  },
  "follow_up_preparation": {
    "part_2_hint": "Part 2 adds real-time notifications for breaking news. You'll need WebSocket connections for live updates, a priority queue for urgent articles, and push notification service integration.",
    "part_3_hint": "Part 3 adds ML-based recommendations. Consider: feature extraction pipeline, model serving infrastructure, A/B testing framework, and offline vs online model updates.",
    "data_structure_evolution": "Part 1: HashMap + Cache \u2192 Part 2: Add WebSocket connections + Priority Queue \u2192 Part 3: Add ML feature store + model serving"
  },
  "communication_script": {
    "opening_verbatim": "Thank you for this problem. I see this is a news feed aggregator similar to Google News or Apple News. Before I start designing, I'd like to clarify a few points and share my initial understanding of the requirements...",
    "after_clarification": "Great, so to summarize: We need to handle 100K QPS reads with <200ms latency, 10M users, 1000 publishers, and provide personalized feeds based on follows and interests. My approach will be a hybrid caching strategy with rule-based ranking. Let me draw the high-level architecture...",
    "while_coding": [
      "I'm designing a three-tier cache here: category level, publisher level, and user feed level...",
      "This scoring function prioritizes followed publishers highest at 2.0 weight...",
      "Cache invalidation triggers here when user preferences change..."
    ],
    "after_coding": "Let me trace through a user request to verify the flow: User requests feed \u2192 check user cache \u2192 miss \u2192 fetch user profile \u2192 merge category caches for their interests \u2192 merge publisher caches for follows \u2192 score and rank \u2192 cache result \u2192 return. This should be under 200ms even on cache miss.",
    "when_stuck_verbatim": "I'm considering the trade-offs here... Let me think about what the main bottleneck is. With 100K QPS, we definitely need caching. The question is where to cache...",
    "after_mistake": "Actually, I realize I need to invalidate the user's cached feed when they follow a new publisher, otherwise they'd see stale content. Let me add that...",
    "before_moving_on": "This design handles the core requirements: 100K QPS with caching, <200ms latency with hybrid approach, personalization with weighted scoring, and cold start with trending fallback. The system can scale horizontally by adding API servers and sharding Redis by user_id. Ready for questions or Part 2?"
  },
  "interviewer_perspective": {
    "what_they_evaluate": [
      "Can candidate identify this as a read-heavy system and propose appropriate caching?",
      "Does candidate understand the trade-offs between pull vs push?",
      "Can candidate design a simple but effective ranking algorithm?",
      "Does candidate consider edge cases like cold start, cache invalidation?",
      "Can candidate estimate capacity and propose scaling strategies?"
    ],
    "bonus_points": [
      "Drawing a clear architecture diagram without being asked",
      "Mentioning specific technologies (Redis, Kafka) with rationale",
      "Calculating back-of-envelope numbers (cache size, QPS per server)",
      "Considering operational aspects (monitoring, alerting, graceful degradation)",
      "Discussing A/B testing for ranking algorithm improvements"
    ],
    "red_flags": [
      "Not considering caching for a 100K QPS system",
      "Proposing complex ML ranking without acknowledging latency concerns",
      "Ignoring cache invalidation on preference changes",
      "Not handling cold start for new users",
      "Unable to estimate system capacity"
    ],
    "what_differentiates_strong_candidates": "Strong candidates think about the problem from multiple angles: user experience (fast, fresh feeds), system efficiency (caching, compute costs), and operational reality (monitoring, failures). They ask smart clarifying questions, acknowledge trade-offs explicitly, and propose iterative solutions that can evolve."
  },
  "time_milestones": {
    "by_5_min": "Understand problem, identify it as read-heavy system needing caching, ask 2-3 clarifying questions",
    "by_10_min": "Have high-level architecture sketched: ingestion pipeline, storage layer, caching layer, API layer",
    "by_20_min": "Deep dive on caching strategy (multi-level caching) and feed generation algorithm",
    "by_30_min": "Cover ranking/scoring algorithm, data models, API design",
    "by_40_min": "Discuss scaling (sharding, replication), edge cases (cold start, dedup, cache invalidation)",
    "by_50_min": "Address follow-up questions, discuss production considerations",
    "warning_signs": "If still clarifying at 10 min or haven't discussed caching by 20 min, you're behind. Focus on the most important aspects: caching and feed generation."
  },
  "recovery_strategies": {
    "when_you_make_a_bug": "Say: 'Actually, I realize I forgot to invalidate the cache when user preferences change. Let me add that - it's critical for consistency.' Fix it and move on.",
    "when_you_dont_know_syntax": "Say: 'I don't remember the exact Redis command, but conceptually I need a sorted set with TTL. I'd look up the syntax in practice.'",
    "when_approach_is_wrong": "Say: 'I initially thought about pre-computing all user feeds, but that won't scale with 10M users. Let me reconsider with a hybrid approach...'",
    "when_completely_stuck": "Say: 'I'm not sure about the best way to handle X. Could you give me a hint about what trade-offs are acceptable?'",
    "when_running_out_of_time": "Say: 'I'm running low on time. Let me focus on the core design and mention what I'd add given more time: monitoring, A/B testing, graceful degradation.'"
  },
  "ai_copilot_tips": {
    "when_using_cursor_or_copilot": "In system design, AI is less directly useful. Use it for: generating example data models, scaffolding code structure, creating test scenarios.",
    "what_to_do": [
      "Use AI to generate data class definitions quickly",
      "Use for scaffolding API endpoint signatures",
      "Use for generating test cases and example data"
    ],
    "what_not_to_do": [
      "Don't ask AI to 'design the system' - that's what interviewer is evaluating",
      "Don't let AI generate architecture decisions without understanding them",
      "Don't rely on AI for capacity estimation - do the math yourself"
    ],
    "how_to_demonstrate_understanding": "After AI suggests something, explain WHY it works: 'The AI suggested Redis sorted sets here, and that's correct because we need efficient top-K retrieval with TTL support.'",
    "expectation_adjustment": "For system design, AI helps less with the creative design work. Focus on demonstrating YOUR understanding of trade-offs and design decisions."
  },
  "signal_points": {
    "wow_factors": [
      "Calculating cache hit rates and estimating latency distribution",
      "Drawing clear architecture diagrams with data flow arrows",
      "Mentioning operational concerns: monitoring, alerting, SLOs",
      "Proposing A/B testing framework for ranking improvements",
      "Discussing graceful degradation when components fail"
    ],
    "subtle_signals_of_experience": [
      "Mentioning cache stampede prevention",
      "Considering warm-up strategies after deployment",
      "Discussing schema evolution and backward compatibility",
      "Thinking about observability: logging, tracing, metrics"
    ]
  },
  "red_flags_to_avoid": {
    "behavioral": [
      "Jumping to architecture without understanding requirements",
      "Not asking clarifying questions",
      "Getting defensive when interviewer challenges a design decision",
      "Talking for 10+ minutes without checking in with interviewer"
    ],
    "technical": [
      "Not considering caching for high-QPS systems",
      "Ignoring consistency issues with distributed caches",
      "Proposing over-complex solutions (ML ranking for 200ms latency)",
      "Forgetting about failure modes and error handling"
    ],
    "communication": [
      "Using jargon without explaining (CAP theorem, eventual consistency) - define terms",
      "Going too deep into implementation details too early",
      "Not summarizing decisions before moving to next topic",
      "Talking about code before architecture is clear"
    ]
  },
  "final_checklist": {
    "before_saying_done": [
      "Did I address all functional requirements (register, fetch, feed, follow, interests, like)?",
      "Did I explain how 100K QPS is handled (caching strategy)?",
      "Did I show how <200ms latency is achieved (hybrid approach)?",
      "Did I handle edge cases (cold start, dedup, cache invalidation)?",
      "Did I discuss scaling strategy (horizontal scaling, sharding)?",
      "Did I mention monitoring and operational concerns?"
    ],
    "quick_code_review": [
      "Data models are clean and well-defined",
      "Cache keys are consistent and documented",
      "API methods have clear inputs/outputs",
      "Error handling for invalid inputs"
    ]
  },
  "production_considerations": {
    "what_id_add_in_production": [
      "**Rate limiting** for API and crawler endpoints",
      "**Circuit breakers** for external publisher APIs",
      "**Metrics and monitoring** for cache hit rates, latency percentiles, error rates",
      "**Distributed tracing** to debug slow requests",
      "**A/B testing framework** for ranking algorithm experiments",
      "**Feature flags** for gradual rollout of changes",
      "**Graceful degradation** - serve stale cache if fresh computation fails"
    ],
    "why_not_in_interview": "Focus on core design decisions. Mention these verbally to show senior thinking, but don't over-engineer the solution.",
    "how_to_mention": "Say: 'In production, I'd also add rate limiting, circuit breakers, and comprehensive monitoring. For this interview, I've focused on the core feed generation and caching logic.'"
  },
  "generated_at": "2026-01-18T18:46:38.155021",
  "_meta": {
    "problem_id": "news_feed_aggregator",
    "part_number": null,
    "model": "claude-opus-4-5-20251101"
  }
}