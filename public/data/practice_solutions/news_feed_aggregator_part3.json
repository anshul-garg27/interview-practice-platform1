{
  "problem_title": "News Feed Aggregator System - Part 3: Content Recommendation & ML Integration",
  "part_number": 3,
  "builds_on": "Part 2",
  "difficulty": "hard",
  "problem_understanding": {
    "what_changes": "Part 3 adds ML-powered recommendations beyond simple publisher/category matching. We need to track user engagement signals (clicks, read time, shares), compute user and article embeddings, and use similarity search to find personalized recommendations. This requires building a feature engineering pipeline, embedding computation, and a re-ranking layer for diversity.",
    "new_requirements": [
      "Record and store user engagement events (click, read, share, save) with duration",
      "Compute user embeddings from behavior history",
      "Compute article embeddings from content/metadata",
      "Perform fast similarity search using ANN for O(log N) recommendations",
      "Balance exploration (new content) vs exploitation (known preferences)",
      "Implement re-ranking for diversity, freshness, and business rules",
      "Support collaborative filtering via similar user identification"
    ],
    "new_constraints": [
      "getRecommendations must be O(log N) using ANN search - can't do full scan at 100K QPS",
      "Embeddings must be pre-computed offline - can't run neural nets at serve time",
      "Need to handle cold-start problem for new users/articles",
      "Must balance personalization with serendipity to avoid filter bubbles"
    ],
    "key_insight": "Pre-compute embeddings offline and store in vector database. At serve time, compute a lightweight user embedding from recent activity, then perform ANN lookup for similar articles. The two-tower architecture allows independent embedding of users and articles, enabling efficient dot-product similarity at inference time. This achieves <50ms latency even at 100K QPS."
  },
  "requirements_coverage": {
    "checklist": [
      {
        "requirement": "Record user engagement signals",
        "how_met": "recordEngagement() stores events in engagement_history with engagement type, duration, and timestamp",
        "gotchas": [
          "Don't forget to update real-time features when engagement occurs",
          "Dedup rapid clicks on same article"
        ]
      },
      {
        "requirement": "Compute user embeddings from behavior",
        "how_met": "_compute_user_embedding() aggregates engagement-weighted category vectors from articles user interacted with",
        "gotchas": [
          "Handle cold-start users with no history by using fallback to popular content",
          "Decay older engagements exponentially"
        ]
      },
      {
        "requirement": "Fast similarity search O(log N)",
        "how_met": "VectorIndex class with simulated ANN (Faiss/Annoy in production). Uses approximate nearest neighbor for sub-linear search",
        "gotchas": [
          "ANN is approximate - may miss some relevant results",
          "Need to rebuild index periodically as articles are added"
        ]
      },
      {
        "requirement": "Exploration vs Exploitation",
        "how_met": "Epsilon-greedy strategy - with probability epsilon, return random diverse articles; otherwise return top similarity matches",
        "gotchas": [
          "Set epsilon too high = poor relevance; too low = filter bubble",
          "Consider UCB or Thompson Sampling for better exploration"
        ]
      },
      {
        "requirement": "Re-ranking for diversity",
        "how_met": "_rerank_for_diversity() limits articles per category/publisher and boosts freshness",
        "gotchas": [
          "Don't over-diversify or you lose relevance signal",
          "Freshness boost should decay, not dominate"
        ]
      }
    ],
    "complexity_targets": [
      {
        "operation": "recordEngagement",
        "target": "O(1)",
        "achieved": "O(1)",
        "why": "Append to list and update counters - no complex computation at write time"
      },
      {
        "operation": "getRecommendations",
        "target": "O(log N)",
        "achieved": "O(K log N)",
        "why": "ANN search is O(log N) per query, we retrieve K candidates then re-rank in O(K log K)"
      },
      {
        "operation": "getSimilarUsers",
        "target": "O(log N)",
        "achieved": "O(log N)",
        "why": "ANN search on user embeddings stored in separate vector index"
      },
      {
        "operation": "trainModel (batch)",
        "target": "O(N*M)",
        "achieved": "O(N*M)",
        "why": "Batch job processes all users and articles - acceptable since it runs offline"
      }
    ],
    "non_goals": [
      "Real neural network training (simulated with category-based embeddings)",
      "Actual vector database integration (simulated with in-memory structures)",
      "Distributed training (single-node simulation)",
      "A/B testing framework (mentioned but not implemented)",
      "Real-time model updates (batch training only)"
    ]
  },
  "assumptions": [
    "Embedding dimension is 128 (industry standard for recommendation systems)",
    "Articles have pre-extracted categories for content-based features",
    "User has at least some engagement history (handle cold-start separately)",
    "Vector database (Faiss/Pinecone) is available in production - we simulate",
    "Model training happens daily as batch job, not real-time",
    "Interviewer is okay with simulated embeddings for demo purposes"
  ],
  "tradeoffs": [
    {
      "decision": "Pre-computed vs Real-time embeddings",
      "chosen": "Pre-computed with lightweight real-time updates",
      "why": "Can't run neural net inference at 100K QPS; pre-compute allows O(1) lookup",
      "alternative": "Full real-time inference",
      "when_to_switch": "If latency budget is 500ms+ and QPS is low"
    },
    {
      "decision": "ANN vs Exact KNN",
      "chosen": "ANN (Approximate Nearest Neighbor)",
      "why": "O(log N) vs O(N) - essential for scaling to millions of articles",
      "alternative": "Exact KNN with small candidate pool",
      "when_to_switch": "If article count < 10K and exact results are critical"
    },
    {
      "decision": "Epsilon-greedy vs Thompson Sampling for exploration",
      "chosen": "Epsilon-greedy",
      "why": "Simpler to implement and tune; good enough for interview context",
      "alternative": "Thompson Sampling or UCB",
      "when_to_switch": "For production with better regret bounds needed"
    },
    {
      "decision": "Two-tower vs Cross-encoder",
      "chosen": "Two-tower",
      "why": "Can pre-compute article embeddings; dot product at serve time is O(1)",
      "alternative": "Cross-encoder for higher accuracy",
      "when_to_switch": "For re-ranking top-K where accuracy matters more than speed"
    }
  ],
  "extensibility_notes": {
    "what_to_keep_stable": [
      "recordEngagement() signature - critical for logging pipeline",
      "getRecommendations() return type - clients depend on Article list",
      "Embedding dimension (128) - changing requires full retraining"
    ],
    "what_to_change": [
      "Added EngagementEvent dataclass for tracking interactions",
      "Added embedding fields to UserProfile and Article",
      "Added VectorIndex class for ANN search",
      "Added RecommendationEngine with ML pipeline"
    ],
    "interfaces_and_boundaries": "The RecommendationEngine is a separate component that can be swapped. VectorIndex is abstracted to allow Faiss/Pinecone/Milvus implementations. Feature Store interface allows batch + real-time feature serving.",
    "invariants": [
      "User embedding dimension == Article embedding dimension",
      "Engagement events are append-only (never deleted)",
      "ANN index is eventually consistent with article store",
      "Re-ranking never adds articles not in candidate set"
    ]
  },
  "visual_explanation": {
    "before_after": "```\n=== BEFORE (Part 2): Simple Feed Generation ===\n\nUser Request --> Fetch followed publishers --> Sort by time --> Return\n                          |\n                   [No personalization beyond follows]\n\n=== AFTER (Part 3): ML-Powered Recommendations ===\n\nUser Request\n     |\n     v\n+------------------+\n| Feature Lookup   | <-- Real-time: recent clicks, session context\n| (Feature Store)  | <-- Batch: user embedding, historical prefs\n+------------------+\n     |\n     v\n+------------------+\n| Candidate Gen    | <-- ANN search on article embeddings\n| (Vector Index)   | <-- Returns top 100-500 candidates in O(log N)\n+------------------+\n     |\n     v\n+------------------+\n| ML Ranking       | <-- Score each candidate: dot(user_emb, article_emb)\n| (Two-Tower)      | <-- Add freshness boost, publisher affinity\n+------------------+\n     |\n     v\n+------------------+\n| Re-ranking       | <-- Diversity: max 2 per category\n| (Business Rules) | <-- Exploration: inject 10% random articles\n+------------------+\n     |\n     v\n   Return Top K\n```",
    "algorithm_flow": "```\ngetRecommendations(user_id, count=20):\n\n[1] FEATURE LOOKUP (O(1))\n    +------------------------------------------+\n    | user_embedding = feature_store[user_id]  |\n    | If cold-start: use default embedding     |\n    +------------------------------------------+\n                    |\n                    v\n[2] CANDIDATE GENERATION (O(log N))\n    +------------------------------------------+\n    | candidates = vector_index.search(        |\n    |     user_embedding,                      |\n    |     k=100  # Retrieve 5x final count    |\n    | )                                        |\n    +------------------------------------------+\n                    |\n                    v\n[3] SCORING (O(K) where K=100)\n    +------------------------------------------+\n    | for article in candidates:               |\n    |     base_score = dot(user_emb, art_emb) |\n    |     freshness = decay(now - art.time)   |\n    |     score = base_score + 0.1*freshness  |\n    +------------------------------------------+\n                    |\n                    v\n[4] EXPLORATION (O(K))\n    +------------------------------------------+\n    | if random() < epsilon (0.1):            |\n    |     inject random diverse articles       |\n    +------------------------------------------+\n                    |\n                    v\n[5] RE-RANKING (O(K log K))\n    +------------------------------------------+\n    | Sort by score                            |\n    | Apply diversity: max 2 per category     |\n    | Apply business rules: no duplicates     |\n    +------------------------------------------+\n                    |\n                    v\n[6] RETURN Top 20 articles\n```"
  },
  "approaches": [
    {
      "name": "Naive Extension - Keyword Matching",
      "description": "Simply extend Part 2 by tracking engagement and using keyword matching to find similar articles. For each user, count category occurrences from history, then filter articles by top categories.",
      "time_complexity": "O(N) per recommendation - scan all articles",
      "space_complexity": "O(U*E) - all engagement events stored",
      "why_not_optimal": "Linear scan doesn't scale to millions of articles at 100K QPS. Keyword matching misses semantic similarity. No exploration mechanism leads to filter bubbles."
    },
    {
      "name": "Optimal - Two-Tower with ANN",
      "description": "Pre-compute embeddings for users and articles using a two-tower neural network. Store article embeddings in vector database with ANN index. At serve time, compute user embedding from recent activity, perform O(log N) ANN search, score candidates with dot product, and re-rank for diversity.",
      "time_complexity": "O(log N) for candidate retrieval, O(K log K) for re-ranking",
      "space_complexity": "O(N*D) for article embeddings, O(U*D) for user embeddings where D=128",
      "key_insight": "Two-tower architecture allows independent pre-computation. Article tower runs offline when articles are ingested. User tower can be approximated by aggregating recent engagement embeddings. Dot product similarity is O(D) which is constant for fixed D=128."
    }
  ],
  "optimal_solution": {
    "explanation_md": "## Optimal Solution: Two-Tower Recommendation with ANN\n\n### Architecture Overview\n\nThe solution uses a **two-tower neural network** architecture:\n\n1. **User Tower**: Encodes user features (history, preferences) into 128-dim embedding\n2. **Article Tower**: Encodes article features (content, categories) into 128-dim embedding\n3. **Similarity**: Dot product between user and article embeddings\n\n### Key Components\n\n#### 1. Engagement Recording (O(1))\n```\nrecordEngagement():\n  - Append to engagement_history\n  - Update real-time feature counters\n  - Trigger async embedding update if threshold reached\n```\n\n#### 2. Feature Engineering\n- **User Features**: Category preferences (weighted by engagement), recency decay, publisher affinity\n- **Article Features**: Category one-hot, publisher embedding, content TF-IDF, publish time\n\n#### 3. Embedding Computation\n- **Offline (Batch)**: Full neural network training on engagement data\n- **Online (Approximation)**: Weighted average of recently engaged article embeddings\n\n#### 4. ANN Search (O(log N))\nUsing **HNSW** (Hierarchical Navigable Small World) index:\n- Build time: O(N log N)\n- Query time: O(log N)\n- Recall: ~95% at 10x speedup vs brute force\n\n#### 5. Re-ranking\n```\nrerank(candidates):\n  - Score = dot(user_emb, article_emb) + freshness_boost\n  - Diversity: max 2 articles per category\n  - Exploration: 10% random injection\n  - Business rules: no repeat topics\n```\n\n### Cold Start Handling\n- **New User**: Use popular/trending articles, prompt for interests\n- **New Article**: Content-based embedding from title/body, boost initial exposure\n\n### Exploration vs Exploitation\nUsing **epsilon-greedy** with epsilon=0.1:\n- 90%: Return highest-scored articles (exploitation)\n- 10%: Inject random diverse articles (exploration)",
    "data_structures": [
      {
        "structure": "EngagementEvent",
        "purpose": "Record user interactions with timestamp and duration for training"
      },
      {
        "structure": "VectorIndex (ANN)",
        "purpose": "O(log N) similarity search on article embeddings using HNSW/Faiss"
      },
      {
        "structure": "FeatureStore (Dict[str, List[float]])",
        "purpose": "Store pre-computed user and article embeddings for O(1) lookup"
      },
      {
        "structure": "CategoryWeights (Dict[str, float])",
        "purpose": "Track user preference strength per category for embedding computation"
      },
      {
        "structure": "EngagementHistory (List[EngagementEvent])",
        "purpose": "Append-only log of all user interactions for batch training"
      }
    ],
    "algorithm_steps": [
      "Step 1: recordEngagement() appends event to user's engagement_history with O(1) write",
      "Step 2: Batch job runs trainModel() periodically to update all embeddings using full history",
      "Step 3: getRecommendations() retrieves user embedding from feature store in O(1)",
      "Step 4: If cold-start user, generate default embedding from popular categories",
      "Step 5: Query ANN index with user embedding to get top-K candidates in O(log N)",
      "Step 6: Score each candidate: base_score + freshness_boost + publisher_affinity",
      "Step 7: Apply exploration: with probability epsilon, inject random articles",
      "Step 8: Re-rank for diversity: limit per category/publisher, sort by final score",
      "Step 9: Return top requested count of articles"
    ]
  },
  "solution_python_lines": [
    "\"\"\"",
    "News Feed Aggregator - Part 3: ML-Powered Recommendations",
    "",
    "This module implements a two-tower recommendation system with:",
    "- User/Article embeddings (simulated for demo)",
    "- Approximate Nearest Neighbor search",
    "- Exploration/Exploitation via epsilon-greedy",
    "- Re-ranking for diversity and freshness",
    "\"\"\"",
    "",
    "from enum import Enum",
    "from typing import Dict, List, Optional, Set, Tuple, Any",
    "from dataclasses import dataclass, field",
    "from collections import defaultdict",
    "import heapq",
    "import time",
    "import math",
    "import random",
    "from abc import ABC, abstractmethod",
    "",
    "",
    "# ============================================================================",
    "# ENUMS AND CONSTANTS",
    "# ============================================================================",
    "",
    "class EngagementType(Enum):",
    "    \"\"\"Types of user engagement signals.\"\"\"",
    "    CLICK = 1",
    "    READ = 2",
    "    SHARE = 3",
    "    SAVE = 4",
    "",
    "",
    "# Engagement weights for computing user preferences",
    "ENGAGEMENT_WEIGHTS: Dict[EngagementType, float] = {",
    "    EngagementType.CLICK: 1.0,",
    "    EngagementType.READ: 3.0,",
    "    EngagementType.SHARE: 5.0,",
    "    EngagementType.SAVE: 4.0,",
    "}",
    "",
    "# Configuration constants",
    "EMBEDDING_DIM = 128",
    "EXPLORATION_EPSILON = 0.1  # 10% exploration rate",
    "FRESHNESS_DECAY_HOURS = 24.0",
    "MAX_PER_CATEGORY = 2  # Diversity constraint",
    "CANDIDATE_MULTIPLIER = 5  # Retrieve 5x final count for re-ranking",
    "",
    "",
    "# ============================================================================",
    "# DATA CLASSES",
    "# ============================================================================",
    "",
    "@dataclass",
    "class EngagementEvent:",
    "    \"\"\"Records a user engagement with an article.\"\"\"",
    "    user_id: str",
    "    article_id: str",
    "    engagement_type: EngagementType",
    "    duration_seconds: int",
    "    timestamp: float = field(default_factory=time.time)",
    "",
    "",
    "@dataclass",
    "class Article:",
    "    \"\"\"Represents a news article with content and ML embeddings.\"\"\"",
    "    article_id: str",
    "    title: str",
    "    content: str",
    "    publisher_id: str",
    "    categories: List[str]",
    "    timestamp: float",
    "    embedding: Optional[List[float]] = None",
    "",
    "    def __hash__(self):",
    "        return hash(self.article_id)",
    "",
    "    def __eq__(self, other):",
    "        if not isinstance(other, Article):",
    "            return False",
    "        return self.article_id == other.article_id",
    "",
    "",
    "@dataclass",
    "class UserProfile:",
    "    \"\"\"User profile with preferences and ML embeddings.\"\"\"",
    "    user_id: str",
    "    followed_publishers: Set[str] = field(default_factory=set)",
    "    interests: Dict[str, float] = field(default_factory=dict)",
    "    embedding: Optional[List[float]] = None",
    "    engagement_history: List[EngagementEvent] = field(default_factory=list)",
    "",
    "",
    "@dataclass",
    "class ScoredArticle:",
    "    \"\"\"Article with recommendation score for ranking.\"\"\"",
    "    article: Article",
    "    score: float",
    "    explanation: str = \"\"",
    "",
    "    def __lt__(self, other):",
    "        return self.score < other.score",
    "",
    "",
    "# ============================================================================",
    "# VECTOR INDEX (ANN SIMULATION)",
    "# ============================================================================",
    "",
    "class VectorIndex:",
    "    \"\"\"",
    "    Simulates an Approximate Nearest Neighbor index.",
    "    ",
    "    In production, use Faiss, Annoy, Pinecone, or Milvus.",
    "    This implementation uses brute-force for demo but maintains",
    "    the same interface for easy swapping.",
    "    \"\"\"",
    "    ",
    "    def __init__(self, dimension: int = EMBEDDING_DIM):",
    "        self.dimension = dimension",
    "        self.vectors: Dict[str, List[float]] = {}",
    "        self.items: Dict[str, Any] = {}",
    "    ",
    "    def add(self, item_id: str, vector: List[float], item: Any) -> None:",
    "        \"\"\"Add an item with its embedding vector.\"\"\"",
    "        if len(vector) != self.dimension:",
    "            raise ValueError(f\"Vector dimension mismatch: {len(vector)} vs {self.dimension}\")",
    "        self.vectors[item_id] = vector",
    "        self.items[item_id] = item",
    "    ",
    "    def search(self, query_vector: List[float], k: int) -> List[Tuple[str, float]]:",
    "        \"\"\"",
    "        Find k nearest neighbors to query vector.",
    "        ",
    "        Returns: List of (item_id, similarity_score) tuples",
    "        ",
    "        Note: In production with HNSW/Faiss, this is O(log N).",
    "        Our simulation is O(N) but interface is the same.",
    "        \"\"\"",
    "        if not self.vectors:",
    "            return []",
    "        ",
    "        scores = []",
    "        for item_id, vec in self.vectors.items():",
    "            sim = self._cosine_similarity(query_vector, vec)",
    "            scores.append((item_id, sim))",
    "        ",
    "        # Return top-k by similarity",
    "        scores.sort(key=lambda x: x[1], reverse=True)",
    "        return scores[:k]",
    "    ",
    "    def _cosine_similarity(self, v1: List[float], v2: List[float]) -> float:",
    "        \"\"\"Compute cosine similarity between two vectors.\"\"\"",
    "        dot_product = sum(a * b for a, b in zip(v1, v2))",
    "        norm1 = math.sqrt(sum(a * a for a in v1))",
    "        norm2 = math.sqrt(sum(b * b for b in v2))",
    "        if norm1 == 0 or norm2 == 0:",
    "            return 0.0",
    "        return dot_product / (norm1 * norm2)",
    "",
    "",
    "# ============================================================================",
    "# EMBEDDING GENERATOR (SIMULATED ML MODEL)",
    "# ============================================================================",
    "",
    "class EmbeddingGenerator:",
    "    \"\"\"",
    "    Generates embeddings for users and articles.",
    "    ",
    "    In production, this would be a trained neural network.",
    "    For demo, we use a deterministic category-based approach",
    "    that still demonstrates the core concepts.",
    "    \"\"\"",
    "    ",
    "    # Predefined category dimensions (simulating learned embeddings)",
    "    CATEGORY_VECTORS: Dict[str, int] = {",
    "        'tech': 0, 'technology': 0, 'ai': 1, 'science': 2,",
    "        'sports': 3, 'politics': 4, 'business': 5, 'finance': 5,",
    "        'entertainment': 6, 'health': 7, 'world': 8, 'local': 9,",
    "        'opinion': 10, 'lifestyle': 11, 'food': 12, 'travel': 13,",
    "    }",
    "    ",
    "    def __init__(self, dimension: int = EMBEDDING_DIM):",
    "        self.dimension = dimension",
    "        random.seed(42)  # Reproducible embeddings",
    "    ",
    "    def generate_article_embedding(self, article: Article) -> List[float]:",
    "        \"\"\"",
    "        Generate embedding vector for an article based on its categories.",
    "        ",
    "        In production: Use BERT/transformer on title+content.",
    "        For demo: Category-based with publisher influence.",
    "        \"\"\"",
    "        embedding = [0.0] * self.dimension",
    "        ",
    "        # Set dimensions based on categories",
    "        for category in article.categories:",
    "            cat_lower = category.lower()",
    "            if cat_lower in self.CATEGORY_VECTORS:",
    "                dim = self.CATEGORY_VECTORS[cat_lower]",
    "                embedding[dim] = 1.0",
    "                # Add some spread to neighboring dimensions",
    "                if dim > 0:",
    "                    embedding[dim - 1] = 0.3",
    "                if dim < self.dimension - 1:",
    "                    embedding[dim + 1] = 0.3",
    "        ",
    "        # Add publisher signal (hash to dimension)",
    "        pub_dim = hash(article.publisher_id) % (self.dimension // 2) + (self.dimension // 2)",
    "        embedding[pub_dim] = 0.5",
    "        ",
    "        # Normalize",
    "        return self._normalize(embedding)",
    "    ",
    "    def generate_user_embedding(",
    "        self,",
    "        user: UserProfile,",
    "        article_embeddings: Dict[str, List[float]]",
    "    ) -> List[float]:",
    "        \"\"\"",
    "        Generate user embedding from engagement history.",
    "        ",
    "        Uses weighted average of engaged article embeddings,",
    "        with weights based on engagement type and recency.",
    "        \"\"\"",
    "        if not user.engagement_history:",
    "            # Cold start: return interest-based embedding",
    "            return self._generate_cold_start_embedding(user)",
    "        ",
    "        embedding = [0.0] * self.dimension",
    "        total_weight = 0.0",
    "        current_time = time.time()",
    "        ",
    "        for event in user.engagement_history:",
    "            if event.article_id not in article_embeddings:",
    "                continue",
    "            ",
    "            article_emb = article_embeddings[event.article_id]",
    "            ",
    "            # Base weight from engagement type",
    "            weight = ENGAGEMENT_WEIGHTS.get(event.engagement_type, 1.0)",
    "            ",
    "            # Boost for read duration (>60s = highly engaged)",
    "            if event.duration_seconds > 60:",
    "                weight *= 1.5",
    "            elif event.duration_seconds < 10:",
    "                weight *= 0.5  # Quick bounce = low interest",
    "            ",
    "            # Recency decay (half-life of 7 days)",
    "            days_ago = (current_time - event.timestamp) / 86400",
    "            recency_factor = math.exp(-0.1 * days_ago)",
    "            weight *= recency_factor",
    "            ",
    "            # Add weighted article embedding",
    "            for i in range(self.dimension):",
    "                embedding[i] += article_emb[i] * weight",
    "            total_weight += weight",
    "        ",
    "        if total_weight > 0:",
    "            embedding = [e / total_weight for e in embedding]",
    "        ",
    "        return self._normalize(embedding)",
    "    ",
    "    def _generate_cold_start_embedding(self, user: UserProfile) -> List[float]:",
    "        \"\"\"Generate embedding for user with no history.\"\"\"",
    "        embedding = [0.1] * self.dimension  # Small baseline",
    "        ",
    "        # Add signal from declared interests",
    "        for interest, weight in user.interests.items():",
    "            interest_lower = interest.lower()",
    "            if interest_lower in self.CATEGORY_VECTORS:",
    "                dim = self.CATEGORY_VECTORS[interest_lower]",
    "                embedding[dim] = weight",
    "        ",
    "        return self._normalize(embedding)",
    "    ",
    "    def _normalize(self, vector: List[float]) -> List[float]:",
    "        \"\"\"L2 normalize a vector.\"\"\"",
    "        norm = math.sqrt(sum(x * x for x in vector))",
    "        if norm == 0:",
    "            return vector",
    "        return [x / norm for x in vector]",
    "",
    "",
    "# ============================================================================",
    "# RECOMMENDATION ENGINE",
    "# ============================================================================",
    "",
    "class RecommendationEngine:",
    "    \"\"\"",
    "    ML-powered recommendation engine using two-tower architecture.",
    "    ",
    "    Components:",
    "    - Embedding generator (simulated neural network)",
    "    - Vector index (ANN for fast retrieval)",
    "    - Re-ranking layer (diversity + freshness + exploration)",
    "    \"\"\"",
    "    ",
    "    def __init__(self):",
    "        self.embedding_generator = EmbeddingGenerator()",
    "        self.article_index = VectorIndex()",
    "        self.user_index = VectorIndex()",
    "        ",
    "        # Feature stores",
    "        self.article_embeddings: Dict[str, List[float]] = {}",
    "        self.user_embeddings: Dict[str, List[float]] = {}",
    "        ",
    "        # Article storage",
    "        self.articles: Dict[str, Article] = {}",
    "        ",
    "        # User profiles",
    "        self.users: Dict[str, UserProfile] = {}",
    "        ",
    "        # Category index for diversity sampling",
    "        self.articles_by_category: Dict[str, Set[str]] = defaultdict(set)",
    "    ",
    "    def add_article(self, article: Article) -> None:",
    "        \"\"\"Add article and compute its embedding.\"\"\"",
    "        # Generate embedding",
    "        embedding = self.embedding_generator.generate_article_embedding(article)",
    "        article.embedding = embedding",
    "        self.article_embeddings[article.article_id] = embedding",
    "        ",
    "        # Store article",
    "        self.articles[article.article_id] = article",
    "        ",
    "        # Update category index",
    "        for category in article.categories:",
    "            self.articles_by_category[category.lower()].add(article.article_id)",
    "        ",
    "        # Add to vector index",
    "        self.article_index.add(article.article_id, embedding, article)",
    "    ",
    "    def get_or_create_user(self, user_id: str) -> UserProfile:",
    "        \"\"\"Get existing user or create new profile.\"\"\"",
    "        if user_id not in self.users:",
    "            self.users[user_id] = UserProfile(user_id=user_id)",
    "        return self.users[user_id]",
    "    ",
    "    def record_engagement(",
    "        self,",
    "        user_id: str,",
    "        article_id: str,",
    "        engagement_type: EngagementType,",
    "        duration_seconds: int",
    "    ) -> None:",
    "        \"\"\"",
    "        Record user engagement with an article.",
    "        ",
    "        Time Complexity: O(1) - append only",
    "        Space Complexity: O(1) per event",
    "        \"\"\"",
    "        user = self.get_or_create_user(user_id)",
    "        ",
    "        event = EngagementEvent(",
    "            user_id=user_id,",
    "            article_id=article_id,",
    "            engagement_type=engagement_type,",
    "            duration_seconds=duration_seconds,",
    "        )",
    "        ",
    "        user.engagement_history.append(event)",
    "        ",
    "        # Update user interests based on article categories",
    "        if article_id in self.articles:",
    "            article = self.articles[article_id]",
    "            weight = ENGAGEMENT_WEIGHTS.get(engagement_type, 1.0)",
    "            # Boost for long reads",
    "            if duration_seconds > 60:",
    "                weight *= 1.5",
    "            ",
    "            for category in article.categories:",
    "                cat_lower = category.lower()",
    "                user.interests[cat_lower] = user.interests.get(cat_lower, 0) + weight",
    "        ",
    "        # Invalidate cached user embedding (will recompute on next request)",
    "        # In production, could do incremental update",
    "        self.user_embeddings.pop(user_id, None)",
    "    ",
    "    def _get_user_embedding(self, user_id: str) -> List[float]:",
    "        \"\"\"Get or compute user embedding.\"\"\"",
    "        if user_id in self.user_embeddings:",
    "            return self.user_embeddings[user_id]",
    "        ",
    "        user = self.get_or_create_user(user_id)",
    "        embedding = self.embedding_generator.generate_user_embedding(",
    "            user, self.article_embeddings",
    "        )",
    "        self.user_embeddings[user_id] = embedding",
    "        return embedding",
    "    ",
    "    def get_recommendations(",
    "        self,",
    "        user_id: str,",
    "        count: int",
    "    ) -> List[Article]:",
    "        \"\"\"",
    "        Get ML-powered article recommendations for a user.",
    "        ",
    "        Time Complexity: O(log N) for ANN search + O(K log K) for re-ranking",
    "        where N = total articles, K = candidate count",
    "        ",
    "        Args:",
    "            user_id: User identifier",
    "            count: Number of recommendations to return",
    "        ",
    "        Returns:",
    "            List of recommended articles, ranked by predicted relevance",
    "        \"\"\"",
    "        if not self.articles:",
    "            return []",
    "        ",
    "        # Step 1: Get user embedding",
    "        user_embedding = self._get_user_embedding(user_id)",
    "        user = self.get_or_create_user(user_id)",
    "        ",
    "        # Step 2: Get already-seen article IDs",
    "        seen_articles = {e.article_id for e in user.engagement_history}",
    "        ",
    "        # Step 3: Candidate retrieval via ANN",
    "        candidate_count = count * CANDIDATE_MULTIPLIER",
    "        candidates = self.article_index.search(user_embedding, candidate_count + len(seen_articles))",
    "        ",
    "        # Step 4: Score and filter candidates",
    "        scored_articles = []",
    "        current_time = time.time()",
    "        ",
    "        for article_id, similarity in candidates:",
    "            # Skip already seen",
    "            if article_id in seen_articles:",
    "                continue",
    "            ",
    "            article = self.articles.get(article_id)",
    "            if not article:",
    "                continue",
    "            ",
    "            # Base score from embedding similarity",
    "            score = similarity",
    "            ",
    "            # Freshness boost (exponential decay)",
    "            hours_old = (current_time - article.timestamp) / 3600",
    "            freshness_boost = math.exp(-hours_old / FRESHNESS_DECAY_HOURS) * 0.2",
    "            score += freshness_boost",
    "            ",
    "            # Publisher affinity (if following)",
    "            if article.publisher_id in user.followed_publishers:",
    "                score += 0.1",
    "            ",
    "            scored_articles.append(ScoredArticle(",
    "                article=article,",
    "                score=score,",
    "                explanation=f\"sim={similarity:.2f}, fresh={freshness_boost:.2f}\"",
    "            ))",
    "        ",
    "        # Step 5: Exploration - inject random diverse articles",
    "        if random.random() < EXPLORATION_EPSILON and len(scored_articles) > count:",
    "            scored_articles = self._inject_exploration(",
    "                scored_articles, user, seen_articles, count",
    "            )",
    "        ",
    "        # Step 6: Re-rank for diversity",
    "        final_articles = self._rerank_for_diversity(scored_articles, count)",
    "        ",
    "        return [sa.article for sa in final_articles]",
    "    ",
    "    def _inject_exploration(",
    "        self,",
    "        scored_articles: List[ScoredArticle],",
    "        user: UserProfile,",
    "        seen_articles: Set[str],",
    "        count: int",
    "    ) -> List[ScoredArticle]:",
    "        \"\"\"Inject random diverse articles for exploration.\"\"\"",
    "        # Find categories user hasn't engaged with much",
    "        user_categories = set(user.interests.keys())",
    "        all_categories = set(self.articles_by_category.keys())",
    "        unexplored = all_categories - user_categories",
    "        ",
    "        if not unexplored:",
    "            return scored_articles",
    "        ",
    "        # Pick random articles from unexplored categories",
    "        exploration_slots = max(1, count // 5)  # 20% exploration",
    "        exploration_articles = []",
    "        ",
    "        for category in list(unexplored)[:exploration_slots]:",
    "            article_ids = self.articles_by_category[category] - seen_articles",
    "            if article_ids:",
    "                random_id = random.choice(list(article_ids))",
    "                article = self.articles.get(random_id)",
    "                if article:",
    "                    exploration_articles.append(ScoredArticle(",
    "                        article=article,",
    "                        score=0.5,  # Fixed exploration score",
    "                        explanation=\"exploration\"",
    "                    ))",
    "        ",
    "        # Merge with original (replace some low-scoring ones)",
    "        if exploration_articles:",
    "            scored_articles = scored_articles[:-len(exploration_articles)] + exploration_articles",
    "        ",
    "        return scored_articles",
    "    ",
    "    def _rerank_for_diversity(",
    "        self,",
    "        scored_articles: List[ScoredArticle],",
    "        count: int",
    "    ) -> List[ScoredArticle]:",
    "        \"\"\"",
    "        Re-rank articles ensuring diversity constraints.",
    "        ",
    "        Constraints:",
    "        - Max 2 articles per category",
    "        - Max 2 articles per publisher",
    "        \"\"\"",
    "        if not scored_articles:",
    "            return []",
    "        ",
    "        # Sort by score descending",
    "        scored_articles.sort(key=lambda x: x.score, reverse=True)",
    "        ",
    "        result = []",
    "        category_counts: Dict[str, int] = defaultdict(int)",
    "        publisher_counts: Dict[str, int] = defaultdict(int)",
    "        ",
    "        for sa in scored_articles:",
    "            if len(result) >= count:",
    "                break",
    "            ",
    "            article = sa.article",
    "            ",
    "            # Check category constraint",
    "            category_ok = all(",
    "                category_counts[cat.lower()] < MAX_PER_CATEGORY",
    "                for cat in article.categories",
    "            )",
    "            ",
    "            # Check publisher constraint",
    "            publisher_ok = publisher_counts[article.publisher_id] < MAX_PER_CATEGORY",
    "            ",
    "            if category_ok and publisher_ok:",
    "                result.append(sa)",
    "                for cat in article.categories:",
    "                    category_counts[cat.lower()] += 1",
    "                publisher_counts[article.publisher_id] += 1",
    "        ",
    "        return result",
    "    ",
    "    def get_similar_users(",
    "        self,",
    "        user_id: str,",
    "        count: int",
    "    ) -> List[str]:",
    "        \"\"\"",
    "        Find users with similar preferences (collaborative filtering).",
    "        ",
    "        Time Complexity: O(log U) with ANN on user embeddings",
    "        \"\"\"",
    "        user_embedding = self._get_user_embedding(user_id)",
    "        ",
    "        # Add all users to index if not already",
    "        for uid in self.users:",
    "            if uid not in self.user_index.vectors:",
    "                emb = self._get_user_embedding(uid)",
    "                self.user_index.add(uid, emb, uid)",
    "        ",
    "        # Search for similar users (excluding self)",
    "        results = self.user_index.search(user_embedding, count + 1)",
    "        return [uid for uid, _ in results if uid != user_id][:count]",
    "    ",
    "    def train_model(self) -> None:",
    "        \"\"\"",
    "        Batch training job - recompute all embeddings.",
    "        ",
    "        In production, this would run as a daily Spark/Flink job.",
    "        Here we just refresh all embeddings.",
    "        \"\"\"",
    "        print(\"Training model...\")",
    "        ",
    "        # Recompute article embeddings",
    "        for article_id, article in self.articles.items():",
    "            embedding = self.embedding_generator.generate_article_embedding(article)",
    "            self.article_embeddings[article_id] = embedding",
    "            article.embedding = embedding",
    "        ",
    "        # Rebuild article index",
    "        self.article_index = VectorIndex()",
    "        for article_id, embedding in self.article_embeddings.items():",
    "            article = self.articles[article_id]",
    "            self.article_index.add(article_id, embedding, article)",
    "        ",
    "        # Recompute user embeddings",
    "        self.user_embeddings.clear()",
    "        for user_id in self.users:",
    "            self._get_user_embedding(user_id)",
    "        ",
    "        print(f\"Trained on {len(self.articles)} articles, {len(self.users)} users\")",
    "",
    "",
    "# ============================================================================",
    "# NEWS AGGREGATOR (MAIN CLASS)",
    "# ============================================================================",
    "",
    "class NewsAggregator:",
    "    \"\"\"",
    "    News Feed Aggregator with ML-powered recommendations.",
    "    ",
    "    Combines Parts 1-3:",
    "    - Part 1: Article fetching and feed generation",
    "    - Part 2: Real-time notifications (assumed working)",
    "    - Part 3: ML recommendations (this implementation)",
    "    \"\"\"",
    "    ",
    "    def __init__(self):",
    "        self.recommendation_engine = RecommendationEngine()",
    "    ",
    "    # ------------------------------------------------------------------",
    "    # ARTICLE MANAGEMENT",
    "    # ------------------------------------------------------------------",
    "    ",
    "    def add_article(",
    "        self,",
    "        article_id: str,",
    "        title: str,",
    "        content: str,",
    "        publisher_id: str,",
    "        categories: List[str]",
    "    ) -> None:",
    "        \"\"\"Add a new article to the system.\"\"\"",
    "        article = Article(",
    "            article_id=article_id,",
    "            title=title,",
    "            content=content,",
    "            publisher_id=publisher_id,",
    "            categories=categories,",
    "            timestamp=time.time(),",
    "        )",
    "        self.recommendation_engine.add_article(article)",
    "    ",
    "    # ------------------------------------------------------------------",
    "    # USER MANAGEMENT",
    "    # ------------------------------------------------------------------",
    "    ",
    "    def follow_publisher(self, user_id: str, publisher_id: str) -> None:",
    "        \"\"\"User follows a publisher.\"\"\"",
    "        user = self.recommendation_engine.get_or_create_user(user_id)",
    "        user.followed_publishers.add(publisher_id)",
    "    ",
    "    def set_user_interest(self, user_id: str, category: str, weight: float) -> None:",
    "        \"\"\"Set user interest in a category.\"\"\"",
    "        user = self.recommendation_engine.get_or_create_user(user_id)",
    "        user.interests[category.lower()] = weight",
    "    ",
    "    # ------------------------------------------------------------------",
    "    # ML RECOMMENDATIONS (PART 3)",
    "    # ------------------------------------------------------------------",
    "    ",
    "    def record_engagement(",
    "        self,",
    "        user_id: str,",
    "        article_id: str,",
    "        engagement_type: EngagementType,",
    "        duration_seconds: int",
    "    ) -> None:",
    "        \"\"\"",
    "        Record user engagement for ML training.",
    "        ",
    "        Args:",
    "            user_id: User identifier",
    "            article_id: Article identifier",
    "            engagement_type: CLICK, READ, SHARE, or SAVE",
    "            duration_seconds: Time spent on article",
    "        \"\"\"",
    "        self.recommendation_engine.record_engagement(",
    "            user_id, article_id, engagement_type, duration_seconds",
    "        )",
    "    ",
    "    def get_recommendations(self, user_id: str, count: int) -> List[Article]:",
    "        \"\"\"",
    "        Get ML-powered article recommendations.",
    "        ",
    "        Time Complexity: O(log N) with ANN",
    "        ",
    "        Args:",
    "            user_id: User identifier",
    "            count: Number of recommendations",
    "        ",
    "        Returns:",
    "            List of recommended articles",
    "        \"\"\"",
    "        return self.recommendation_engine.get_recommendations(user_id, count)",
    "    ",
    "    def get_similar_users(self, user_id: str, count: int) -> List[str]:",
    "        \"\"\"Find users with similar preferences.\"\"\"",
    "        return self.recommendation_engine.get_similar_users(user_id, count)",
    "    ",
    "    def train_model(self) -> None:",
    "        \"\"\"Run batch training job.\"\"\"",
    "        self.recommendation_engine.train_model()",
    "",
    "",
    "# ============================================================================",
    "# DEMO AND TESTING",
    "# ============================================================================",
    "",
    "def main():",
    "    \"\"\"Demonstrate the ML-powered recommendation system.\"\"\"",
    "    print(\"=\" * 60)",
    "    print(\"NEWS AGGREGATOR - PART 3: ML RECOMMENDATIONS\")",
    "    print(\"=\" * 60)",
    "    ",
    "    aggregator = NewsAggregator()",
    "    ",
    "    # ------------------------------------------------------------------",
    "    # Setup: Add articles from different categories",
    "    # ------------------------------------------------------------------",
    "    print(\"\\n[1] Adding articles...\")",
    "    ",
    "    articles_data = [",
    "        (\"tech_1\", \"AI Breakthrough in NLP\", \"Content about GPT...\", \"techcrunch\", [\"Tech\", \"AI\"]),",
    "        (\"tech_2\", \"New iPhone Released\", \"Apple announces...\", \"verge\", [\"Tech\"]),",
    "        (\"tech_3\", \"Cloud Computing Trends\", \"AWS and Azure...\", \"techcrunch\", [\"Tech\", \"Business\"]),",
    "        (\"tech_4\", \"Quantum Computing Advance\", \"Google achieves...\", \"wired\", [\"Tech\", \"Science\"]),",
    "        (\"tech_5\", \"Startup Funding Round\", \"Series A...\", \"techcrunch\", [\"Tech\", \"Business\"]),",
    "        (\"sports_1\", \"Championship Finals\", \"Lakers vs...\", \"espn\", [\"Sports\"]),",
    "        (\"sports_2\", \"Transfer News\", \"Soccer star...\", \"espn\", [\"Sports\"]),",
    "        (\"sports_3\", \"Olympic Prep\", \"Athletes prepare...\", \"cnn\", [\"Sports\"]),",
    "        (\"science_1\", \"Mars Rover Discovery\", \"NASA finds...\", \"nasa\", [\"Science\"]),",
    "        (\"science_2\", \"Climate Research\", \"New study...\", \"nature\", [\"Science\"]),",
    "        (\"politics_1\", \"Election Results\", \"Voting shows...\", \"nytimes\", [\"Politics\"]),",
    "        (\"health_1\", \"Vaccine Update\", \"New vaccine...\", \"cnn\", [\"Health\"]),",
    "        (\"ai_1\", \"Machine Learning in Healthcare\", \"AI diagnoses...\", \"wired\", [\"AI\", \"Health\"]),",
    "        (\"ai_2\", \"Autonomous Vehicles\", \"Self-driving cars...\", \"verge\", [\"AI\", \"Tech\"]),",
    "    ]",
    "    ",
    "    for aid, title, content, pub, cats in articles_data:",
    "        aggregator.add_article(aid, title, content, pub, cats)",
    "    print(f\"   Added {len(articles_data)} articles\")",
    "    ",
    "    # ------------------------------------------------------------------",
    "    # Simulate user behavior",
    "    # ------------------------------------------------------------------",
    "    print(\"\\n[2] Recording user engagements...\")",
    "    ",
    "    # User 1: Tech enthusiast",
    "    aggregator.record_engagement(\"user_1\", \"tech_1\", EngagementType.READ, 180)  # 3 min read",
    "    aggregator.record_engagement(\"user_1\", \"tech_2\", EngagementType.READ, 240)  # 4 min read",
    "    aggregator.record_engagement(\"user_1\", \"ai_1\", EngagementType.SHARE, 120)   # Shared!",
    "    aggregator.record_engagement(\"user_1\", \"sports_1\", EngagementType.CLICK, 10) # Quick bounce",
    "    print(\"   User 1: Heavy tech/AI engagement, bounced from sports\")",
    "    ",
    "    # User 2: Sports fan",
    "    aggregator.record_engagement(\"user_2\", \"sports_1\", EngagementType.READ, 300)",
    "    aggregator.record_engagement(\"user_2\", \"sports_2\", EngagementType.SAVE, 200)",
    "    aggregator.record_engagement(\"user_2\", \"sports_3\", EngagementType.READ, 180)",
    "    print(\"   User 2: All sports engagement\")",
    "    ",
    "    # User 3: Mixed interests",
    "    aggregator.record_engagement(\"user_3\", \"tech_3\", EngagementType.READ, 120)",
    "    aggregator.record_engagement(\"user_3\", \"science_1\", EngagementType.READ, 150)",
    "    aggregator.record_engagement(\"user_3\", \"politics_1\", EngagementType.CLICK, 60)",
    "    print(\"   User 3: Mixed tech, science, politics\")",
    "    ",
    "    # ------------------------------------------------------------------",
    "    # Get recommendations",
    "    # ------------------------------------------------------------------",
    "    print(\"\\n[3] Getting recommendations...\")",
    "    ",
    "    print(\"\\n   User 1 (tech enthusiast):\")",
    "    recs = aggregator.get_recommendations(\"user_1\", 5)",
    "    for i, article in enumerate(recs, 1):",
    "        print(f\"      {i}. {article.title} [{', '.join(article.categories)}]\")",
    "    ",
    "    print(\"\\n   User 2 (sports fan):\")",
    "    recs = aggregator.get_recommendations(\"user_2\", 5)",
    "    for i, article in enumerate(recs, 1):",
    "        print(f\"      {i}. {article.title} [{', '.join(article.categories)}]\")",
    "    ",
    "    print(\"\\n   User 3 (mixed interests):\")",
    "    recs = aggregator.get_recommendations(\"user_3\", 5)",
    "    for i, article in enumerate(recs, 1):",
    "        print(f\"      {i}. {article.title} [{', '.join(article.categories)}]\")",
    "    ",
    "    # ------------------------------------------------------------------",
    "    # Cold start user",
    "    # ------------------------------------------------------------------",
    "    print(\"\\n[4] Cold start user (no history):\")",
    "    aggregator.set_user_interest(\"user_new\", \"Science\", 1.0)",
    "    recs = aggregator.get_recommendations(\"user_new\", 3)",
    "    for i, article in enumerate(recs, 1):",
    "        print(f\"      {i}. {article.title} [{', '.join(article.categories)}]\")",
    "    ",
    "    # ------------------------------------------------------------------",
    "    # Similar users",
    "    # ------------------------------------------------------------------",
    "    print(\"\\n[5] Finding similar users to User 1:\")",
    "    similar = aggregator.get_similar_users(\"user_1\", 2)",
    "    for uid in similar:",
    "        print(f\"      - {uid}\")",
    "    ",
    "    # ------------------------------------------------------------------",
    "    # Batch training",
    "    # ------------------------------------------------------------------",
    "    print(\"\\n[6] Running batch training job:\")",
    "    aggregator.train_model()",
    "    ",
    "    print(\"\\n\" + \"=\" * 60)",
    "    print(\"DEMO COMPLETE\")",
    "    print(\"=\" * 60)",
    "",
    "",
    "if __name__ == \"__main__\":",
    "    main()"
  ],
  "solution_java_lines": [
    "import java.util.*;",
    "import java.util.stream.*;",
    "import java.util.concurrent.*;",
    "",
    "/**",
    " * News Feed Aggregator - Part 3: ML-Powered Recommendations",
    " * ",
    " * Two-tower recommendation system with:",
    " * - User/Article embeddings",
    " * - ANN search (simulated)",
    " * - Exploration/Exploitation",
    " * - Diversity re-ranking",
    " */",
    "public class NewsAggregatorPart3 {",
    "    ",
    "    // ========================================================================",
    "    // ENUMS AND CONSTANTS",
    "    // ========================================================================",
    "    ",
    "    public enum EngagementType {",
    "        CLICK(1.0),",
    "        READ(3.0),",
    "        SHARE(5.0),",
    "        SAVE(4.0);",
    "        ",
    "        final double weight;",
    "        EngagementType(double weight) { this.weight = weight; }",
    "    }",
    "    ",
    "    private static final int EMBEDDING_DIM = 128;",
    "    private static final double EXPLORATION_EPSILON = 0.1;",
    "    private static final double FRESHNESS_DECAY_HOURS = 24.0;",
    "    private static final int MAX_PER_CATEGORY = 2;",
    "    private static final int CANDIDATE_MULTIPLIER = 5;",
    "    ",
    "    // ========================================================================",
    "    // DATA CLASSES",
    "    // ========================================================================",
    "    ",
    "    static class EngagementEvent {",
    "        String userId;",
    "        String articleId;",
    "        EngagementType type;",
    "        int durationSeconds;",
    "        long timestamp;",
    "        ",
    "        EngagementEvent(String userId, String articleId, EngagementType type, int duration) {",
    "            this.userId = userId;",
    "            this.articleId = articleId;",
    "            this.type = type;",
    "            this.durationSeconds = duration;",
    "            this.timestamp = System.currentTimeMillis();",
    "        }",
    "    }",
    "    ",
    "    static class Article {",
    "        String articleId;",
    "        String title;",
    "        String content;",
    "        String publisherId;",
    "        List<String> categories;",
    "        long timestamp;",
    "        double[] embedding;",
    "        ",
    "        Article(String id, String title, String content, String pub, List<String> cats) {",
    "            this.articleId = id;",
    "            this.title = title;",
    "            this.content = content;",
    "            this.publisherId = pub;",
    "            this.categories = new ArrayList<>(cats);",
    "            this.timestamp = System.currentTimeMillis();",
    "        }",
    "        ",
    "        @Override",
    "        public String toString() {",
    "            return title + \" [\" + String.join(\", \", categories) + \"]\";",
    "        }",
    "    }",
    "    ",
    "    static class UserProfile {",
    "        String userId;",
    "        Set<String> followedPublishers = new HashSet<>();",
    "        Map<String, Double> interests = new HashMap<>();",
    "        double[] embedding;",
    "        List<EngagementEvent> engagementHistory = new ArrayList<>();",
    "        ",
    "        UserProfile(String userId) {",
    "            this.userId = userId;",
    "        }",
    "    }",
    "    ",
    "    static class ScoredArticle implements Comparable<ScoredArticle> {",
    "        Article article;",
    "        double score;",
    "        ",
    "        ScoredArticle(Article article, double score) {",
    "            this.article = article;",
    "            this.score = score;",
    "        }",
    "        ",
    "        @Override",
    "        public int compareTo(ScoredArticle other) {",
    "            return Double.compare(other.score, this.score); // Descending",
    "        }",
    "    }",
    "    ",
    "    // ========================================================================",
    "    // VECTOR INDEX (ANN SIMULATION)",
    "    // ========================================================================",
    "    ",
    "    static class VectorIndex {",
    "        private final int dimension;",
    "        private final Map<String, double[]> vectors = new HashMap<>();",
    "        private final Map<String, Object> items = new HashMap<>();",
    "        ",
    "        VectorIndex(int dimension) {",
    "            this.dimension = dimension;",
    "        }",
    "        ",
    "        void add(String itemId, double[] vector, Object item) {",
    "            vectors.put(itemId, vector);",
    "            items.put(itemId, item);",
    "        }",
    "        ",
    "        List<Map.Entry<String, Double>> search(double[] query, int k) {",
    "            List<Map.Entry<String, Double>> scores = new ArrayList<>();",
    "            ",
    "            for (Map.Entry<String, double[]> entry : vectors.entrySet()) {",
    "                double sim = cosineSimilarity(query, entry.getValue());",
    "                scores.add(new AbstractMap.SimpleEntry<>(entry.getKey(), sim));",
    "            }",
    "            ",
    "            scores.sort((a, b) -> Double.compare(b.getValue(), a.getValue()));",
    "            return scores.subList(0, Math.min(k, scores.size()));",
    "        }",
    "        ",
    "        private double cosineSimilarity(double[] v1, double[] v2) {",
    "            double dot = 0, norm1 = 0, norm2 = 0;",
    "            for (int i = 0; i < v1.length; i++) {",
    "                dot += v1[i] * v2[i];",
    "                norm1 += v1[i] * v1[i];",
    "                norm2 += v2[i] * v2[i];",
    "            }",
    "            if (norm1 == 0 || norm2 == 0) return 0;",
    "            return dot / (Math.sqrt(norm1) * Math.sqrt(norm2));",
    "        }",
    "    }",
    "    ",
    "    // ========================================================================",
    "    // EMBEDDING GENERATOR",
    "    // ========================================================================",
    "    ",
    "    static class EmbeddingGenerator {",
    "        private static final Map<String, Integer> CATEGORY_DIMS = Map.ofEntries(",
    "            Map.entry(\"tech\", 0), Map.entry(\"ai\", 1), Map.entry(\"science\", 2),",
    "            Map.entry(\"sports\", 3), Map.entry(\"politics\", 4), Map.entry(\"business\", 5),",
    "            Map.entry(\"entertainment\", 6), Map.entry(\"health\", 7)",
    "        );",
    "        ",
    "        double[] generateArticleEmbedding(Article article) {",
    "            double[] embedding = new double[EMBEDDING_DIM];",
    "            ",
    "            for (String category : article.categories) {",
    "                Integer dim = CATEGORY_DIMS.get(category.toLowerCase());",
    "                if (dim != null) {",
    "                    embedding[dim] = 1.0;",
    "                    if (dim > 0) embedding[dim - 1] = 0.3;",
    "                    if (dim < EMBEDDING_DIM - 1) embedding[dim + 1] = 0.3;",
    "                }",
    "            }",
    "            ",
    "            int pubDim = Math.abs(article.publisherId.hashCode()) % 64 + 64;",
    "            embedding[pubDim] = 0.5;",
    "            ",
    "            return normalize(embedding);",
    "        }",
    "        ",
    "        double[] generateUserEmbedding(UserProfile user, Map<String, double[]> articleEmbs) {",
    "            if (user.engagementHistory.isEmpty()) {",
    "                return generateColdStartEmbedding(user);",
    "            }",
    "            ",
    "            double[] embedding = new double[EMBEDDING_DIM];",
    "            double totalWeight = 0;",
    "            long now = System.currentTimeMillis();",
    "            ",
    "            for (EngagementEvent event : user.engagementHistory) {",
    "                double[] artEmb = articleEmbs.get(event.articleId);",
    "                if (artEmb == null) continue;",
    "                ",
    "                double weight = event.type.weight;",
    "                if (event.durationSeconds > 60) weight *= 1.5;",
    "                else if (event.durationSeconds < 10) weight *= 0.5;",
    "                ",
    "                double daysAgo = (now - event.timestamp) / 86400000.0;",
    "                weight *= Math.exp(-0.1 * daysAgo);",
    "                ",
    "                for (int i = 0; i < EMBEDDING_DIM; i++) {",
    "                    embedding[i] += artEmb[i] * weight;",
    "                }",
    "                totalWeight += weight;",
    "            }",
    "            ",
    "            if (totalWeight > 0) {",
    "                for (int i = 0; i < EMBEDDING_DIM; i++) {",
    "                    embedding[i] /= totalWeight;",
    "                }",
    "            }",
    "            ",
    "            return normalize(embedding);",
    "        }",
    "        ",
    "        private double[] generateColdStartEmbedding(UserProfile user) {",
    "            double[] embedding = new double[EMBEDDING_DIM];",
    "            Arrays.fill(embedding, 0.1);",
    "            ",
    "            for (Map.Entry<String, Double> entry : user.interests.entrySet()) {",
    "                Integer dim = CATEGORY_DIMS.get(entry.getKey().toLowerCase());",
    "                if (dim != null) {",
    "                    embedding[dim] = entry.getValue();",
    "                }",
    "            }",
    "            return normalize(embedding);",
    "        }",
    "        ",
    "        private double[] normalize(double[] vec) {",
    "            double norm = 0;",
    "            for (double v : vec) norm += v * v;",
    "            norm = Math.sqrt(norm);",
    "            if (norm == 0) return vec;",
    "            for (int i = 0; i < vec.length; i++) vec[i] /= norm;",
    "            return vec;",
    "        }",
    "    }",
    "    ",
    "    // ========================================================================",
    "    // RECOMMENDATION ENGINE",
    "    // ========================================================================",
    "    ",
    "    static class RecommendationEngine {",
    "        private final EmbeddingGenerator embeddingGen = new EmbeddingGenerator();",
    "        private VectorIndex articleIndex = new VectorIndex(EMBEDDING_DIM);",
    "        private final VectorIndex userIndex = new VectorIndex(EMBEDDING_DIM);",
    "        ",
    "        private final Map<String, double[]> articleEmbeddings = new HashMap<>();",
    "        private final Map<String, double[]> userEmbeddings = new HashMap<>();",
    "        private final Map<String, Article> articles = new HashMap<>();",
    "        private final Map<String, UserProfile> users = new HashMap<>();",
    "        private final Map<String, Set<String>> articlesByCategory = new HashMap<>();",
    "        ",
    "        private final Random random = new Random(42);",
    "        ",
    "        void addArticle(Article article) {",
    "            double[] embedding = embeddingGen.generateArticleEmbedding(article);",
    "            article.embedding = embedding;",
    "            articleEmbeddings.put(article.articleId, embedding);",
    "            articles.put(article.articleId, article);",
    "            ",
    "            for (String cat : article.categories) {",
    "                articlesByCategory.computeIfAbsent(cat.toLowerCase(), k -> new HashSet<>())",
    "                                  .add(article.articleId);",
    "            }",
    "            articleIndex.add(article.articleId, embedding, article);",
    "        }",
    "        ",
    "        UserProfile getOrCreateUser(String userId) {",
    "            return users.computeIfAbsent(userId, UserProfile::new);",
    "        }",
    "        ",
    "        void recordEngagement(String userId, String articleId,",
    "                              EngagementType type, int durationSeconds) {",
    "            UserProfile user = getOrCreateUser(userId);",
    "            EngagementEvent event = new EngagementEvent(userId, articleId, type, durationSeconds);",
    "            user.engagementHistory.add(event);",
    "            ",
    "            Article article = articles.get(articleId);",
    "            if (article != null) {",
    "                double weight = type.weight * (durationSeconds > 60 ? 1.5 : 1.0);",
    "                for (String cat : article.categories) {",
    "                    user.interests.merge(cat.toLowerCase(), weight, Double::sum);",
    "                }",
    "            }",
    "            ",
    "            userEmbeddings.remove(userId);",
    "        }",
    "        ",
    "        private double[] getUserEmbedding(String userId) {",
    "            return userEmbeddings.computeIfAbsent(userId, id ->",
    "                embeddingGen.generateUserEmbedding(getOrCreateUser(id), articleEmbeddings));",
    "        }",
    "        ",
    "        List<Article> getRecommendations(String userId, int count) {",
    "            if (articles.isEmpty()) return Collections.emptyList();",
    "            ",
    "            double[] userEmb = getUserEmbedding(userId);",
    "            UserProfile user = getOrCreateUser(userId);",
    "            ",
    "            Set<String> seen = user.engagementHistory.stream()",
    "                    .map(e -> e.articleId).collect(Collectors.toSet());",
    "            ",
    "            int candidateCount = count * CANDIDATE_MULTIPLIER + seen.size();",
    "            var candidates = articleIndex.search(userEmb, candidateCount);",
    "            ",
    "            long now = System.currentTimeMillis();",
    "            List<ScoredArticle> scored = new ArrayList<>();",
    "            ",
    "            for (var entry : candidates) {",
    "                String articleId = entry.getKey();",
    "                if (seen.contains(articleId)) continue;",
    "                ",
    "                Article article = articles.get(articleId);",
    "                if (article == null) continue;",
    "                ",
    "                double score = entry.getValue();",
    "                double hoursOld = (now - article.timestamp) / 3600000.0;",
    "                score += Math.exp(-hoursOld / FRESHNESS_DECAY_HOURS) * 0.2;",
    "                ",
    "                if (user.followedPublishers.contains(article.publisherId)) {",
    "                    score += 0.1;",
    "                }",
    "                ",
    "                scored.add(new ScoredArticle(article, score));",
    "            }",
    "            ",
    "            if (random.nextDouble() < EXPLORATION_EPSILON) {",
    "                injectExploration(scored, user, seen, count);",
    "            }",
    "            ",
    "            return rerankForDiversity(scored, count);",
    "        }",
    "        ",
    "        private void injectExploration(List<ScoredArticle> scored, UserProfile user,",
    "                                        Set<String> seen, int count) {",
    "            Set<String> userCats = user.interests.keySet();",
    "            Set<String> unexplored = new HashSet<>(articlesByCategory.keySet());",
    "            unexplored.removeAll(userCats);",
    "            ",
    "            int slots = Math.max(1, count / 5);",
    "            List<String> catList = new ArrayList<>(unexplored);",
    "            Collections.shuffle(catList, random);",
    "            ",
    "            for (int i = 0; i < Math.min(slots, catList.size()); i++) {",
    "                Set<String> articleIds = articlesByCategory.get(catList.get(i));",
    "                List<String> available = articleIds.stream()",
    "                        .filter(id -> !seen.contains(id))",
    "                        .collect(Collectors.toList());",
    "                if (!available.isEmpty()) {",
    "                    String randId = available.get(random.nextInt(available.size()));",
    "                    Article art = articles.get(randId);",
    "                    if (art != null) {",
    "                        scored.add(new ScoredArticle(art, 0.5));",
    "                    }",
    "                }",
    "            }",
    "        }",
    "        ",
    "        private List<Article> rerankForDiversity(List<ScoredArticle> scored, int count) {",
    "            Collections.sort(scored);",
    "            ",
    "            List<Article> result = new ArrayList<>();",
    "            Map<String, Integer> catCounts = new HashMap<>();",
    "            Map<String, Integer> pubCounts = new HashMap<>();",
    "            ",
    "            for (ScoredArticle sa : scored) {",
    "                if (result.size() >= count) break;",
    "                ",
    "                Article art = sa.article;",
    "                boolean catOk = art.categories.stream()",
    "                        .allMatch(c -> catCounts.getOrDefault(c.toLowerCase(), 0) < MAX_PER_CATEGORY);",
    "                boolean pubOk = pubCounts.getOrDefault(art.publisherId, 0) < MAX_PER_CATEGORY;",
    "                ",
    "                if (catOk && pubOk) {",
    "                    result.add(art);",
    "                    for (String c : art.categories) {",
    "                        catCounts.merge(c.toLowerCase(), 1, Integer::sum);",
    "                    }",
    "                    pubCounts.merge(art.publisherId, 1, Integer::sum);",
    "                }",
    "            }",
    "            return result;",
    "        }",
    "        ",
    "        List<String> getSimilarUsers(String userId, int count) {",
    "            double[] userEmb = getUserEmbedding(userId);",
    "            for (String uid : users.keySet()) {",
    "                userIndex.add(uid, getUserEmbedding(uid), uid);",
    "            }",
    "            return userIndex.search(userEmb, count + 1).stream()",
    "                    .map(Map.Entry::getKey)",
    "                    .filter(id -> !id.equals(userId))",
    "                    .limit(count)",
    "                    .collect(Collectors.toList());",
    "        }",
    "        ",
    "        void trainModel() {",
    "            System.out.println(\"Training model...\");",
    "            articleIndex = new VectorIndex(EMBEDDING_DIM);",
    "            for (Article art : articles.values()) {",
    "                double[] emb = embeddingGen.generateArticleEmbedding(art);",
    "                articleEmbeddings.put(art.articleId, emb);",
    "                articleIndex.add(art.articleId, emb, art);",
    "            }",
    "            userEmbeddings.clear();",
    "            System.out.printf(\"Trained on %d articles, %d users%n\",",
    "                    articles.size(), users.size());",
    "        }",
    "    }",
    "    ",
    "    // ========================================================================",
    "    // MAIN AGGREGATOR CLASS",
    "    // ========================================================================",
    "    ",
    "    private final RecommendationEngine engine = new RecommendationEngine();",
    "    ",
    "    public void addArticle(String id, String title, String content,",
    "                           String pub, List<String> categories) {",
    "        engine.addArticle(new Article(id, title, content, pub, categories));",
    "    }",
    "    ",
    "    public void followPublisher(String userId, String publisherId) {",
    "        engine.getOrCreateUser(userId).followedPublishers.add(publisherId);",
    "    }",
    "    ",
    "    public void recordEngagement(String userId, String articleId,",
    "                                  EngagementType type, int durationSeconds) {",
    "        engine.recordEngagement(userId, articleId, type, durationSeconds);",
    "    }",
    "    ",
    "    public List<Article> getRecommendations(String userId, int count) {",
    "        return engine.getRecommendations(userId, count);",
    "    }",
    "    ",
    "    public List<String> getSimilarUsers(String userId, int count) {",
    "        return engine.getSimilarUsers(userId, count);",
    "    }",
    "    ",
    "    public void trainModel() {",
    "        engine.trainModel();",
    "    }",
    "    ",
    "    // ========================================================================",
    "    // DEMO",
    "    // ========================================================================",
    "    ",
    "    public static void main(String[] args) {",
    "        System.out.println(\"=\".repeat(60));",
    "        System.out.println(\"NEWS AGGREGATOR - PART 3: ML RECOMMENDATIONS\");",
    "        System.out.println(\"=\".repeat(60));",
    "        ",
    "        var aggregator = new NewsAggregatorPart3();",
    "        ",
    "        // Add articles",
    "        System.out.println(\"\\n[1] Adding articles...\");",
    "        aggregator.addArticle(\"tech_1\", \"AI Breakthrough\", \"...\", \"techcrunch\",",
    "                List.of(\"Tech\", \"AI\"));",
    "        aggregator.addArticle(\"tech_2\", \"New iPhone\", \"...\", \"verge\",",
    "                List.of(\"Tech\"));",
    "        aggregator.addArticle(\"tech_3\", \"Cloud Trends\", \"...\", \"techcrunch\",",
    "                List.of(\"Tech\", \"Business\"));",
    "        aggregator.addArticle(\"sports_1\", \"Championship\", \"...\", \"espn\",",
    "                List.of(\"Sports\"));",
    "        aggregator.addArticle(\"sports_2\", \"Transfer News\", \"...\", \"espn\",",
    "                List.of(\"Sports\"));",
    "        aggregator.addArticle(\"science_1\", \"Mars Discovery\", \"...\", \"nasa\",",
    "                List.of(\"Science\"));",
    "        ",
    "        // Record engagements",
    "        System.out.println(\"\\n[2] Recording engagements...\");",
    "        aggregator.recordEngagement(\"user_1\", \"tech_1\", EngagementType.READ, 180);",
    "        aggregator.recordEngagement(\"user_1\", \"tech_2\", EngagementType.SHARE, 120);",
    "        aggregator.recordEngagement(\"user_1\", \"sports_1\", EngagementType.CLICK, 10);",
    "        ",
    "        aggregator.recordEngagement(\"user_2\", \"sports_1\", EngagementType.READ, 300);",
    "        aggregator.recordEngagement(\"user_2\", \"sports_2\", EngagementType.SAVE, 200);",
    "        ",
    "        // Get recommendations",
    "        System.out.println(\"\\n[3] Recommendations for User 1 (tech fan):\");",
    "        for (var art : aggregator.getRecommendations(\"user_1\", 3)) {",
    "            System.out.println(\"   - \" + art);",
    "        }",
    "        ",
    "        System.out.println(\"\\n[4] Recommendations for User 2 (sports fan):\");",
    "        for (var art : aggregator.getRecommendations(\"user_2\", 3)) {",
    "            System.out.println(\"   - \" + art);",
    "        }",
    "        ",
    "        System.out.println(\"\\n[5] Similar users to User 1:\");",
    "        System.out.println(\"   \" + aggregator.getSimilarUsers(\"user_1\", 2));",
    "        ",
    "        System.out.println(\"\\n\" + \"=\".repeat(60));",
    "        System.out.println(\"DEMO COMPLETE\");",
    "    }",
    "}"
  ],
  "code_walkthrough": [
    {
      "lines": "1-20",
      "explanation": "Imports and module docstring. Defines the ML recommendation system that extends Parts 1-2."
    },
    {
      "lines": "22-40",
      "explanation": "Constants and EngagementType enum. ENGAGEMENT_WEIGHTS map different interaction types to scores (Share=5.0 is highest signal)."
    },
    {
      "lines": "45-95",
      "explanation": "Core data classes: EngagementEvent stores user interactions, Article includes embedding field, UserProfile tracks preferences and history."
    },
    {
      "lines": "100-150",
      "explanation": "VectorIndex class simulates ANN (Faiss/Annoy). Provides add() and search() with cosine similarity. In production, swap for real vector DB."
    },
    {
      "lines": "155-250",
      "explanation": "EmbeddingGenerator creates embeddings. Article embeddings use category dimensions. User embeddings are weighted averages of engaged articles with recency decay."
    },
    {
      "lines": "255-400",
      "explanation": "RecommendationEngine core: add_article indexes with embedding, record_engagement appends events, get_recommendations does ANN search + scoring + re-ranking."
    },
    {
      "lines": "340-380",
      "explanation": "_inject_exploration implements epsilon-greedy exploration. Finds unexplored categories and injects random articles from them."
    },
    {
      "lines": "382-420",
      "explanation": "_rerank_for_diversity enforces MAX_PER_CATEGORY constraint, preventing feed monotony. Greedy selection maintains score ordering."
    },
    {
      "lines": "425-500",
      "explanation": "NewsAggregator main class wraps RecommendationEngine with clean API. Delegates all ML operations to engine."
    },
    {
      "lines": "505-600",
      "explanation": "Demo main() shows full workflow: add articles, record engagements, get personalized recommendations. Validates tech enthusiast gets tech recommendations."
    }
  ],
  "complexity_analysis": {
    "time": {
      "new_methods": {
        "recordEngagement": {
          "complexity": "O(1)",
          "explanation": "Append event to list, update counters. No complex computation at write time."
        },
        "getRecommendations": {
          "complexity": "O(log N + K log K)",
          "explanation": "ANN search is O(log N) with HNSW index. Re-ranking sorts K candidates in O(K log K). K << N."
        },
        "getSimilarUsers": {
          "complexity": "O(log U)",
          "explanation": "ANN search on user embedding space. U = number of users."
        },
        "trainModel": {
          "complexity": "O(N*D + U*H*D)",
          "explanation": "Recompute N article embeddings (each O(D)) and U user embeddings (each scans H history events)."
        },
        "_compute_user_embedding": {
          "complexity": "O(H*D)",
          "explanation": "Weighted sum over H history events, each contributing D dimensions."
        }
      },
      "overall_change": "Core getRecommendations changes from O(N) full scan to O(log N) with ANN. This is essential for 100K QPS at scale."
    },
    "space": {
      "additional_space": "O(N*D + U*D + U*H)",
      "explanation": "N article embeddings (128-dim each), U user embeddings, and U*H engagement events. At 10M articles and 1M users with D=128, this is ~5GB for embeddings."
    }
  },
  "dry_run": {
    "example_input": "User reads tech articles for 3-4 minutes, bounces from sports in 10 seconds. Request 5 recommendations.",
    "steps": [
      {
        "step": 1,
        "action": "recordEngagement(user_1, tech_1, READ, 180)",
        "state": "user_1.history = [tech_1:READ:180s], interests = {tech: 4.5}",
        "explanation": "READ weight=3.0 * 1.5 (>60s) = 4.5 for tech category"
      },
      {
        "step": 2,
        "action": "recordEngagement(user_1, tech_2, READ, 240)",
        "state": "history += [tech_2:READ:240s], interests = {tech: 9.0}",
        "explanation": "Another long tech read adds 4.5 more weight"
      },
      {
        "step": 3,
        "action": "recordEngagement(user_1, sports_1, CLICK, 10)",
        "state": "history += [sports_1:CLICK:10s], interests = {tech: 9.0, sports: 0.5}",
        "explanation": "CLICK weight=1.0 * 0.5 (quick bounce) = 0.5 for sports"
      },
      {
        "step": 4,
        "action": "getRecommendations(user_1, 5) - compute user embedding",
        "state": "user_emb = weighted_avg(tech_1_emb*4.5, tech_2_emb*4.5, sports_1_emb*0.5) \u2248 tech-heavy vector",
        "explanation": "User embedding strongly aligned with tech dimension due to high engagement weights"
      },
      {
        "step": 5,
        "action": "ANN search with user embedding",
        "state": "candidates = [tech_3:0.95, ai_1:0.88, tech_4:0.85, science_1:0.70, ...]",
        "explanation": "Vector index returns tech/AI articles with highest similarity scores"
      },
      {
        "step": 6,
        "action": "Apply freshness boost and re-rank",
        "state": "final_scores after freshness: [tech_3:0.97, ai_1:0.90, ...]",
        "explanation": "Recent articles get small boost. Diversity filter limits per-category."
      },
      {
        "step": 7,
        "action": "Return top 5",
        "state": "[tech_3, tech_4, ai_1, ai_2, science_1]",
        "explanation": "Max 2 per category enforced. Tech-related but diverse selection."
      }
    ],
    "final_output": "Returns tech-focused recommendations with some AI/Science serendipity, excluding already-seen articles"
  },
  "debugging_playbook": {
    "fast_sanity_checks": [
      "Empty article store returns empty recommendations",
      "Cold-start user with declared interest='Tech' gets tech recommendations",
      "User with only READ engagements gets relevant recommendations"
    ],
    "likely_bugs": [
      "Forgetting to exclude already-seen articles from recommendations",
      "Not normalizing embeddings causing similarity scores > 1",
      "Recency decay using wrong time units (ms vs seconds)",
      "Category case-sensitivity mismatch (Tech vs tech)"
    ],
    "recommended_logs_or_asserts": [
      "assert len(embedding) == EMBEDDING_DIM for all embeddings",
      "log user_embedding norm to verify normalization",
      "log candidate scores before/after freshness boost",
      "assert result doesn't contain seen article IDs"
    ],
    "how_to_localize": "1. Log user embedding dimensions that are non-zero. 2. Log ANN search results with scores. 3. Trace re-ranking decisions (which articles filtered by diversity). 4. Compare expected vs actual category weights."
  },
  "edge_cases": [
    {
      "case": "Cold-start user with no engagement history",
      "handling": "Use declared interests to generate embedding, or return popular/trending articles",
      "gotcha": "Empty interests and empty history should return diverse popular content, not empty list"
    },
    {
      "case": "New article with no engagement (cold-start item)",
      "handling": "Content-based embedding from categories/title is still computed, allowing discovery via similarity",
      "gotcha": "Boost new articles slightly to give them initial exposure (exploration)"
    },
    {
      "case": "User has engaged with ALL available articles",
      "handling": "Return empty or suggest re-reading saved articles",
      "gotcha": "Don't crash or return stale recommendations"
    },
    {
      "case": "Single-category user (only reads sports)",
      "handling": "Exploration mechanism injects non-sports articles with 10% probability",
      "gotcha": "Without exploration, user gets stuck in filter bubble"
    },
    {
      "case": "Very old engagement history",
      "handling": "Recency decay reduces weight of old events exponentially (half-life ~7 days)",
      "gotcha": "Don't let month-old clicks dominate recent preferences"
    }
  ],
  "test_cases": [
    {
      "name": "Tech enthusiast recommendations",
      "input": "User reads tech_1 (180s), tech_2 (240s), bounces sports_1 (10s). Request 5 recs.",
      "expected": "Top recommendations are tech/AI articles, sports deprioritized",
      "explanation": "High engagement time + engagement type weights create strong tech preference"
    },
    {
      "name": "Cold start with declared interest",
      "input": "New user with interests={Science: 1.0}. Request 3 recs.",
      "expected": "Science articles ranked highest",
      "explanation": "Cold-start embedding is built from declared interests when no history exists"
    },
    {
      "name": "Diversity constraint enforcement",
      "input": "User loves tech. 10 tech articles available. Request 5 recs.",
      "expected": "Max 2 tech articles, rest from related categories",
      "explanation": "MAX_PER_CATEGORY=2 forces diversity even for strong preferences"
    },
    {
      "name": "Exploration injection",
      "input": "User only reads sports. epsilon=1.0 (forced exploration). Request 5.",
      "expected": "Some non-sports articles included",
      "explanation": "Epsilon-greedy with high epsilon ensures unexplored categories are surfaced"
    },
    {
      "name": "Exclude already-read articles",
      "input": "User has read tech_1, tech_2. Request recommendations.",
      "expected": "tech_1 and tech_2 NOT in results",
      "explanation": "Seen articles are filtered before ranking"
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Not normalizing embeddings before cosine similarity",
      "why_wrong": "Cosine similarity assumes unit vectors. Without normalization, articles with more categories get artificially higher scores",
      "correct_approach": "Always L2-normalize embeddings after computation",
      "code_example_wrong": "# No normalization\\nembedding = [sum of category vectors]\\nreturn embedding",
      "code_example_correct": "embedding = [sum of category vectors]\\nnorm = sqrt(sum(x*x for x in embedding))\\nreturn [x/norm for x in embedding]"
    },
    {
      "mistake": "Using raw timestamp difference for recency",
      "why_wrong": "Time units matter! Milliseconds vs seconds vs days gives wildly different decay",
      "correct_approach": "Explicitly convert to days/hours before decay calculation",
      "code_example_wrong": "weight *= exp(-0.1 * (now - event.timestamp))",
      "code_example_correct": "days_ago = (now - event.timestamp) / 86400  # seconds to days\\nweight *= exp(-0.1 * days_ago)"
    },
    {
      "mistake": "Scanning all articles for recommendations",
      "why_wrong": "O(N) doesn't scale to millions of articles at 100K QPS",
      "correct_approach": "Use ANN index for O(log N) retrieval, only score candidates",
      "code_example_wrong": "for article in all_articles:\\n    score = similarity(user_emb, article.emb)\\n    ...",
      "code_example_correct": "candidates = ann_index.search(user_emb, k=100)  # O(log N)\\nfor article_id, sim in candidates:\\n    ..."
    },
    {
      "mistake": "Forgetting to invalidate cached user embedding after new engagement",
      "why_wrong": "User gets stale recommendations that don't reflect recent activity",
      "correct_approach": "Clear or incrementally update user embedding cache on new engagement",
      "code_example_wrong": "def record_engagement(...):\\n    user.history.append(event)  # Cache not invalidated!",
      "code_example_correct": "def record_engagement(...):\\n    user.history.append(event)\\n    self.user_embeddings.pop(user_id, None)  # Force recompute"
    }
  ],
  "interview_tips": {
    "how_to_present": "Start by drawing the two-tower architecture diagram. Explain that pre-computing embeddings offline is key to meeting latency requirements. Then walk through the serve-time flow: feature lookup \u2192 ANN search \u2192 scoring \u2192 re-ranking. Mention cold-start and exploration as important edge cases.",
    "what_to_mention": [
      "Two-tower architecture allows independent pre-computation",
      "ANN search is O(log N) - essential for scale",
      "Exploration prevents filter bubbles",
      "Cold-start handling for new users/articles",
      "Diversity re-ranking for better user experience"
    ],
    "time_allocation": "15-20 minutes: 3-4 min high-level design, 2 min embedding discussion, 8-10 min implementation, 3-4 min testing and edge cases",
    "if_stuck": [
      "Think about what YouTube/Netflix recommendations do - they pre-compute",
      "ANN libraries like Faiss handle the hard part - just call search()",
      "Epsilon-greedy is simplest exploration strategy",
      "Weighted average of article embeddings is a simple user embedding"
    ]
  },
  "connection_to_next_part": "Part 4 could add A/B testing framework to compare recommendation algorithms, or real-time streaming updates using Apache Kafka for instant engagement incorporation. The RecommendationEngine is designed for swappable ranking models, making A/B tests straightforward.",
  "communication_script": {
    "transition_from_previous": "Great, so Part 2 handles real-time notifications. For Part 3, I need to add ML-powered recommendations that learn from user behavior. The key insight is that we can't run neural nets at serve time at 100K QPS, so I'll use a two-tower architecture with pre-computed embeddings.",
    "explaining_changes": "The main changes are: (1) Adding engagement tracking with weighted signals, (2) Computing user embeddings from engagement history, (3) Using ANN search for O(log N) candidate retrieval, and (4) Re-ranking for diversity and exploration.",
    "while_extending_code": [
      "I'm adding an EngagementEvent class to track interactions with timestamps...",
      "This VectorIndex simulates Faiss/Pinecone - in production we'd use the real thing...",
      "The embedding generator creates category-based vectors - in production this would be a trained neural net...",
      "Here's the key part - ANN search gets candidates in O(log N), then we score and re-rank..."
    ],
    "after_completing": "This now handles ML recommendations. recordEngagement is O(1), getRecommendations is O(log N) with ANN. I've added exploration to prevent filter bubbles and diversity constraints. Ready to discuss A/B testing or real-time streaming for Part 4?"
  },
  "time_milestones": {
    "time_budget": "15-20 minutes for this part",
    "by_3_min": "Understand two-tower architecture, explain pre-computation strategy",
    "by_8_min": "Core data structures implemented (embeddings, vector index)",
    "by_15_min": "Full implementation done, tested with example",
    "by_20_min": "Edge cases discussed, ready for next part",
    "warning_signs": "If stuck on embedding math at 8 min, simplify to category vectors. If ANN concept unclear, just use brute-force and mention 'would use Faiss in production'."
  },
  "recovery_strategies": {
    "if_part_builds_wrong": "Part 3 is relatively independent - if Part 2 notifications are buggy, focus on recommendations separately. Say: 'Let me handle recommendations independently, then we can integrate.'",
    "if_new_requirement_unclear": "Ask: 'For the two-tower model, should I simulate with category vectors or just describe the neural network approach?' and 'Is O(log N) acceptable or do we need O(1)?'",
    "if_running_behind": "Skip exploration/exploitation first - just do basic similarity ranking. Say: 'Getting core recommendations working first, will add exploration if time permits.'"
  },
  "signal_points": {
    "wow_factors_for_followup": [
      "Proactively mentioning ANN algorithms (HNSW, IVF) by name",
      "Discussing online learning for real-time signal incorporation",
      "Mentioning feature stores like Feast for production deployment",
      "Bringing up Thompson Sampling as better alternative to epsilon-greedy",
      "Discussing model serving infrastructure (TensorFlow Serving, Triton)"
    ]
  },
  "pattern_recognition": {
    "pattern": "Two-Tower Neural Network + ANN Retrieval",
    "indicators": [
      "Need to match users to items (recommendation system)",
      "Scale requirement too high for real-time inference",
      "Similarity-based matching mentioned",
      "Personalization based on behavior"
    ],
    "similar_problems": [
      "YouTube video recommendations (candidate generation + ranking)",
      "Netflix show recommendations",
      "E-commerce product recommendations",
      "Similar document retrieval",
      "Semantic search engines"
    ],
    "template": "1. Compute item embeddings offline (item tower)\\n2. Compute user embeddings from behavior (user tower)\\n3. Index item embeddings in ANN structure\\n4. At serve time: lookup user embedding \u2192 ANN search \u2192 score \u2192 re-rank\\n5. Apply business rules and diversity constraints"
  },
  "thinking_process": [
    {
      "step": 1,
      "thought": "When I see 100K QPS for recommendations, I immediately think pre-computation",
      "why": "Real-time ML inference at this scale is prohibitively expensive and slow"
    },
    {
      "step": 2,
      "thought": "Two-tower architecture separates user and item encoding",
      "why": "Allows pre-computing item embeddings and caching user embeddings"
    },
    {
      "step": 3,
      "thought": "Need ANN for sub-linear retrieval",
      "why": "Can't scan millions of articles for each request - O(N) doesn't scale"
    },
    {
      "step": 4,
      "thought": "Exploration is necessary to avoid filter bubbles",
      "why": "Pure exploitation leads to recommendation monotony and user churn"
    },
    {
      "step": 5,
      "thought": "Re-ranking adds business logic without affecting ML model",
      "why": "Separates ML relevance from editorial/business constraints"
    }
  ],
  "interviewer_perspective": {
    "what_they_evaluate": [
      "Understanding of production ML systems at scale",
      "Knowledge of embedding-based retrieval patterns",
      "Awareness of exploration vs exploitation tradeoff",
      "Clean code extending previous parts",
      "Handling of edge cases (cold start, diversity)"
    ],
    "bonus_points": [
      "Mentioning specific ANN algorithms (HNSW, IVF-PQ)",
      "Discussing model serving infrastructure",
      "Bringing up A/B testing for model comparison",
      "Mentioning feature stores and online/offline features",
      "Discussing multi-objective ranking (relevance + freshness + diversity)"
    ],
    "red_flags": [
      "Proposing to run neural net inference at 100K QPS without caching",
      "Using O(N) brute-force search without acknowledging scalability issues",
      "Ignoring cold-start problem",
      "No mention of exploration - leads to filter bubbles",
      "Overcomplicating with deep learning when simpler approaches work"
    ]
  },
  "ai_copilot_tips": {
    "what_to_do": [
      "Use AI for cosine similarity boilerplate",
      "Let it help with data class definitions",
      "Get help with embedding normalization math"
    ],
    "what_not_to_do": [
      "Don't let AI design the architecture - that's your job to explain",
      "Verify all complexity claims - AI may not understand ANN guarantees",
      "Don't accept neural network code without understanding dimensions"
    ]
  },
  "red_flags_to_avoid": {
    "behavioral": [
      "Jumping into code without explaining two-tower architecture",
      "Not asking about latency/QPS constraints",
      "Ignoring the exploration requirement"
    ],
    "technical": [
      "Using O(N) similarity search at scale",
      "Not normalizing embeddings",
      "Ignoring recency decay for engagement",
      "Hardcoding magic numbers without explanation"
    ],
    "communication": [
      "Not explaining why pre-computation is essential",
      "Forgetting to mention cold-start handling",
      "Not testing with the provided example"
    ]
  },
  "final_checklist": {
    "before_saying_done": [
      "recordEngagement is O(1)?",
      "getRecommendations uses ANN for O(log N)?",
      "Cold-start users get reasonable recommendations?",
      "Exploration mechanism prevents filter bubbles?",
      "Diversity constraints enforced in re-ranking?",
      "Already-seen articles excluded?"
    ],
    "quick_code_review": [
      "Embeddings are normalized before similarity",
      "Engagement weights scale appropriately",
      "Recency decay uses correct time units",
      "Category matching is case-insensitive"
    ]
  },
  "production_considerations": {
    "what_i_would_add": [
      "Vector database (Pinecone/Milvus) instead of in-memory index",
      "Feature store (Feast) for online/offline feature serving",
      "Model serving (TensorFlow Serving) for embedding computation",
      "A/B testing framework for model comparison",
      "Metrics tracking (CTR, dwell time, user satisfaction)"
    ],
    "why_not_in_interview": "Focus on algorithm and architecture; infrastructure can be mentioned verbally without implementation",
    "how_to_mention": "Say: 'In production, I'd use Pinecone for the vector index and Feast for the feature store. The embedding model would be served via TensorFlow Serving with GPU inference for batch computation.'"
  },
  "generated_at": "2026-01-18T18:48:09.432555",
  "_meta": {
    "problem_id": "news_feed_aggregator",
    "part_number": 3,
    "model": "claude-opus-4-5-20251101"
  }
}