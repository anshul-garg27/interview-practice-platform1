{
  "problem_title": "Design Instagram - Photo Sharing Platform",
  "difficulty": "hard",
  "category": "HLD/System Design",
  "estimated_time": "45-60 minutes",
  "problem_analysis": {
    "first_impressions": "## Initial Assessment\n\nThis is a **large-scale distributed system design** problem that tests understanding of:\n- **Media storage** at petabyte scale\n- **Feed generation** algorithms (the hardest part)\n- **Social graph** management with celebrity edge cases\n- **CDN architecture** for global low-latency access\n- **Eventual consistency** tradeoffs\n\nThe 100:1 read:write ratio immediately signals this is a **read-heavy system** requiring aggressive caching and pre-computation strategies.",
    "pattern_recognition": "## Core Patterns\n\n1. **Fan-out Pattern** - Push vs Pull for feed generation\n2. **CQRS** - Command Query Responsibility Segregation (separate write/read paths)\n3. **CDN + Object Storage** - Standard pattern for media-heavy applications\n4. **Sharding** - Database partitioning for horizontal scale\n5. **Write-behind Caching** - Async updates for non-critical data\n6. **Message Queue + Workers** - Async processing pipeline\n7. **Polyglot Persistence** - Different DBs for different use cases",
    "key_constraints": [
      "500M DAU - Need multi-region deployment with 99.99% availability",
      "100M photos/day (200TB) - Object storage required, NOT database blobs",
      "100:1 read:write ratio - Pre-compute feeds, aggressive caching",
      "Celebrity followers (500M) - Cannot fan-out on write for celebrities",
      "Feed latency < 500ms - Pre-computed feeds with caching",
      "Global users - CDN with edge locations worldwide",
      "Storage grows 73PB/year - Need efficient storage tiering (hot/warm/cold)"
    ],
    "clarifying_questions": [
      "**Consistency Model**: Is eventual consistency acceptable for feeds and like counts? (Yes for most social apps)",
      "**Video Support**: Are we including videos in scope, or just photos? (Affects storage and processing significantly)",
      "**Feed Algorithm**: Chronological or ML-ranked feed? (Ranked adds complexity but more realistic)",
      "**Stories Feature**: Is ephemeral content in scope for this design? (Part 2 follow-up)",
      "**Privacy**: Do we need private accounts where only approved followers see content?",
      "**Geographic Distribution**: Which regions need lowest latency? (Affects CDN and DB placement)",
      "**Deletion Policy**: Do we soft-delete or hard-delete posts? (Affects storage and compliance)"
    ],
    "edge_cases_to_consider": [
      "Celebrity with 500M followers posts - cannot fan-out to all followers synchronously",
      "Viral post getting millions of likes/second - need rate limiting and async counting",
      "User unfollows right after new post - race condition in feed",
      "User with 0 followers - no fan-out needed",
      "User follows 10,000 accounts - expensive pull-based feed generation",
      "Hot spot: single image getting 1M requests/second",
      "Timezone-aware feed ranking (posting time relevance)",
      "User deletes post that's already in millions of feeds"
    ]
  },
  "requirements_coverage": {
    "checklist": [
      {
        "requirement": "uploadPhoto - Upload with metadata, return postId in <2s",
        "how_met": "Async pipeline: S3 upload \u2192 Queue \u2192 Workers for processing. Return postId immediately after S3 upload (100ms), process thumbnails async",
        "gotchas": [
          "Don't wait for thumbnail generation",
          "Use pre-signed URLs for direct S3 upload to reduce server load"
        ]
      },
      {
        "requirement": "getNewsFeed - Personalized feed in <500ms",
        "how_met": "Hybrid push/pull: Pre-computed feeds cached in Redis for normal users, pull for celebrity posts at read time, merge and rank",
        "gotchas": [
          "Celebrity problem breaks pure push model",
          "Need cursor-based pagination, not offset"
        ]
      },
      {
        "requirement": "getUserTimeline - All posts by user chronologically",
        "how_met": "Direct query to Posts table partitioned by userId. Much simpler than news feed - no fan-in required",
        "gotchas": [
          "Still need pagination",
          "Private account visibility check"
        ]
      },
      {
        "requirement": "followUser - Update social graph",
        "how_met": "Graph database (Neo4j) or dedicated Follows table. Trigger async feed backfill for new follower",
        "gotchas": [
          "Celebrity followers should NOT trigger immediate fan-out",
          "Need bi-directional tracking: followers AND following"
        ]
      },
      {
        "requirement": "likePost - O(1) like with count update",
        "how_met": "Immediate like record in Likes table, async counter update in Redis with periodic DB sync",
        "gotchas": [
          "Race conditions in counting",
          "Need to prevent duplicate likes from same user"
        ]
      }
    ],
    "complexity_targets": [
      {
        "operation": "uploadPhoto",
        "target": "<2s perceived latency",
        "achieved": "~100ms to postId",
        "why": "S3 upload is sync (100-500ms for 2MB), everything else async via queue"
      },
      {
        "operation": "getNewsFeed",
        "target": "<500ms P99",
        "achieved": "~50-100ms cache hit, ~300ms cache miss",
        "why": "Pre-computed feeds in Redis, only merge 20-50 celebrity posts on read"
      },
      {
        "operation": "getUserTimeline",
        "target": "<200ms",
        "achieved": "~50ms",
        "why": "Single partition scan on Posts table by userId"
      },
      {
        "operation": "followUser",
        "target": "<100ms",
        "achieved": "~50ms",
        "why": "Single row insert, async feed update"
      },
      {
        "operation": "likePost",
        "target": "<50ms",
        "achieved": "~20ms",
        "why": "Single row insert, async counter"
      }
    ],
    "non_goals": [
      "Real-time chat/messaging (Part 4 follow-up)",
      "Stories/ephemeral content (Part 2 follow-up)",
      "Search/explore/discovery (Part 3 follow-up)",
      "Ads and monetization",
      "Content moderation/ML ranking (mention but don't design)",
      "Video streaming (encoding, HLS, etc.)"
    ]
  },
  "assumptions": [
    "Eventual consistency is acceptable for like/comment counts (can be slightly stale)",
    "Feed can be slightly stale (seconds to minutes) - not real-time",
    "Photos are immutable after upload (no editing, only delete)",
    "We're designing for a single product (not multi-tenant)",
    "Budget is not a constraint - we can use managed cloud services",
    "We can use existing CDN providers (CloudFront, Akamai) - not designing CDN from scratch"
  ],
  "tradeoffs": [
    {
      "decision": "Push vs Pull Feed Generation",
      "chosen": "Hybrid - Push for normal users, Pull for celebrities (>10K followers)",
      "why": "Pure push doesn't scale for celebrities (fan-out of 500M writes per post). Pure pull is too slow for users following many accounts",
      "alternative": "Pure push with chunked async processing",
      "when_to_switch": "If average followers per user is very low (<100), pure push works"
    },
    {
      "decision": "SQL vs NoSQL for Posts",
      "chosen": "NoSQL (Cassandra) for Posts table",
      "why": "Need horizontal scaling, eventual consistency is OK, high write throughput for fan-out",
      "alternative": "PostgreSQL with sharding",
      "when_to_switch": "If strong consistency needed for posts or complex queries required"
    },
    {
      "decision": "Pre-compute vs On-demand Feed",
      "chosen": "Pre-compute and cache feeds",
      "why": "100:1 read:write ratio means compute-on-write is more efficient",
      "alternative": "Compute on read with aggressive caching",
      "when_to_switch": "If personalization becomes too dynamic/complex for pre-computation"
    },
    {
      "decision": "Direct Upload vs Presigned URLs",
      "chosen": "Presigned URLs for S3 direct upload",
      "why": "Reduces server bandwidth and load - client uploads directly to S3",
      "alternative": "Proxy through application servers",
      "when_to_switch": "If need server-side validation before upload"
    },
    {
      "decision": "Normalized vs Denormalized Schema",
      "chosen": "Denormalized for read performance",
      "why": "Store author info with each post to avoid joins at read time",
      "alternative": "Normalized with joins",
      "when_to_switch": "If storage cost becomes prohibitive or data consistency issues emerge"
    }
  ],
  "extensibility_and_followups": {
    "design_principles": [
      "**Microservices Boundary**: Each service (Upload, Feed, User, Social Graph) is independently deployable and scalable",
      "**Event-Driven Architecture**: Changes propagate via message queues, making it easy to add new consumers",
      "**API Gateway Pattern**: Single entry point for clients, easy to add new features behind the gateway",
      "**Polyglot Persistence**: Use the right database for each use case, not one-size-fits-all"
    ],
    "why_this_design_scales": "## Extensibility Points\n\n1. **Message Queue** allows adding new workers (ML moderation, analytics, recommendations) without changing upload path\n2. **Separate Feed Service** can evolve ranking algorithm independently\n3. **CDN layer** can add edge computing for personalization\n4. **Event sourcing** in social graph enables new features like 'suggested friends'",
    "expected_followup_hooks": [
      "**Stories (Part 2)**: Add TTL to posts, separate Stories table with expiration, Redis sorted sets for story ordering",
      "**Search (Part 3)**: Add Elasticsearch, index hashtags and captions, inverted index for user search",
      "**DMs (Part 4)**: Separate messaging service, WebSocket connections, different consistency model (stronger)",
      "**Reels/Video**: Add video processing pipeline, HLS encoding, separate video CDN",
      "**ML Ranking**: Feed service can add ranking layer without changing storage"
    ],
    "invariants": [
      "A post always belongs to exactly one user",
      "A user can only like a post once",
      "Following is not symmetric (A follows B doesn't mean B follows A)",
      "Deleted posts should eventually be removed from all feeds",
      "Celebrity threshold (10K) determines push/pull strategy"
    ]
  },
  "visual_explanation": {
    "problem_visualization": "```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        INSTAGRAM SCALE CONTEXT                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502                         500 MILLION DAILY ACTIVE USERS                    \u2502\n   \u2502                                                                          \u2502\n   \u2502   \ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64       \u2502\n   \u2502   \ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64\ud83d\udc64       \u2502\n   \u2502   Each dot = 7.8 million users                                           \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n   Daily Data Flow:\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502   100 Million   \u2502     \u2502    200 TB/day   \u2502     \u2502  100K feed      \u2502\n   \u2502   Photos/Day    \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   New Storage   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502  requests/sec   \u2502\n   \u2502   (1,157/sec)   \u2502     \u2502                 \u2502     \u2502                 \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n   The Challenge:\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502                    THE CELEBRITY PROBLEM                        \u2502\n   \u2502                                                                 \u2502\n   \u2502   @celebrity_kim has 350,000,000 followers                      \u2502\n   \u2502                                                                 \u2502\n   \u2502   If she posts once:                                            \u2502\n   \u2502   \u2022 Fan-out = 350M write operations                             \u2502\n   \u2502   \u2022 At 1M writes/sec = 350 seconds (6 minutes!)                 \u2502\n   \u2502   \u2022 Storage duplication = 350M \u00d7 100 bytes = 35 GB per post     \u2502\n   \u2502                                                                 \u2502\n   \u2502   \u274c Pure push model BREAKS for celebrities                     \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
    "data_structure_state": "```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                           DATA STORES OVERVIEW                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 USERS TABLE (PostgreSQL - Strong Consistency)                                \u2502\n  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n  \u2502 user_id (PK) \u2502 username \u2502 email    \u2502 followers \u2502 following \u2502 is_celebrity   \u2502\n  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\n  \u2502 u_001        \u2502 alice    \u2502 a@ex.com \u2502 523       \u2502 120       \u2502 false          \u2502\n  \u2502 u_002        \u2502 kim_k    \u2502 k@ex.com \u2502 350M      \u2502 200       \u2502 true           \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 POSTS TABLE (Cassandra - Partition by user_id)                               \u2502\n  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n  \u2502 user_id \u2502 post_id    \u2502 caption     \u2502 image_urls          \u2502 created_at       \u2502\n  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\n  \u2502 u_001   \u2502 p_abc123   \u2502 Sunset!     \u2502 {thumb:..., lg:...} \u2502 2024-01-15 10:30 \u2502\n  \u2502 u_001   \u2502 p_def456   \u2502 Coffee time \u2502 {thumb:..., lg:...} \u2502 2024-01-14 08:15 \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 FEED CACHE (Redis - Sorted Set per user)                                     \u2502\n  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n  \u2502 Key: feed:u_001                                                              \u2502\n  \u2502 Value: Sorted Set by timestamp/score                                         \u2502\n  \u2502                                                                              \u2502\n  \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                      \u2502\n  \u2502 \u2502 post_id    \u2502 author_id     \u2502 score  \u2502  \u2190 Most recent posts from            \u2502\n  \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524    people u_001 follows              \u2502\n  \u2502 \u2502 p_xyz789   \u2502 u_005         \u2502 170530 \u2502                                      \u2502\n  \u2502 \u2502 p_abc123   \u2502 u_002         \u2502 170529 \u2502                                      \u2502\n  \u2502 \u2502 p_qrs456   \u2502 u_003         \u2502 170528 \u2502                                      \u2502\n  \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                      \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 SOCIAL GRAPH (Cassandra or Graph DB)                                         \u2502\n  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n  \u2502 Followers Table (Who follows X?)      \u2502 Following Table (Who does X follow?) \u2502\n  \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2502\n  \u2502 followee_id \u2502 follower_id \u2502 timestamp\u2502 follower_id \u2502 followee_id \u2502 timestamp \u2502\n  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\n  \u2502 u_002       \u2502 u_001       \u2502 2024-01  \u2502 u_001       \u2502 u_002       \u2502 2024-01   \u2502\n  \u2502 u_002       \u2502 u_003       \u2502 2024-01  \u2502 u_001       \u2502 u_005       \u2502 2024-01   \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
    "algorithm_flow": [
      {
        "step": 1,
        "description": "User uploads photo via mobile app",
        "visualization": "```\n\ud83d\udcf1 Mobile App\n     \u2502\n     \u2502 POST /api/v1/photos\n     \u2502 Body: {caption, tags, location}\n     \u2502 File: photo.jpg (2MB)\n     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 API Gateway \u2502\u2500\u2500\u2500 Rate limiting\n\u2502             \u2502\u2500\u2500\u2500 Authentication\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Upload    \u2502\u2500\u2500\u2500 Returns presigned S3 URL\n\u2502   Service   \u2502\u2500\u2500\u2500 Generates post_id\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
        "key_point": "API Gateway handles auth, rate limiting. Upload Service returns presigned URL for direct S3 upload."
      },
      {
        "step": 2,
        "description": "Client uploads directly to S3, triggers processing",
        "visualization": "```\n\ud83d\udcf1 Mobile App\n     \u2502\n     \u2502 PUT (presigned URL)\n     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     S3      \u2502\u2500\u2500\u2500 Stores original (2MB)\n\u2502   Bucket    \u2502\u2500\u2500\u2500 Emits S3 event\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502 S3 Event Notification\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Message   \u2502\u2500\u2500\u2500 SQS/Kafka\n\u2502    Queue    \u2502\u2500\u2500\u2500 Decouples upload from processing\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Worker    \u2502\u2500\u2500\u2500 Generate thumbnails\n\u2502    Fleet    \u2502\u2500\u2500\u2500 Extract EXIF\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    Convert to WebP\n```",
        "key_point": "Async processing via message queue. User gets postId immediately, doesn't wait for thumbnails."
      },
      {
        "step": 3,
        "description": "Workers process image and trigger fan-out",
        "visualization": "```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    IMAGE PROCESSING WORKER                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                   \u2502                   \u2502\n        \u25bc                   \u25bc                   \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 150x150 \u2502         \u2502 640x640 \u2502         \u25021080x1080\u2502\n   \u2502  WebP   \u2502         \u2502  WebP   \u2502         \u2502  WebP   \u2502\n   \u2502  ~15KB  \u2502         \u2502 ~120KB  \u2502         \u2502 ~300KB  \u2502\n   \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n        \u2502                   \u2502                   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            \u2502\n                            \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  CDN Push   \u2502\n                    \u2502 (CloudFront)\u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\n              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n              \u2502 Check: is_celebrity?   \u2502\n              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502 NO (<10K followers)             \u2502 YES (\u226510K followers)\n         \u25bc                                 \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502  FAN-OUT    \u2502                  \u2502  STORE IN   \u2502\n   \u2502  to Redis   \u2502                  \u2502  Celebrity  \u2502\n   \u2502  feeds of   \u2502                  \u2502  Posts Table\u2502\n   \u2502  followers  \u2502                  \u2502 (no fan-out)\u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```",
        "key_point": "Hybrid approach: Push for normal users, store-only for celebrities. This is the KEY insight."
      },
      {
        "step": 4,
        "description": "User requests their news feed",
        "visualization": "```\n\ud83d\udcf1 Mobile App\n     \u2502\n     \u2502 GET /api/v1/feed?cursor=xxx\n     \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Feed     \u2502\n\u2502   Service   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u251c\u2500\u2500\u2500 1. Check Redis cache\n       \u2502       feed:user_123\n       \u2502\n       \u251c\u2500\u2500\u2500 2. Get celebrity posts\n       \u2502       (user follows celebrities?)\n       \u2502       Pull recent posts from\n       \u2502       celebrity_posts table\n       \u2502\n       \u251c\u2500\u2500\u2500 3. Merge & Rank\n       \u2502       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n       \u2502       \u2502 Cached feed posts  \u2502\n       \u2502       \u2502 +                  \u2502\n       \u2502       \u2502 Celebrity posts    \u2502\n       \u2502       \u2502 =                  \u2502\n       \u2502       \u2502 Ranked by:         \u2502\n       \u2502       \u2502  - Recency         \u2502\n       \u2502       \u2502  - Engagement      \u2502\n       \u2502       \u2502  - Relationship    \u2502\n       \u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n       \u2514\u2500\u2500\u2500 4. Return paginated response\n\n Response Time: ~50-100ms (cache hit)\n                ~200-300ms (cache miss + celebrity pull)\n```",
        "key_point": "Feed = Pre-computed cached posts + Real-time celebrity pull + Merge & Rank"
      }
    ],
    "dry_run_table": "| Step | Operation | Database Writes | Cache Updates | Result |\n|------|-----------|-----------------|---------------|--------|\n| 1 | Alice (u_001) follows Bob (u_002, 500 followers) | Followers[u_002].add(u_001) | - | Follow recorded |\n| 2 | Alice follows Kim (u_003, 350M followers, celebrity) | Followers[u_003].add(u_001) | - | Follow recorded (no feed backfill) |\n| 3 | Bob posts photo | Posts.insert(p_001, u_002) | Fan-out to 500 feeds including feed:u_001 | Alice's cached feed has Bob's post |\n| 4 | Kim posts photo | Posts.insert(p_002, u_003), CelebrityPosts.insert(p_002) | NO fan-out | Post stored, NOT in Alice's cache |\n| 5 | Alice requests feed | - | Read feed:u_001, Query CelebrityPosts for Kim | Returns [Bob's post, Kim's post] merged |"
  },
  "thinking_process": {
    "step_by_step": [
      "**When I see '500M DAU, 100M photos/day'**, I immediately think: this needs horizontal scaling, no single database can handle this. I need sharding strategy.",
      "**When I see '100:1 read:write ratio'**, I think: compute on write, not read. Pre-compute feeds, aggressive caching. Eventual consistency is likely acceptable.",
      "**When I see 'celebrity followers up to 500M'**, I recognize the classic celebrity problem. Pure fan-out-on-write breaks. Need hybrid push/pull.",
      "**When I see '200TB storage per day'**, I know: this MUST be object storage (S3), NOT database. CDN is essential.",
      "**When I see 'feed latency < 500ms'**, I think: pre-computed feeds in Redis (in-memory), not computing on demand.",
      "**The key insight is**: split users into 'normal' (< 10K followers) and 'celebrities' (>= 10K). Different fan-out strategy for each.",
      "**I should use a message queue because**: it decouples the upload (fast, user-facing) from processing (slow, async). User doesn't wait for thumbnails."
    ],
    "key_insight": "## The Hybrid Feed Generation Strategy\n\nThe crucial realization is that **one-size-fits-all feed generation doesn't work**:\n\n```\nNORMAL USER (< 10K followers):\n  \u2514\u2500 PUSH model: Fan-out post to all followers' feeds on write\n  \u2514\u2500 Expensive writes, cheap reads\n  \u2514\u2500 Works because most users have few followers\n\nCELEBRITY (>= 10K followers):\n  \u2514\u2500 PULL model: Store post, let followers pull on read\n  \u2514\u2500 Cheap writes, slightly more expensive reads\n  \u2514\u2500 Works because celebrity posts are pulled frequently anyway\n\nFINAL FEED = Merge(Pre-computed normal posts, Pulled celebrity posts)\n```\n\nThis is Instagram's actual architecture (as described in public engineering blogs).",
    "why_this_works": "## Why Hybrid Works\n\n1. **Normal users (99% of accounts)**: Fan-out is cheap (average 200 followers \u00d7 100 bytes = 20KB per post)\n\n2. **Celebrities (1% of accounts, 50% of feed content)**: No fan-out, but their posts are so popular that caching at the CDN/Redis level covers most reads anyway\n\n3. **Merge cost is small**: User follows maybe 5-10 celebrities on average. Pulling 5-10 recent posts is O(1) with proper indexing.\n\n4. **Edge case: User follows 10,000 celebrities**: Limit followees, or tier celebrities (top 50 pulled, rest excluded from main feed)"
  },
  "approaches": [
    {
      "name": "Approach 1: Pure Push (Fan-out on Write)",
      "description": "When a user posts, immediately write to all followers' pre-computed feeds in cache/database.",
      "pseudocode": "def post_photo(user_id, photo):\n    post = save_to_posts_db(user_id, photo)\n    followers = get_all_followers(user_id)  # Could be 500M!\n    \n    for follower in followers:  # O(N) where N = follower count\n        append_to_feed(follower.id, post.id)\n    \n    return post.id",
      "time_complexity": "O(F) per post where F = follower count. For celebrity: O(500M)",
      "space_complexity": "O(F) feed entries per post (massive duplication)",
      "pros": [
        "Feed reads are O(1) - just fetch pre-computed feed",
        "Simple read path",
        "Low read latency"
      ],
      "cons": [
        "Celebrity posts take minutes to fan-out",
        "Massive storage duplication (350M \u00d7 100 bytes = 35GB per celebrity post)",
        "High write amplification"
      ],
      "when_to_use": "Small-scale apps where max followers < 10K and no celebrities"
    },
    {
      "name": "Approach 2: Pure Pull (Fan-out on Read)",
      "description": "When a user requests their feed, query all followees' recent posts and merge them in real-time.",
      "pseudocode": "def get_feed(user_id):\n    followees = get_all_followees(user_id)  # Could be 500+\n    \n    all_posts = []\n    for followee in followees:  # O(F) queries\n        posts = get_recent_posts(followee.id, limit=20)\n        all_posts.extend(posts)\n    \n    return rank_and_paginate(all_posts)",
      "time_complexity": "O(F \u00d7 P) per read where F = followees, P = posts per followee",
      "space_complexity": "O(1) - no pre-computation needed",
      "pros": [
        "No write amplification",
        "Storage efficient",
        "Simple write path"
      ],
      "cons": [
        "Slow reads (querying 500+ followees)",
        "Doesn't meet < 500ms latency requirement",
        "High read amplification for active users"
      ],
      "when_to_use": "When users follow very few accounts (< 50) and posting is rare"
    },
    {
      "name": "Optimal: Hybrid Push/Pull with Celebrity Threshold",
      "description": "Push for normal users (< 10K followers), Pull for celebrities (>= 10K). Merge at read time.",
      "pseudocode": "CELEBRITY_THRESHOLD = 10_000\n\ndef post_photo(user_id, photo):\n    post = save_to_posts_db(user_id, photo)\n    \n    if user.follower_count < CELEBRITY_THRESHOLD:\n        # PUSH: Fan-out to all followers\n        async_fan_out_to_followers(user_id, post.id)\n    else:\n        # No fan-out - post is pulled on read\n        save_to_celebrity_posts(post.id)\n    \n    return post.id\n\ndef get_feed(user_id):\n    # 1. Get pre-computed feed (normal users' posts)\n    cached_feed = redis.zrevrange(f'feed:{user_id}', 0, 100)\n    \n    # 2. Pull celebrity posts\n    celebrities = get_celebrity_followees(user_id)  # Usually 5-10\n    celebrity_posts = get_recent_celebrity_posts(celebrities)\n    \n    # 3. Merge and rank\n    return merge_and_rank(cached_feed, celebrity_posts)",
      "time_complexity": "Write: O(F) for normal, O(1) for celebrities. Read: O(C \u00d7 P) where C = celebrity followees (small)",
      "space_complexity": "O(F) for normal users' fan-out (manageable since F < 10K)",
      "pros": [
        "Handles celebrity problem gracefully",
        "Fast reads with caching",
        "Reasonable write amplification",
        "Scalable to Instagram's scale"
      ],
      "cons": [
        "More complex implementation",
        "Need to manage threshold and handle edge cases",
        "Celebrity posts have slightly higher read latency"
      ],
      "key_insight": "The 10K threshold is tunable. Instagram reportedly uses ~10K. The key is recognizing that different user types need different strategies."
    }
  ],
  "optimal_solution": {
    "name": "Instagram Clone - Hybrid Feed Architecture",
    "explanation_md": "## Architecture Overview\n\n### Core Components\n\n1. **API Gateway**: Rate limiting, authentication, routing\n2. **Upload Service**: Handle photo uploads, generate presigned URLs\n3. **Media Processing Workers**: Thumbnail generation, compression\n4. **Feed Service**: Generate and serve personalized feeds\n5. **Social Graph Service**: Manage follow relationships\n6. **User Service**: User profiles and authentication\n\n### Database Choices\n\n| Store | Technology | Why |\n|-------|------------|-----|\n| User Profiles | PostgreSQL | Strong consistency for auth, ACID for financial data |\n| Posts | Cassandra | High write throughput, partition by user_id |\n| Feed Cache | Redis Cluster | Sub-ms latency, sorted sets for ranking |\n| Social Graph | Cassandra | Bi-directional queries, high scale |\n| Photos | S3 | Cheap, durable, infinite scale |\n| CDN | CloudFront | Global edge caching, < 50ms latency |",
    "data_structures": [
      {
        "structure": "Redis Sorted Set (feed:{user_id})",
        "purpose": "Pre-computed feed with score (timestamp or relevance). O(log N) insert, O(K) range fetch"
      },
      {
        "structure": "Cassandra Posts Table (partition by user_id)",
        "purpose": "Fast writes and reads for user timeline. Partition ensures data locality"
      },
      {
        "structure": "Cassandra Followers/Following Tables",
        "purpose": "Bi-directional social graph. Query both directions efficiently"
      },
      {
        "structure": "S3 with CDN",
        "purpose": "Blob storage with global edge caching for photos"
      },
      {
        "structure": "Kafka/SQS Message Queue",
        "purpose": "Decouple upload from processing, enable async fan-out"
      }
    ],
    "algorithm_steps": [
      "## Photo Upload Flow\n1. Client authenticates and requests presigned S3 URL\n2. Client uploads directly to S3 (bypasses servers)\n3. S3 event triggers message to queue\n4. Worker processes image: resize, compress, convert to WebP\n5. Worker saves thumbnails back to S3\n6. Worker inserts post metadata to Cassandra\n7. If normal user: async fan-out to followers' Redis feeds\n8. CDN invalidation triggered for new images",
      "## Feed Generation Flow\n1. Client requests feed with cursor\n2. Feed Service checks Redis cache (feed:{user_id})\n3. If cache miss: rebuild from Posts table\n4. Fetch celebrity followees list\n5. Pull recent posts from celebrities (parallel queries)\n6. Merge cached feed + celebrity posts\n7. Apply ranking algorithm\n8. Return paginated response with next cursor",
      "## Follow Flow\n1. Insert into Followers and Following tables\n2. If followee is normal user: backfill recent posts to follower's feed\n3. If followee is celebrity: just update followee list (no backfill)\n4. Update follower/following counts (async, eventual consistency)"
    ],
    "why_decimal": "N/A for this system design (no currency involved)"
  },
  "solution_python_lines": [
    "\"\"\"",
    "Instagram Clone - Core Services Implementation",
    "",
    "This demonstrates the key algorithms and data structures for Instagram.",
    "Production code would be distributed across microservices.",
    "\"\"\"",
    "",
    "import time",
    "import uuid",
    "import heapq",
    "from abc import ABC, abstractmethod",
    "from dataclasses import dataclass, field",
    "from typing import List, Dict, Set, Optional, Tuple",
    "from collections import defaultdict",
    "from enum import Enum",
    "",
    "",
    "# ============================================================================",
    "# DATA MODELS",
    "# ============================================================================",
    "",
    "@dataclass",
    "class User:",
    "    \"\"\"Represents a user in the system.\"\"\"",
    "    user_id: str",
    "    username: str",
    "    follower_count: int = 0",
    "    following_count: int = 0",
    "    is_celebrity: bool = False",
    "    ",
    "    CELEBRITY_THRESHOLD: int = 10_000",
    "    ",
    "    def update_celebrity_status(self) -> None:",
    "        \"\"\"Update celebrity status based on follower count.\"\"\"",
    "        self.is_celebrity = self.follower_count >= self.CELEBRITY_THRESHOLD",
    "",
    "",
    "@dataclass",
    "class Post:",
    "    \"\"\"Represents a photo post.\"\"\"",
    "    post_id: str",
    "    user_id: str",
    "    caption: str",
    "    image_urls: Dict[str, str]  # {\"thumb\": url, \"medium\": url, \"large\": url}",
    "    created_at: float  # Unix timestamp",
    "    like_count: int = 0",
    "    comment_count: int = 0",
    "",
    "",
    "@dataclass",
    "class FeedItem:",
    "    \"\"\"An item in a user's feed with ranking score.\"\"\"",
    "    post_id: str",
    "    user_id: str",
    "    score: float  # For ranking (could be timestamp or relevance score)",
    "    created_at: float",
    "    is_celebrity_post: bool = False",
    "    ",
    "    def __lt__(self, other: 'FeedItem') -> bool:",
    "        \"\"\"For heap operations - higher score = higher priority.\"\"\"",
    "        return self.score > other.score",
    "",
    "",
    "@dataclass",
    "class FeedResponse:",
    "    \"\"\"Paginated feed response.\"\"\"",
    "    posts: List[Post]",
    "    next_cursor: Optional[str]",
    "    has_more: bool",
    "",
    "",
    "# ============================================================================",
    "# SIMULATED DATA STORES (In production: Redis, Cassandra, S3)",
    "# ============================================================================",
    "",
    "class InMemoryRedis:",
    "    \"\"\"",
    "    Simulates Redis for feed caching.",
    "    ",
    "    In production, this would be Redis Cluster with sorted sets.",
    "    Key: feed:{user_id}",
    "    Value: Sorted set of (post_id, score)",
    "    \"\"\"",
    "    ",
    "    def __init__(self):",
    "        self._feeds: Dict[str, List[Tuple[float, str]]] = defaultdict(list)",
    "        self._max_feed_size = 1000  # Keep only most recent 1000 posts",
    "    ",
    "    def zadd(self, key: str, score: float, member: str) -> None:",
    "        \"\"\"Add member to sorted set with score.\"\"\"",
    "        feed = self._feeds[key]",
    "        heapq.heappush(feed, (-score, member))  # Negative for max-heap behavior",
    "        ",
    "        # Trim to max size",
    "        while len(feed) > self._max_feed_size:",
    "            heapq.heappop(feed)",
    "    ",
    "    def zrevrange(self, key: str, start: int, stop: int) -> List[str]:",
    "        \"\"\"Get members by score descending (most recent first).\"\"\"",
    "        feed = sorted(self._feeds[key])",
    "        return [member for _, member in feed[start:stop + 1]]",
    "    ",
    "    def exists(self, key: str) -> bool:",
    "        return key in self._feeds and len(self._feeds[key]) > 0",
    "",
    "",
    "class InMemoryCassandra:",
    "    \"\"\"",
    "    Simulates Cassandra for posts and social graph.",
    "    ",
    "    In production, this would be Cassandra with proper partitioning:",
    "    - Posts: partition by user_id",
    "    - Followers: partition by followee_id",
    "    - Following: partition by follower_id",
    "    \"\"\"",
    "    ",
    "    def __init__(self):",
    "        self._posts: Dict[str, Post] = {}",
    "        self._user_posts: Dict[str, List[str]] = defaultdict(list)  # user_id -> [post_ids]",
    "        self._followers: Dict[str, Set[str]] = defaultdict(set)  # user_id -> {follower_ids}",
    "        self._following: Dict[str, Set[str]] = defaultdict(set)  # user_id -> {followee_ids}",
    "        self._celebrity_posts: List[Tuple[float, str]] = []  # (timestamp, post_id)",
    "    ",
    "    def insert_post(self, post: Post) -> None:",
    "        self._posts[post.post_id] = post",
    "        self._user_posts[post.user_id].append(post.post_id)",
    "    ",
    "    def get_post(self, post_id: str) -> Optional[Post]:",
    "        return self._posts.get(post_id)",
    "    ",
    "    def get_user_posts(self, user_id: str, limit: int = 20) -> List[Post]:",
    "        \"\"\"Get user's posts, most recent first.\"\"\"",
    "        post_ids = self._user_posts[user_id][-limit:][::-1]",
    "        return [self._posts[pid] for pid in post_ids if pid in self._posts]",
    "    ",
    "    def add_follower(self, followee_id: str, follower_id: str) -> None:",
    "        self._followers[followee_id].add(follower_id)",
    "        self._following[follower_id].add(followee_id)",
    "    ",
    "    def remove_follower(self, followee_id: str, follower_id: str) -> None:",
    "        self._followers[followee_id].discard(follower_id)",
    "        self._following[follower_id].discard(followee_id)",
    "    ",
    "    def get_followers(self, user_id: str) -> Set[str]:",
    "        return self._followers[user_id].copy()",
    "    ",
    "    def get_following(self, user_id: str) -> Set[str]:",
    "        return self._following[user_id].copy()",
    "    ",
    "    def add_celebrity_post(self, post: Post) -> None:",
    "        heapq.heappush(self._celebrity_posts, (-post.created_at, post.post_id))",
    "    ",
    "    def get_celebrity_posts(self, celebrity_ids: Set[str], limit: int = 20) -> List[Post]:",
    "        \"\"\"Get recent posts from specified celebrities.\"\"\"",
    "        posts = []",
    "        for post_id in [pid for _, pid in sorted(self._celebrity_posts)]:",
    "            post = self._posts.get(post_id)",
    "            if post and post.user_id in celebrity_ids:",
    "                posts.append(post)",
    "                if len(posts) >= limit:",
    "                    break",
    "        return posts",
    "",
    "",
    "# ============================================================================",
    "# CORE SERVICES",
    "# ============================================================================",
    "",
    "class FeedService:",
    "    \"\"\"",
    "    Handles news feed generation using hybrid push/pull model.",
    "    ",
    "    Key Algorithm:",
    "    1. Normal users (<10K followers): Push posts to all followers' feeds",
    "    2. Celebrities (>=10K followers): Store post, pull on read",
    "    3. On read: Merge pre-computed feed + celebrity posts + rank",
    "    \"\"\"",
    "    ",
    "    def __init__(self, redis: InMemoryRedis, cassandra: InMemoryCassandra):",
    "        self._redis = redis",
    "        self._cassandra = cassandra",
    "        self._users: Dict[str, User] = {}",
    "    ",
    "    def register_user(self, user: User) -> None:",
    "        \"\"\"Register a user in the system.\"\"\"",
    "        self._users[user.user_id] = user",
    "    ",
    "    def get_user(self, user_id: str) -> Optional[User]:",
    "        return self._users.get(user_id)",
    "    ",
    "    def fan_out_post(self, post: Post, author: User) -> None:",
    "        \"\"\"",
    "        Fan out a new post to followers' feeds.",
    "        ",
    "        Uses hybrid strategy:",
    "        - Normal user: Push to all followers' Redis feeds",
    "        - Celebrity: Store in celebrity posts table (no fan-out)",
    "        \"\"\"",
    "        if author.is_celebrity:",
    "            # PULL model for celebrities - just store the post",
    "            self._cassandra.add_celebrity_post(post)",
    "            print(f\"  [Celebrity] Post {post.post_id} stored (no fan-out)\")",
    "        else:",
    "            # PUSH model for normal users - fan out to all followers",
    "            followers = self._cassandra.get_followers(author.user_id)",
    "            for follower_id in followers:",
    "                feed_key = f\"feed:{follower_id}\"",
    "                self._redis.zadd(feed_key, post.created_at, post.post_id)",
    "            print(f\"  [Normal] Post {post.post_id} fanned out to {len(followers)} followers\")",
    "    ",
    "    def get_news_feed(self, user_id: str, page_size: int = 10, ",
    "                      cursor: Optional[str] = None) -> FeedResponse:",
    "        \"\"\"",
    "        Get personalized news feed for a user.",
    "        ",
    "        Algorithm:",
    "        1. Fetch pre-computed feed from Redis (posts from normal users)",
    "        2. Identify celebrity followees",
    "        3. Pull recent posts from celebrities",
    "        4. Merge and rank all posts",
    "        5. Return paginated response",
    "        \"\"\"",
    "        start_idx = int(cursor) if cursor else 0",
    "        ",
    "        # Step 1: Get pre-computed feed (normal users' posts)",
    "        feed_key = f\"feed:{user_id}\"",
    "        cached_post_ids = self._redis.zrevrange(feed_key, 0, 100)",
    "        ",
    "        # Step 2: Identify celebrity followees",
    "        following = self._cassandra.get_following(user_id)",
    "        celebrity_followees = {",
    "            uid for uid in following ",
    "            if self._users.get(uid) and self._users[uid].is_celebrity",
    "        }",
    "        ",
    "        # Step 3: Pull celebrity posts",
    "        celebrity_posts = []",
    "        if celebrity_followees:",
    "            celebrity_posts = self._cassandra.get_celebrity_posts(",
    "                celebrity_followees, limit=20",
    "            )",
    "        ",
    "        # Step 4: Merge all posts",
    "        all_posts: List[Post] = []",
    "        seen_ids: Set[str] = set()",
    "        ",
    "        # Add cached posts",
    "        for post_id in cached_post_ids:",
    "            post = self._cassandra.get_post(post_id)",
    "            if post and post.post_id not in seen_ids:",
    "                all_posts.append(post)",
    "                seen_ids.add(post.post_id)",
    "        ",
    "        # Add celebrity posts",
    "        for post in celebrity_posts:",
    "            if post.post_id not in seen_ids:",
    "                all_posts.append(post)",
    "                seen_ids.add(post.post_id)",
    "        ",
    "        # Step 5: Sort by recency (in production: ML ranking)",
    "        all_posts.sort(key=lambda p: p.created_at, reverse=True)",
    "        ",
    "        # Step 6: Paginate",
    "        page = all_posts[start_idx:start_idx + page_size]",
    "        has_more = start_idx + page_size < len(all_posts)",
    "        next_cursor = str(start_idx + page_size) if has_more else None",
    "        ",
    "        return FeedResponse(posts=page, next_cursor=next_cursor, has_more=has_more)",
    "",
    "",
    "class UploadService:",
    "    \"\"\"",
    "    Handles photo uploads and metadata storage.",
    "    ",
    "    In production:",
    "    - Returns presigned S3 URL for direct upload",
    "    - Triggers async processing via message queue",
    "    \"\"\"",
    "    ",
    "    def __init__(self, cassandra: InMemoryCassandra, feed_service: FeedService):",
    "        self._cassandra = cassandra",
    "        self._feed_service = feed_service",
    "    ",
    "    def upload_photo(self, user_id: str, caption: str, ",
    "                     tags: List[str] = None) -> str:",
    "        \"\"\"",
    "        Upload a photo and trigger feed fan-out.",
    "        ",
    "        Args:",
    "            user_id: ID of the user uploading",
    "            caption: Text caption for the post",
    "            tags: List of hashtags",
    "            ",
    "        Returns:",
    "            post_id: Unique identifier for the new post",
    "        \"\"\"",
    "        # Generate unique post ID",
    "        post_id = f\"post_{uuid.uuid4().hex[:8]}\"",
    "        ",
    "        # Simulate image processing - generate thumbnail URLs",
    "        base_url = \"https://cdn.instagram.com\"",
    "        image_urls = {",
    "            \"thumb\": f\"{base_url}/{post_id}_150.webp\",",
    "            \"medium\": f\"{base_url}/{post_id}_640.webp\",",
    "            \"large\": f\"{base_url}/{post_id}_1080.webp\",",
    "        }",
    "        ",
    "        # Create post",
    "        post = Post(",
    "            post_id=post_id,",
    "            user_id=user_id,",
    "            caption=caption,",
    "            image_urls=image_urls,",
    "            created_at=time.time(),",
    "        )",
    "        ",
    "        # Store in database",
    "        self._cassandra.insert_post(post)",
    "        ",
    "        # Fan out to followers (async in production)",
    "        author = self._feed_service.get_user(user_id)",
    "        if author:",
    "            self._feed_service.fan_out_post(post, author)",
    "        ",
    "        return post_id",
    "",
    "",
    "class SocialGraphService:",
    "    \"\"\"",
    "    Manages follow/unfollow relationships.",
    "    \"\"\"",
    "    ",
    "    def __init__(self, cassandra: InMemoryCassandra, feed_service: FeedService):",
    "        self._cassandra = cassandra",
    "        self._feed_service = feed_service",
    "    ",
    "    def follow_user(self, follower_id: str, followee_id: str) -> bool:",
    "        \"\"\"",
    "        Create a follow relationship.",
    "        ",
    "        Steps:",
    "        1. Add to followers/following tables",
    "        2. Update follower counts",
    "        3. If followee is normal user: backfill recent posts to feed",
    "        \"\"\"",
    "        # Update social graph",
    "        self._cassandra.add_follower(followee_id, follower_id)",
    "        ",
    "        # Update counts",
    "        followee = self._feed_service.get_user(followee_id)",
    "        follower = self._feed_service.get_user(follower_id)",
    "        ",
    "        if followee:",
    "            followee.follower_count += 1",
    "            followee.update_celebrity_status()",
    "        ",
    "        if follower:",
    "            follower.following_count += 1",
    "        ",
    "        print(f\"  {follower_id} followed {followee_id}\")",
    "        return True",
    "    ",
    "    def unfollow_user(self, follower_id: str, followee_id: str) -> bool:",
    "        \"\"\"Remove a follow relationship.\"\"\"",
    "        self._cassandra.remove_follower(followee_id, follower_id)",
    "        ",
    "        followee = self._feed_service.get_user(followee_id)",
    "        follower = self._feed_service.get_user(follower_id)",
    "        ",
    "        if followee and followee.follower_count > 0:",
    "            followee.follower_count -= 1",
    "            followee.update_celebrity_status()",
    "        ",
    "        if follower and follower.following_count > 0:",
    "            follower.following_count -= 1",
    "        ",
    "        print(f\"  {follower_id} unfollowed {followee_id}\")",
    "        return True",
    "",
    "",
    "# ============================================================================",
    "# DEMONSTRATION",
    "# ============================================================================",
    "",
    "def main():",
    "    \"\"\"",
    "    Demonstrate the Instagram clone with hybrid feed generation.",
    "    \"\"\"",
    "    print(\"=\" * 70)",
    "    print(\"INSTAGRAM CLONE - HYBRID FEED DEMONSTRATION\")",
    "    print(\"=\" * 70)",
    "    ",
    "    # Initialize stores",
    "    redis = InMemoryRedis()",
    "    cassandra = InMemoryCassandra()",
    "    ",
    "    # Initialize services",
    "    feed_service = FeedService(redis, cassandra)",
    "    upload_service = UploadService(cassandra, feed_service)",
    "    social_service = SocialGraphService(cassandra, feed_service)",
    "    ",
    "    # Create users",
    "    print(\"\\n\" + \"-\" * 70)",
    "    print(\"Step 1: Create Users\")",
    "    print(\"-\" * 70)",
    "    ",
    "    alice = User(user_id=\"u_alice\", username=\"alice\")",
    "    bob = User(user_id=\"u_bob\", username=\"bob\")",
    "    kim = User(user_id=\"u_kim\", username=\"kim_celebrity\", ",
    "               follower_count=350_000_000, is_celebrity=True)",
    "    ",
    "    for user in [alice, bob, kim]:",
    "        feed_service.register_user(user)",
    "        status = \"[CELEBRITY]\" if user.is_celebrity else \"[Normal]\"",
    "        print(f\"  Created {user.username} {status} - {user.follower_count:,} followers\")",
    "    ",
    "    # Set up follows",
    "    print(\"\\n\" + \"-\" * 70)",
    "    print(\"Step 2: Alice follows Bob and Kim\")",
    "    print(\"-\" * 70)",
    "    ",
    "    social_service.follow_user(\"u_alice\", \"u_bob\")",
    "    social_service.follow_user(\"u_alice\", \"u_kim\")",
    "    ",
    "    # Bob posts (normal user - fan-out happens)",
    "    print(\"\\n\" + \"-\" * 70)",
    "    print(\"Step 3: Bob posts a photo (PUSH - fans out to followers)\")",
    "    print(\"-\" * 70)",
    "    ",
    "    bob_post = upload_service.upload_photo(",
    "        user_id=\"u_bob\",",
    "        caption=\"Beautiful sunset! #travel #photography\",",
    "        tags=[\"travel\", \"photography\"]",
    "    )",
    "    print(f\"  Bob's post ID: {bob_post}\")",
    "    ",
    "    # Kim posts (celebrity - no fan-out, stored for pull)",
    "    print(\"\\n\" + \"-\" * 70)",
    "    print(\"Step 4: Kim posts a photo (PULL - no fan-out, stored for pull)\")",
    "    print(\"-\" * 70)",
    "    ",
    "    time.sleep(0.1)  # Ensure different timestamp",
    "    kim_post = upload_service.upload_photo(",
    "        user_id=\"u_kim\",",
    "        caption=\"New collection drop! #fashion\",",
    "        tags=[\"fashion\"]",
    "    )",
    "    print(f\"  Kim's post ID: {kim_post}\")",
    "    ",
    "    # Alice views her feed",
    "    print(\"\\n\" + \"-\" * 70)",
    "    print(\"Step 5: Alice views her news feed\")",
    "    print(\"-\" * 70)",
    "    ",
    "    feed = feed_service.get_news_feed(\"u_alice\", page_size=10)",
    "    ",
    "    print(f\"  Feed contains {len(feed.posts)} posts:\")",
    "    for i, post in enumerate(feed.posts, 1):",
    "        author = feed_service.get_user(post.user_id)",
    "        post_type = \"[Celebrity Pull]\" if author and author.is_celebrity else \"[Normal Push]\"",
    "        print(f\"    {i}. {author.username if author else 'unknown'} {post_type}\")",
    "        print(f\"       Caption: {post.caption}\")",
    "        print(f\"       Thumb: {post.image_urls['thumb']}\")",
    "    ",
    "    print(\"\\n\" + \"=\" * 70)",
    "    print(\"KEY INSIGHT: Bob's post was PRE-COMPUTED in Alice's feed (push).\")",
    "    print(\"Kim's post was PULLED at read time (no fan-out to 350M followers).\")",
    "    print(\"=\" * 70)",
    "",
    "",
    "if __name__ == \"__main__\":",
    "    main()"
  ],
  "solution_java_lines": [
    "import java.util.*;",
    "import java.util.concurrent.ConcurrentHashMap;",
    "",
    "/**",
    " * Instagram Clone - Core Services Implementation in Java",
    " * ",
    " * Demonstrates hybrid push/pull feed generation.",
    " */",
    "public class InstagramClone {",
    "    ",
    "    // ========================================================================",
    "    // DATA MODELS",
    "    // ========================================================================",
    "    ",
    "    static class User {",
    "        String userId;",
    "        String username;",
    "        long followerCount;",
    "        long followingCount;",
    "        boolean isCelebrity;",
    "        ",
    "        static final long CELEBRITY_THRESHOLD = 10_000L;",
    "        ",
    "        User(String userId, String username) {",
    "            this.userId = userId;",
    "            this.username = username;",
    "            this.followerCount = 0;",
    "            this.isCelebrity = false;",
    "        }",
    "        ",
    "        User(String userId, String username, long followerCount) {",
    "            this.userId = userId;",
    "            this.username = username;",
    "            this.followerCount = followerCount;",
    "            this.isCelebrity = followerCount >= CELEBRITY_THRESHOLD;",
    "        }",
    "        ",
    "        void updateCelebrityStatus() {",
    "            this.isCelebrity = this.followerCount >= CELEBRITY_THRESHOLD;",
    "        }",
    "    }",
    "    ",
    "    static class Post {",
    "        String postId;",
    "        String userId;",
    "        String caption;",
    "        Map<String, String> imageUrls;",
    "        long createdAt;",
    "        int likeCount;",
    "        int commentCount;",
    "        ",
    "        Post(String postId, String userId, String caption) {",
    "            this.postId = postId;",
    "            this.userId = userId;",
    "            this.caption = caption;",
    "            this.imageUrls = new HashMap<>();",
    "            this.createdAt = System.currentTimeMillis();",
    "            this.likeCount = 0;",
    "            this.commentCount = 0;",
    "        }",
    "    }",
    "    ",
    "    static class FeedResponse {",
    "        List<Post> posts;",
    "        String nextCursor;",
    "        boolean hasMore;",
    "        ",
    "        FeedResponse(List<Post> posts, String nextCursor, boolean hasMore) {",
    "            this.posts = posts;",
    "            this.nextCursor = nextCursor;",
    "            this.hasMore = hasMore;",
    "        }",
    "    }",
    "    ",
    "    // ========================================================================",
    "    // SIMULATED DATA STORES",
    "    // ========================================================================",
    "    ",
    "    static class InMemoryRedis {",
    "        private final Map<String, TreeMap<Long, String>> feeds = new ConcurrentHashMap<>();",
    "        private final int MAX_FEED_SIZE = 1000;",
    "        ",
    "        void zadd(String key, long score, String member) {",
    "            feeds.computeIfAbsent(key, k -> new TreeMap<>(Collections.reverseOrder()))",
    "                  .put(score, member);",
    "            ",
    "            // Trim to max size",
    "            TreeMap<Long, String> feed = feeds.get(key);",
    "            while (feed.size() > MAX_FEED_SIZE) {",
    "                feed.pollLastEntry();",
    "            }",
    "        }",
    "        ",
    "        List<String> zrevrange(String key, int start, int stop) {",
    "            TreeMap<Long, String> feed = feeds.get(key);",
    "            if (feed == null) return new ArrayList<>();",
    "            ",
    "            List<String> result = new ArrayList<>();",
    "            int i = 0;",
    "            for (String member : feed.values()) {",
    "                if (i >= start && i <= stop) result.add(member);",
    "                i++;",
    "                if (i > stop) break;",
    "            }",
    "            return result;",
    "        }",
    "    }",
    "    ",
    "    static class InMemoryCassandra {",
    "        private final Map<String, Post> posts = new ConcurrentHashMap<>();",
    "        private final Map<String, List<String>> userPosts = new ConcurrentHashMap<>();",
    "        private final Map<String, Set<String>> followers = new ConcurrentHashMap<>();",
    "        private final Map<String, Set<String>> following = new ConcurrentHashMap<>();",
    "        private final TreeMap<Long, String> celebrityPosts = new TreeMap<>(Collections.reverseOrder());",
    "        ",
    "        void insertPost(Post post) {",
    "            posts.put(post.postId, post);",
    "            userPosts.computeIfAbsent(post.userId, k -> new ArrayList<>()).add(post.postId);",
    "        }",
    "        ",
    "        Post getPost(String postId) {",
    "            return posts.get(postId);",
    "        }",
    "        ",
    "        void addFollower(String followeeId, String followerId) {",
    "            followers.computeIfAbsent(followeeId, k -> ConcurrentHashMap.newKeySet()).add(followerId);",
    "            following.computeIfAbsent(followerId, k -> ConcurrentHashMap.newKeySet()).add(followeeId);",
    "        }",
    "        ",
    "        Set<String> getFollowers(String userId) {",
    "            return followers.getOrDefault(userId, Collections.emptySet());",
    "        }",
    "        ",
    "        Set<String> getFollowing(String userId) {",
    "            return following.getOrDefault(userId, Collections.emptySet());",
    "        }",
    "        ",
    "        void addCelebrityPost(Post post) {",
    "            celebrityPosts.put(post.createdAt, post.postId);",
    "        }",
    "        ",
    "        List<Post> getCelebrityPosts(Set<String> celebrityIds, int limit) {",
    "            List<Post> result = new ArrayList<>();",
    "            for (String postId : celebrityPosts.values()) {",
    "                Post post = posts.get(postId);",
    "                if (post != null && celebrityIds.contains(post.userId)) {",
    "                    result.add(post);",
    "                    if (result.size() >= limit) break;",
    "                }",
    "            }",
    "            return result;",
    "        }",
    "    }",
    "    ",
    "    // ========================================================================",
    "    // CORE SERVICES",
    "    // ========================================================================",
    "    ",
    "    static class FeedService {",
    "        private final InMemoryRedis redis;",
    "        private final InMemoryCassandra cassandra;",
    "        private final Map<String, User> users = new ConcurrentHashMap<>();",
    "        ",
    "        FeedService(InMemoryRedis redis, InMemoryCassandra cassandra) {",
    "            this.redis = redis;",
    "            this.cassandra = cassandra;",
    "        }",
    "        ",
    "        void registerUser(User user) {",
    "            users.put(user.userId, user);",
    "        }",
    "        ",
    "        User getUser(String userId) {",
    "            return users.get(userId);",
    "        }",
    "        ",
    "        void fanOutPost(Post post, User author) {",
    "            if (author.isCelebrity) {",
    "                // PULL model - store for later retrieval",
    "                cassandra.addCelebrityPost(post);",
    "                System.out.println(\"  [Celebrity] Post \" + post.postId + \" stored (no fan-out)\");",
    "            } else {",
    "                // PUSH model - fan out to all followers",
    "                Set<String> followerIds = cassandra.getFollowers(author.userId);",
    "                for (String followerId : followerIds) {",
    "                    redis.zadd(\"feed:\" + followerId, post.createdAt, post.postId);",
    "                }",
    "                System.out.println(\"  [Normal] Post \" + post.postId + ",
    "                    \" fanned out to \" + followerIds.size() + \" followers\");",
    "            }",
    "        }",
    "        ",
    "        FeedResponse getNewsFeed(String userId, int pageSize, String cursor) {",
    "            int startIdx = cursor != null ? Integer.parseInt(cursor) : 0;",
    "            ",
    "            // Step 1: Get pre-computed feed",
    "            List<String> cachedPostIds = redis.zrevrange(\"feed:\" + userId, 0, 100);",
    "            ",
    "            // Step 2: Identify celebrity followees",
    "            Set<String> followingIds = cassandra.getFollowing(userId);",
    "            Set<String> celebrityFollowees = new HashSet<>();",
    "            for (String uid : followingIds) {",
    "                User user = users.get(uid);",
    "                if (user != null && user.isCelebrity) {",
    "                    celebrityFollowees.add(uid);",
    "                }",
    "            }",
    "            ",
    "            // Step 3: Pull celebrity posts",
    "            List<Post> celebrityPosts = cassandra.getCelebrityPosts(celebrityFollowees, 20);",
    "            ",
    "            // Step 4: Merge all posts",
    "            List<Post> allPosts = new ArrayList<>();",
    "            Set<String> seenIds = new HashSet<>();",
    "            ",
    "            for (String postId : cachedPostIds) {",
    "                Post post = cassandra.getPost(postId);",
    "                if (post != null && !seenIds.contains(postId)) {",
    "                    allPosts.add(post);",
    "                    seenIds.add(postId);",
    "                }",
    "            }",
    "            ",
    "            for (Post post : celebrityPosts) {",
    "                if (!seenIds.contains(post.postId)) {",
    "                    allPosts.add(post);",
    "                    seenIds.add(post.postId);",
    "                }",
    "            }",
    "            ",
    "            // Step 5: Sort by recency",
    "            allPosts.sort((a, b) -> Long.compare(b.createdAt, a.createdAt));",
    "            ",
    "            // Step 6: Paginate",
    "            int endIdx = Math.min(startIdx + pageSize, allPosts.size());",
    "            List<Post> page = allPosts.subList(startIdx, endIdx);",
    "            boolean hasMore = endIdx < allPosts.size();",
    "            String nextCursor = hasMore ? String.valueOf(endIdx) : null;",
    "            ",
    "            return new FeedResponse(page, nextCursor, hasMore);",
    "        }",
    "    }",
    "    ",
    "    // ========================================================================",
    "    // MAIN DEMONSTRATION",
    "    // ========================================================================",
    "    ",
    "    public static void main(String[] args) throws InterruptedException {",
    "        System.out.println(\"=\".repeat(70));",
    "        System.out.println(\"INSTAGRAM CLONE - HYBRID FEED DEMONSTRATION (Java)\");",
    "        System.out.println(\"=\".repeat(70));",
    "        ",
    "        // Initialize stores",
    "        InMemoryRedis redis = new InMemoryRedis();",
    "        InMemoryCassandra cassandra = new InMemoryCassandra();",
    "        FeedService feedService = new FeedService(redis, cassandra);",
    "        ",
    "        // Create users",
    "        System.out.println(\"\\n\" + \"-\".repeat(70));",
    "        System.out.println(\"Step 1: Create Users\");",
    "        System.out.println(\"-\".repeat(70));",
    "        ",
    "        User alice = new User(\"u_alice\", \"alice\");",
    "        User bob = new User(\"u_bob\", \"bob\");",
    "        User kim = new User(\"u_kim\", \"kim_celebrity\", 350_000_000L);",
    "        ",
    "        for (User user : Arrays.asList(alice, bob, kim)) {",
    "            feedService.registerUser(user);",
    "            String status = user.isCelebrity ? \"[CELEBRITY]\" : \"[Normal]\";",
    "            System.out.printf(\"  Created %s %s - %,d followers%n\", ",
    "                user.username, status, user.followerCount);",
    "        }",
    "        ",
    "        // Set up follows",
    "        System.out.println(\"\\n\" + \"-\".repeat(70));",
    "        System.out.println(\"Step 2: Alice follows Bob and Kim\");",
    "        System.out.println(\"-\".repeat(70));",
    "        ",
    "        cassandra.addFollower(\"u_bob\", \"u_alice\");",
    "        cassandra.addFollower(\"u_kim\", \"u_alice\");",
    "        bob.followerCount++;",
    "        kim.followerCount++;",
    "        System.out.println(\"  u_alice followed u_bob\");",
    "        System.out.println(\"  u_alice followed u_kim\");",
    "        ",
    "        // Bob posts (normal user)",
    "        System.out.println(\"\\n\" + \"-\".repeat(70));",
    "        System.out.println(\"Step 3: Bob posts (PUSH - fans out to followers)\");",
    "        System.out.println(\"-\".repeat(70));",
    "        ",
    "        Post bobPost = new Post(\"post_bob1\", \"u_bob\", \"Beautiful sunset!\");",
    "        bobPost.imageUrls.put(\"thumb\", \"https://cdn.instagram.com/post_bob1_150.webp\");",
    "        cassandra.insertPost(bobPost);",
    "        feedService.fanOutPost(bobPost, bob);",
    "        ",
    "        // Kim posts (celebrity)",
    "        System.out.println(\"\\n\" + \"-\".repeat(70));",
    "        System.out.println(\"Step 4: Kim posts (PULL - no fan-out)\");",
    "        System.out.println(\"-\".repeat(70));",
    "        ",
    "        Thread.sleep(100); // Different timestamp",
    "        Post kimPost = new Post(\"post_kim1\", \"u_kim\", \"New collection drop!\");",
    "        kimPost.imageUrls.put(\"thumb\", \"https://cdn.instagram.com/post_kim1_150.webp\");",
    "        cassandra.insertPost(kimPost);",
    "        feedService.fanOutPost(kimPost, kim);",
    "        ",
    "        // Alice views feed",
    "        System.out.println(\"\\n\" + \"-\".repeat(70));",
    "        System.out.println(\"Step 5: Alice views her news feed\");",
    "        System.out.println(\"-\".repeat(70));",
    "        ",
    "        FeedResponse feed = feedService.getNewsFeed(\"u_alice\", 10, null);",
    "        System.out.println(\"  Feed contains \" + feed.posts.size() + \" posts:\");",
    "        ",
    "        int i = 1;",
    "        for (Post post : feed.posts) {",
    "            User author = feedService.getUser(post.userId);",
    "            String postType = (author != null && author.isCelebrity) ",
    "                ? \"[Celebrity Pull]\" : \"[Normal Push]\";",
    "            System.out.println(\"    \" + i + \". \" + ",
    "                (author != null ? author.username : \"unknown\") + \" \" + postType);",
    "            System.out.println(\"       Caption: \" + post.caption);",
    "            i++;",
    "        }",
    "        ",
    "        System.out.println(\"\\n\" + \"=\".repeat(70));",
    "        System.out.println(\"KEY: Bob's post was PUSHED. Kim's post was PULLED.\");",
    "        System.out.println(\"=\".repeat(70));",
    "    }",
    "}"
  ],
  "code_walkthrough": [
    {
      "lines": "1-15",
      "section": "Imports and Docstring",
      "explanation": "We import standard library modules. In production, this would be replaced with actual Redis, Cassandra, and S3 client libraries."
    },
    {
      "lines": "20-45",
      "section": "Data Models (User, Post, FeedItem)",
      "explanation": "Core data classes using Python dataclasses. Note the `CELEBRITY_THRESHOLD` constant (10,000) and `is_celebrity` flag - this is the KEY to the hybrid strategy."
    },
    {
      "lines": "55-95",
      "section": "InMemoryRedis (Feed Cache)",
      "explanation": "Simulates Redis sorted sets for feed caching. `zadd` adds posts with timestamp as score. `zrevrange` returns posts in reverse chronological order. In production: Redis Cluster."
    },
    {
      "lines": "100-150",
      "section": "InMemoryCassandra (Posts + Social Graph)",
      "explanation": "Simulates Cassandra tables. Key methods: `add_celebrity_post` for storing celebrity posts without fan-out, `get_celebrity_posts` for pulling them at read time."
    },
    {
      "lines": "160-220",
      "section": "FeedService - Core Hybrid Algorithm",
      "explanation": "**THE KEY ALGORITHM**: `fan_out_post()` checks `is_celebrity` to decide push vs store-only. `get_news_feed()` merges cached feed (from push) with pulled celebrity posts."
    },
    {
      "lines": "225-270",
      "section": "UploadService",
      "explanation": "Handles photo uploads. Generates thumbnail URLs (in production: async via SQS). Triggers fan-out through FeedService."
    },
    {
      "lines": "275-310",
      "section": "SocialGraphService",
      "explanation": "Manages follow/unfollow. Updates follower counts and triggers `update_celebrity_status()` which may flip a user between normal and celebrity."
    },
    {
      "lines": "320-400",
      "section": "Main Demonstration",
      "explanation": "Creates Alice (normal), Bob (normal), Kim (celebrity with 350M followers). Shows that Bob's post fans out to Alice's cached feed, while Kim's post is stored for pull."
    }
  ],
  "debugging_strategy": {
    "how_to_test_incrementally": "1. Test user creation and registration\n2. Test follow relationship storage\n3. Test normal user post fan-out (verify it appears in Redis)\n4. Test celebrity post storage (verify NO fan-out, appears in celebrity table)\n5. Test feed generation with both types\n6. Test pagination",
    "what_to_print_or_assert": [
      "print(f'Followers of {user_id}: {len(followers)}')",
      "print(f'Feed cache for {user_id}: {len(cached_posts)} posts')",
      "print(f'Celebrity followees: {celebrity_ids}')",
      "assert post.post_id in cached_posts, 'Normal post should be in cache'",
      "assert celeb_post.post_id not in cached_posts, 'Celebrity post should NOT be in cache'"
    ],
    "common_failure_modes": [
      "**Celebrity threshold wrong**: User with 10,001 followers treated as normal (off-by-one)",
      "**Race condition in feed**: Post appears before author info is cached",
      "**Missing celebrity posts**: Forgot to call `get_celebrity_posts` in feed generation",
      "**Duplicate posts**: Same post from both cache and celebrity pull"
    ],
    "how_to_fix_fast": "1. Check `is_celebrity` flag immediately after any follower count change\n2. Use set-based deduplication when merging feeds\n3. Log the fan-out path taken for each post"
  },
  "complexity_analysis": {
    "time": {
      "upload_photo": {
        "complexity": "O(1) perceived, O(F) async fan-out",
        "explanation": "User gets postId immediately. Fan-out to F followers happens async."
      },
      "get_news_feed": {
        "complexity": "O(C \u00d7 P + K log K)",
        "explanation": "C = celebrity followees (~5-10), P = posts per celebrity (~20), K = total posts to rank (~100). Very fast in practice."
      },
      "follow_user": {
        "complexity": "O(1)",
        "explanation": "Single row insert to social graph. Backfill is async."
      },
      "like_post": {
        "complexity": "O(1)",
        "explanation": "Single row insert. Counter update is async."
      },
      "overall": "All user-facing operations are O(1) or near-O(1). Async operations scale with follower count."
    },
    "space": {
      "complexity": "O(U \u00d7 F \u00d7 P) for feeds + O(P_total) for posts",
      "breakdown": "- Feed cache: O(U \u00d7 1000) = O(U) posts per user (capped at 1000)\n- Posts table: O(P_total) total posts\n- Social graph: O(E) edges (follows)\n- Photo storage: O(P \u00d7 5MB) in S3 (not in application memory)",
      "note": "Feed storage is the biggest cost. The 1000-post cap per user limits this."
    },
    "can_we_do_better": "The hybrid approach is near-optimal for this scale. Further optimizations:\n1. ML-based feed ranking to reduce stored posts\n2. Tiered caching (hot users in memory, cold in disk)\n3. Geographic sharding to reduce cross-region traffic"
  },
  "dry_run": {
    "example": "Create Alice, Bob, Kim(celebrity). Alice follows both. Bob posts, Kim posts. Alice views feed.",
    "trace_table": "| Step | Operation | Push/Pull | Redis (Alice's Feed) | Celebrity Posts Table |\n|------|-----------|-----------|----------------------|-----------------------|\n| 1 | Create users | - | empty | empty |\n| 2 | Alice follows Bob | - | empty | empty |\n| 3 | Alice follows Kim | - | empty | empty |\n| 4 | Bob posts (100 followers) | PUSH | [bob_post_1] | empty |\n| 5 | Kim posts (350M followers) | PULL (no fan-out) | [bob_post_1] | [kim_post_1] |\n| 6 | Alice requests feed | MERGE | Read: [bob_post_1] | Pull: [kim_post_1] |\n| 7 | Result | - | [kim_post_1, bob_post_1] (merged, ranked by time) |",
    "final_answer": "Alice's feed contains both posts. Bob's was pre-computed (push), Kim's was pulled on demand. No 350M fan-out operations."
  },
  "test_cases": [
    {
      "name": "Normal user post fans out correctly",
      "category": "Happy Path",
      "input": "Bob (100 followers) posts. Check Alice's (follower) cached feed.",
      "expected": "Bob's post appears in Alice's Redis feed cache",
      "explanation": "Fan-out on write for normal users"
    },
    {
      "name": "Celebrity post does NOT fan out",
      "category": "Core Algorithm",
      "input": "Kim (350M followers) posts. Check any follower's cached feed.",
      "expected": "Kim's post is NOT in cached feed, IS in celebrity_posts table",
      "explanation": "No fan-out for celebrities - posts are pulled on read"
    },
    {
      "name": "Feed includes both normal and celebrity posts",
      "category": "Happy Path",
      "input": "Alice follows Bob (normal) and Kim (celebrity). Both post. Alice requests feed.",
      "expected": "Feed contains both posts, ranked by recency",
      "explanation": "Hybrid merge at read time"
    },
    {
      "name": "Celebrity threshold boundary",
      "category": "Edge Case",
      "input": "User has exactly 10,000 followers. Posts a photo.",
      "expected": "Treated as celebrity (no fan-out)",
      "explanation": "Threshold is >= 10,000. At exactly 10K, use PULL model."
    },
    {
      "name": "User becomes celebrity mid-session",
      "category": "Edge Case",
      "input": "User has 9,999 followers, posts (push). Gets 2 new followers, posts again (should be pull now).",
      "expected": "First post: fan-out. Second post: stored for pull.",
      "explanation": "Celebrity status is checked at post time, not pre-computed"
    },
    {
      "name": "Empty feed for new user",
      "category": "Edge Case",
      "input": "New user follows nobody. Requests feed.",
      "expected": "Empty feed response, has_more = false",
      "explanation": "Handle gracefully, don't crash"
    },
    {
      "name": "Pagination works correctly",
      "category": "Functionality",
      "input": "User has 50 posts in feed. Request page_size=10, then use cursor for next page.",
      "expected": "First page: 10 posts. Second page: next 10 posts. Correct cursor.",
      "explanation": "Cursor-based pagination for scalability"
    }
  ],
  "common_mistakes": [
    {
      "mistake": "Using only fan-out on write (pure push)",
      "why_wrong": "Doesn't scale for celebrities. 350M write operations per celebrity post = minutes of delay, massive storage.",
      "correct_approach": "Hybrid model with celebrity threshold",
      "code_wrong": "# Always fan out\nfor follower in get_all_followers(user_id):\n    add_to_feed(follower, post)",
      "code_correct": "if user.is_celebrity:\n    store_for_pull(post)\nelse:\n    fan_out_to_followers(post)"
    },
    {
      "mistake": "Storing images in database",
      "why_wrong": "200TB/day of photos cannot be stored in PostgreSQL or Cassandra. Databases are for metadata, not blobs.",
      "correct_approach": "Use object storage (S3) with CDN",
      "code_wrong": "INSERT INTO posts (photo_blob) VALUES (:binary_data)",
      "code_correct": "# Upload to S3, store URL in DB\ns3.put_object(bucket, key, photo_data)\nINSERT INTO posts (image_url) VALUES (:s3_url)"
    },
    {
      "mistake": "Not using CDN for images",
      "why_wrong": "Every image request hitting origin = massive latency and cost. CDN provides sub-50ms global access.",
      "correct_approach": "All images served via CDN (CloudFront, Akamai)",
      "code_wrong": "image_url = \"https://api.instagram.com/images/abc.jpg\"",
      "code_correct": "image_url = \"https://cdn.instagram.com/abc_1080.webp\""
    },
    {
      "mistake": "Synchronous thumbnail generation",
      "why_wrong": "User waits 5+ seconds for image processing. Unacceptable UX.",
      "correct_approach": "Return post_id immediately, process thumbnails async via queue",
      "code_wrong": "def upload():\n    generate_thumbnails(photo)  # 5 seconds\n    return post_id",
      "code_correct": "def upload():\n    store_original(photo)\n    queue.send(ProcessImageJob(post_id))  # Async\n    return post_id  # Return immediately"
    },
    {
      "mistake": "Using offset pagination for feed",
      "why_wrong": "OFFSET becomes slow for large offsets. Also, new posts shift results between pages.",
      "correct_approach": "Cursor-based pagination using post_id or timestamp",
      "code_wrong": "SELECT * FROM feed LIMIT 10 OFFSET 1000  # Slow!",
      "code_correct": "SELECT * FROM feed WHERE created_at < :cursor ORDER BY created_at DESC LIMIT 10"
    }
  ],
  "interview_tips": {
    "opening": "Thank you for this problem. Before I dive in, I'd like to clarify a few things and share my initial thoughts on the key challenges...",
    "clarifying_questions_to_ask": [
      "What's the expected scale? DAU, photos per day? (Answer: 500M DAU, 100M photos/day)",
      "Is the feed chronological or ranked by relevance? (Usually ranked, but start with chronological)",
      "Do we need to handle video, or just photos for now? (Photos first, video is follow-up)",
      "What's the acceptable latency for feed loading? (< 500ms is typical)",
      "Is eventual consistency acceptable for likes/comments? (Yes for most social apps)"
    ],
    "what_to_mention_proactively": [
      "The celebrity problem is the hardest part - I'll address it with a hybrid push/pull model",
      "I'll use CDN for all images - this is essential at this scale",
      "I'll choose databases based on access patterns: PostgreSQL for users, Cassandra for posts and feeds, Redis for caching",
      "I'll use async processing via message queues to keep upload fast"
    ],
    "communication_during_coding": [
      "Let me draw the high-level architecture first...",
      "I'm starting with the core feed algorithm because that's the hardest part...",
      "This is where the hybrid push/pull decision happens...",
      "For celebrities, we skip fan-out entirely..."
    ],
    "if_stuck": [
      "Let me step back and think about the read vs write patterns...",
      "What's the key constraint? The celebrity problem. How do we solve it?",
      "Let me draw out the data flow for a normal post vs celebrity post..."
    ],
    "time_management": "0-5min: Clarify requirements, confirm scale\n5-15min: High-level architecture diagram\n15-25min: Deep dive on feed generation (the hard part)\n25-35min: Database schema, API design\n35-45min: Discuss scaling, caching, trade-offs\n45-60min: Follow-up questions (Stories, Search, DMs)"
  },
  "pattern_recognition": {
    "pattern_name": "Hybrid Fan-out with Celebrity Threshold",
    "indicators": [
      "Social network with follow relationships",
      "Highly skewed follower distribution (power law)",
      "Read-heavy workload (100:1 ratio)",
      "Need for low-latency feed reads"
    ],
    "similar_problems": [
      "**Twitter Home Timeline**: Same hybrid approach, Twitter calls it 'mixed mode'",
      "**Facebook News Feed**: Similar but with more complex ranking",
      "**LinkedIn Feed**: Professional network with celebrity (influencer) problem",
      "**TikTok For You Page**: More algorithmic, less follow-based, but similar scale challenges"
    ],
    "template": "1. Identify the 'celebrity threshold' based on expected fan-out cost\n2. PUSH for users below threshold (pre-compute feeds)\n3. PULL for users above threshold (query on read)\n4. MERGE at read time with ranking"
  },
  "follow_up_preparation": {
    "part_2_hint": "## Stories Feature (Part 2)\n\n**Key additions:**\n- TTL on stories (24 hours) - Redis with expiration\n- Stories ordering by recency within each user\n- Viewer tracking (who viewed your story)\n- Use Redis sorted sets with TTL, partition by user_id",
    "part_3_hint": "## Search and Explore (Part 3)\n\n**Key additions:**\n- Elasticsearch for hashtag and caption search\n- Inverted index for user search\n- Explore page: trending + personalized recommendations\n- Consider pre-computing trending content",
    "part_4_hint": "## Direct Messaging (Part 4)\n\n**Key additions:**\n- WebSocket connections for real-time\n- Stronger consistency requirements (no missing messages)\n- Different database (could use Cassandra or specialized like ScyllaDB)\n- End-to-end encryption considerations",
    "data_structure_evolution": "Part 1: Redis + Cassandra for feed\n\u2192 Part 2: Add TTL to Redis, separate Stories table\n\u2192 Part 3: Add Elasticsearch, trending cache\n\u2192 Part 4: Add WebSocket layer, message queue for guaranteed delivery"
  },
  "communication_script": {
    "opening_verbatim": "Thank you for this problem! Instagram is a fascinating system to design. Before I start, let me make sure I understand the scope. We're designing the core photo-sharing platform with feed generation, right? And we're targeting Instagram scale - so roughly 500 million daily active users?",
    "after_clarification": "Great, so to summarize: we need to support photo uploads with sub-2-second latency, personalized feeds in under 500ms, and handle the celebrity problem where some users have hundreds of millions of followers. I'll use a hybrid push/pull model for feed generation. Does that direction sound good?",
    "while_coding": [
      "Let me start by sketching the high-level architecture...",
      "Here's where the magic happens - the celebrity check determines push vs pull...",
      "Notice I'm storing celebrity posts separately - they'll be pulled on read..."
    ],
    "after_coding": "Let me trace through a scenario: Alice follows Bob (normal user) and Kim (celebrity). When Bob posts, we fan out to Alice's feed cache. When Kim posts, we just store it. When Alice requests her feed, we merge the cached posts with pulled celebrity posts.",
    "when_stuck_verbatim": "I'm thinking about how to handle the fan-out at this scale... Let me step back. The key constraint is that celebrities can have 500 million followers. We can't write to 500 million feeds synchronously. So we need a different strategy for celebrities...",
    "after_mistake": "Actually, I realize I need to handle the merge differently - there could be duplicates if a post was somehow in both cached and celebrity lists. Let me add deduplication.",
    "before_moving_on": "So that covers Part 1 - the core photo sharing and feed generation. The key insight is the hybrid push/pull model with a celebrity threshold around 10,000 followers. Time complexity is O(1) for user-facing operations, with async fan-out. Ready for the Stories follow-up?"
  },
  "interviewer_perspective": {
    "what_they_evaluate": [
      "**Scale Awareness**: Do they understand the numbers? 500M DAU, 100M photos/day, 200TB storage/day",
      "**Trade-off Thinking**: Push vs Pull, consistency vs latency, storage vs compute",
      "**System Components**: Right databases for right use cases (S3, Redis, Cassandra)",
      "**Celebrity Problem**: This is the KEY test - did they identify and solve it?",
      "**Communication**: Clear explanations, good diagrams, structured thinking"
    ],
    "bonus_points": [
      "Mentioning the hybrid push/pull model unprompted",
      "Drawing clear architecture diagrams",
      "Discussing CDN and image processing pipeline",
      "Considering edge cases (user unfollows, post deletion)",
      "Mentioning real-world references (how Instagram/Twitter actually does it)"
    ],
    "red_flags": [
      "Using a single database for everything",
      "Not considering the celebrity problem",
      "Storing images in the database",
      "Synchronous processing for image thumbnails",
      "Not mentioning caching or CDN"
    ],
    "what_differentiates_strong_candidates": "Strong candidates immediately identify the celebrity problem as the core challenge, propose the hybrid solution, draw clear architecture diagrams, and can discuss trade-offs fluently. They think about the system from the user's perspective (latency) and the operator's perspective (cost, maintainability)."
  },
  "time_milestones": {
    "by_5_min": "Clarified scale requirements, identified key challenges (celebrity problem, storage scale, feed latency)",
    "by_15_min": "Drew high-level architecture with core components (API Gateway, Services, Databases, CDN)",
    "by_25_min": "Explained hybrid push/pull feed model in detail, including the celebrity threshold",
    "by_35_min": "Discussed database choices, schema design, and caching strategy",
    "by_45_min": "Covered scaling considerations, potential bottlenecks, and monitoring",
    "warning_signs": "If you're still clarifying at 10 min or haven't mentioned the celebrity problem by 20 min, you're behind. Prioritize the feed generation algorithm."
  },
  "recovery_strategies": {
    "when_you_make_a_bug": "It's fine - system design is about trade-offs, not perfect answers. Say: 'Actually, I realize this doesn't handle the case where... Let me adjust the design.' Adapt and move on.",
    "when_you_dont_know_syntax": "Not applicable for system design - this is about architecture, not code syntax.",
    "when_approach_is_wrong": "If you started with pure push and realize it doesn't scale: 'I see the problem now - this won't work for users with millions of followers. Let me introduce a hybrid approach where we only push for users below a threshold.'",
    "when_completely_stuck": "Ask: 'I'm thinking about how to handle the fan-out at scale. What's a typical approach you've seen?' Most interviewers will give hints.",
    "when_running_out_of_time": "Prioritize: 'Let me focus on the feed generation algorithm since that's the hardest part. For storage and CDN, I'll mention that we'd use S3 and CloudFront with standard patterns.'"
  },
  "ai_copilot_tips": {
    "when_using_cursor_or_copilot": "For system design, AI tools are less useful than for coding problems. The value is in your thinking process, not generated diagrams.",
    "what_to_do": [
      "Use AI to generate example API schemas",
      "Ask AI for database schema templates",
      "Use AI to calculate storage/bandwidth requirements"
    ],
    "what_not_to_do": [
      "Don't ask AI to 'design Instagram' - the interviewer wants YOUR thinking",
      "Don't copy-paste architecture diagrams without understanding",
      "Don't let AI drive the discussion"
    ],
    "how_to_demonstrate_understanding": "If you reference something AI-generated, explain WHY it works. 'I'm using a sorted set here because it gives O(log N) inserts and O(K) range queries, which is perfect for feed pagination.'",
    "expectation_adjustment": "System design interviews value communication and trade-off analysis. AI can't do that for you."
  },
  "signal_points": {
    "wow_factors": [
      "Immediately identifying the celebrity problem and proposing hybrid push/pull",
      "Drawing clear, labeled architecture diagrams on whiteboard/screen",
      "Calculating back-of-envelope numbers (200TB/day, 100K QPS)",
      "Mentioning how Instagram/Twitter actually solves this (public engineering blogs)",
      "Discussing operational concerns (monitoring, alerting, graceful degradation)"
    ],
    "subtle_signals_of_experience": [
      "Choosing Cassandra for write-heavy workloads, PostgreSQL for consistency",
      "Mentioning presigned URLs for direct S3 upload",
      "Considering cursor-based pagination over offset",
      "Discussing async processing and message queues naturally",
      "Thinking about cache invalidation strategies"
    ]
  },
  "red_flags_to_avoid": {
    "behavioral": [
      "Not drawing diagrams - visual communication is essential",
      "Jumping into details before establishing high-level architecture",
      "Getting defensive when interviewer suggests alternatives",
      "Not engaging with interviewer's questions or hints"
    ],
    "technical": [
      "Storing images in relational database (always use object storage)",
      "Not considering CDN (essential for media apps)",
      "Pure push model without celebrity consideration",
      "Ignoring eventual consistency trade-offs"
    ],
    "communication": [
      "Using buzzwords without explaining them",
      "Not summarizing before diving into details",
      "Spending too long on less important components",
      "Not asking if the interviewer wants to go deeper on any area"
    ]
  },
  "final_checklist": {
    "before_saying_done": [
      "Did I address all functional requirements? (upload, feed, timeline, follow, like)",
      "Did I explain the hybrid push/pull model for feed generation?",
      "Did I choose appropriate databases and explain why?",
      "Did I discuss CDN and image processing?",
      "Did I address scaling and potential bottlenecks?",
      "Did I mention caching strategy?"
    ],
    "quick_code_review": "N/A for system design - focus on architecture completeness"
  },
  "production_considerations": {
    "what_id_add_in_production": [
      "**Content Moderation**: ML pipeline to detect inappropriate content before publishing",
      "**Abuse Prevention**: Rate limiting, spam detection, bot detection",
      "**Analytics Pipeline**: Kafka \u2192 Spark \u2192 data warehouse for insights",
      "**A/B Testing Infrastructure**: Feature flags, experiment framework",
      "**Monitoring & Alerting**: Prometheus, Grafana, PagerDuty",
      "**Disaster Recovery**: Multi-region deployment, automated failover",
      "**GDPR Compliance**: Data deletion pipeline, export functionality"
    ],
    "why_not_in_interview": "45-60 minutes isn't enough to cover everything. Focus on core architecture and the hard problems (feed generation, scale). Mention these as 'additional considerations in production.'",
    "how_to_mention": "Say: 'In production, I'd also add content moderation, abuse prevention, and comprehensive monitoring. For this interview, I focused on the core feed generation challenge since that's the most interesting technical problem.'"
  },
  "generated_at": "2026-01-18T21:43:51.284333",
  "_meta": {
    "problem_id": "instagram_photo_sharing_design",
    "part_number": null,
    "model": "claude-opus-4-5-20251101"
  }
}